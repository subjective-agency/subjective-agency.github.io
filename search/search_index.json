{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Wapaganda Documentation Base","text":""},{"location":"#project-layout","title":"Project layout","text":"<ul> <li>Architecture Decision Records</li> <li>Application Registry</li> <li>Infrastructure</li> <li>Database</li> <li>Requests for Comments</li> </ul>"},{"location":"apps/","title":"Application Registry","text":"<p>This is a comprehensive list of internal applications used in the Wapaganda project</p>"},{"location":"apps/#structure-of-an-applicaiton-registry-record-arr","title":"Structure of an Applicaiton Registry Record (ARR)","text":"<p>Application Registry concept (and, correspondingly, ARRs) are inspired by Architecture Decision Records (ADRs)</p> <p>Application Registry Record is a document that describes an application that constitutes a part of a larger project. Each such document must contain 4 sections:</p> <ol> <li>General     In this section, describe general purpose and point of the application. This is a place to share broader context if necessary.</li> <li>Code     In this section, describe relevant repositories/modules, any nuances related to the code implementation.</li> <li>Entities     In this section, describe what entities are involved, and how they are connected.</li> <li>Flow Diagram     In this section, put flow diagram that describes how the application works. It could be an image, but recommended approach is Mermaid, where either <code>flow</code> or <code>block</code> diagram types could be used. (Note that <code>block</code> type is in beta, and isn't supported on some platforms)</li> <li>History     This optional section is reserved for any notes that make sense in context but aren't necessarily relevant anymore.</li> </ol>"},{"location":"apps/#applications","title":"Applications","text":"<ul> <li>ARR-1: Ave Media</li> <li>ARR-2: Periodicals</li> <li>ARR-3: Transfactory</li> <li>ARR-4: Tgram</li> <li>ARR-14: Web<ul> <li>Database Search was, at some point, a separate application.</li> </ul> </li> <li>ARR-5: SyncManager<ul> <li>ARR-7: Relevance service</li> <li>ARR-6: Queue service</li> <li>ARR-8: Smart Adder</li> <li>ARR-9: TheorySync</li> <li>ARR-10: Voiceprint Saver</li> <li>ad hoc</li> </ul> </li> <li>ARR-11: Transsuply &amp; Transsuply2</li> <li>ARR-12: Cooperation</li> <li>ARR-13: EntryBot</li> <li>ARR-15: Internal Review</li> <li>ARR-16: Voice Collection</li> </ul>"},{"location":"architecture/","title":"Architecture Decision Records","text":""},{"location":"architecture/#about-adrs","title":"About ADRs","text":"<p>Architecture Decision Record is a document that describes a particular decision or solution related to the fundamental functionality of the product. Each such document must contain 4 sections:</p> <ol> <li> <p>Context    In this section, describe the relevant circumstances, be they of technical or general nature, to answer the question why the change is/was necessary.</p> </li> <li> <p>Decision    In this section, describe what was implemented, and how. Be as specific as possible: this should be a comprehensive statement of a technical solution.</p> </li> <li> <p>Status    This should be just a single word or expression indicating current status of the document. Possible values: <code>accepted</code>, <code>in discussion</code>, <code>succeeded by</code>.</p> </li> <li> <p>Consequences    In this section, describe briefly expected or actualized consequences of the decision. Negative consequences should be described in detail; if there are no negative ones, this should be mentioned.</p> </li> </ol> <p>Aside from this 4-section structure, the shape of the information remains up to the author.</p>"},{"location":"architecture/#legend","title":"Legend","text":"<p>These symbols should be used in the title of an ADR document as a quick indication of its status.</p> Symbol Meaning \u221e Cooking (in progress, in discussion) \ud83d\uddf6\u227d Deprecated in favor of other ADR \ud83d\uddf6 Deprecated \ud83d\uddf8 Accepted (active)"},{"location":"architecture/#accepted-active-adrs","title":"Accepted &amp; Active ADRs","text":"<ol> <li>ADR-27 describes current structure of the project.</li> <li>ADR-25 describes Infisical-based secrets management within the project.</li> <li>ADR-10 describes the standard of personal images handling.</li> <li>ADR-17 describes nuances of the ISCO-08 adoption.</li> <li>ADR-19 describes nuances of a classification approach to organizations.</li> <li>ADR-21 describes automatic provenance mechanism.</li> <li>ADR-22 describes implementation of the custom database type.</li> <li>ADR-23 describes general approach to maintaining the project in 3 languages.</li> <li>ADR-18 describes JavaScript coding standards.</li> <li>ADR-30 describes approach and implementation of the website's <code>Theory</code> section.</li> <li>ADR-24 addresses nuances of subprojects' dockerization in <code>polylith</code> architecture.</li> <li>ADR-31 describes centralized logging with Logfire.</li> <li>ADR-33 describes nuances of the private Docker repository setup.</li> </ol>"},{"location":"architecture/#in-progress-under-revision-planned","title":"In Progress | Under Revision | Planned","text":"<ol> <li>ADR-20 describes technical details of the Ruskymir Creed implementation.</li> <li>ADR-7 would describe detailed view for an instance of a media episode.</li> <li>ADR-8 would describe the mechanism of automatic mark-up of transcribed texts.</li> <li>ADR-13 would describe theory and reasoning for the general approach to media segments.</li> <li>ADR-14 describes approach to handling printed data, such as books and articles.</li> <li>ADR-29 would accumulate nuances related to non-DB data storage.</li> <li>ADR-32 would describe the set up and process for backing up the database with <code>pgbackrest</code>.</li> </ol>"},{"location":"architecture/#abandoned-rejected-deprecated","title":"Abandoned | Rejected | Deprecated","text":"<ol> <li>ADR-28 describes the previous approach to backing up database, one based on JSON structured export.</li> <li>ADR-1 describes the original set up of the project. Superseded with ADR-12.</li> <li>ADR-12 describes 2nd stage in the project's evolution. Superseded with ADR-27.</li> <li>ADR-5 Deprecated due to removal of MongoDB from the stack.</li> <li>ADR-11 was about database summary stats. Rejected because not ADR material (moved elsewhere).</li> <li>ADR-15 was an attempt to collect common terms. Rejected because not ADR material (moved elsewhere).</li> <li>ADR-2 Used to describe how secrets are managed with the use of Doppler. Superseded with ADR-25.</li> <li>ADR-6 aimed to describe approach to transcripts, but got outdated before it was finished. Superseded with ADR-26.</li> <li>ADR-3 attempted to impose a desired git flow on the project. Rejected.</li> <li>ADR-9 aimed to describe localization approach, but got outdated before it was finished. Superseded with ADR-23.</li> <li>ADR-4 describes the web project's Django-based REST API. This was deprecated in favor of the new FastAPI-based backend.</li> <li>ADR-16 would've described the point and approach behind the public <code>Theory</code> section, but got outdated before it was finished. Superseded with ADR-30.</li> <li>ADR-26 was dedicated to Transfactory application before Application registry was introduced.</li> <li>ADR-36 was dedicated to Periodicals application before Application registry was introduced.</li> <li>ADR-37 was dedicated to Tgram application before Application registry was introduced.</li> <li>ADR-38 was dedicated to Ave Media application before Application registry was introduced.</li> </ol>"},{"location":"db/","title":"Database Guide","text":""},{"location":"db/#basics","title":"Basics","text":"<p>The master database of the Wapaganda project is PostgreSQL. Current version: 16.</p>"},{"location":"db/#extensions","title":"Extensions","text":"<p>The following extensions are installed:</p> <ul> <li>pgroonga. Support for full-text search, based on the Groonga project.</li> <li>pg_cron. Running scheduled jobs inside the DB</li> <li>pg_prewarm. Not really used. Supposed to speed up the db.</li> <li>plpgsql. Procedural language. Previously, default PL for complex functions.</li> <li>plpython3u. Procedural language. Current default PL for complex functions.</li> <li>postgres_fdw. Foreign wrapper allowing inter-database operations.</li> <li>cube</li> <li>vector</li> <li>uuid-ossp</li> </ul> <p>See below for installation details.</p>"},{"location":"db/#documentation-as-comments","title":"Documentation As Comments","text":"<p>Documentation on the database is maintained (kinda) in the form of comments to the tables and fields. All relevant and non-obvious columns were commented on. General DB flow now includes writing a comment on any such relevant column when adding it.</p> <p>Comments can be added in a standard PostgreSQL fashion via Bytebase.</p> <p>Comments can be retrieved by running a stored function <code>get_column_comment(table_name text, table_column text)</code> (or, again, viewed in Bytebase)</p>"},{"location":"db/#backup","title":"Backup","text":""},{"location":"db/#pgbackrest","title":"PgBackRest","text":"<p>TBD</p>"},{"location":"db/#indexes","title":"Indexes","text":"<p>TBD</p>"},{"location":"db/#fulltext-search","title":"Fulltext Search","text":"<p>Full-text search is implemented with <code>pgroonga</code> extension. <code>pgroonga</code> indexes were created for the following tables:</p> Table Fields Index name Search func <code>people</code> <code>fullname</code> <code>pgroonga_people_fullname</code> - <code>search_in_people_skim</code>- <code>search_in_people</code> <code>public.organizations</code> <code>name</code>, <code>short_name</code> <code>pgroonga_orgs_name_shortname</code> - <code>search_in_orgs</code> <code>public.youtube_vids</code> <code>title</code>, <code>description</code> <code>pgroonga_youtube_vids_title_desc_index</code> <code>search_in_youtube_vids</code> <code>public.komso_episodes</code> <code>title</code>, <code>description</code> <code>pgroonga_komso_vids_title_desc_index</code> <code>search_in_komso_episodes</code> <code>public.ntv_episodes</code> <code>title</code>, <code>description</code> <code>pgroonga_ntv_episodes_title_desc_index</code> <code>search_in_ntv_episodes</code> <code>public.smotrim_episodes</code> <code>title</code>, <code>description</code> <code>pgroonga_smotrim_episodes_title_desc_index</code> <code>search_in_smotrim_episodes</code> <code>data.transcribed_content</code> <code>content</code> <code>pgroonga_transcribed_content_index</code> <code>search_in_transcribed</code> <code>data.printed_content</code> <code>raw_content</code> <code>pgroonga_printed_content_index</code> <code>search_in_printed</code> <code>data.text_media</code> <code>title</code>, <code>excerpt</code> <code>author_data</code> <code>content</code> <code>pgroonga_text_media_title_excerpt_index</code> <code>pgroonga_text_media_author_data_index</code> <code>pgroonga_text_media_content_index</code> <code>search_in_text_media</code> <code>enums.isco08_taxonomy</code> <code>term</code> <code>pgroonga_isco08_taxonomy_term_index</code> <code>search_in_isco8</code> <code>enums.isco08_index</code> <code>name</code> <code>pgroonga_isco08_index_name_index</code> <code>search_in_isco8_index</code> <code>enums.orgs_taxonomy</code> <code>term</code> <code>pgroonga_orgs_taxonomy_term_index</code> <code>search_in_orgs_taxonomy</code> <code>enums.rucr_taxonomy</code> <code>content</code> <code>pgroonga_rucr_taxonomy_content_index</code> <code>search_in_rucr_taxonomy</code> <code>data.telegram_messages</code> <code>content</code> <code>pgroonga_telegram_messages_content</code> <ul> <li>Each of the search functions takes a string as the only argument, and returns a table with results. Naming policy: every <code>pgroonga</code> index name must start with word <code>pgroonga</code></li> <li>An index can be created with the following query:</li> </ul> <p><pre><code>create index pgroonga_table_name_index on table_name using pgroonga(column1, column2);\n</code></pre> * Existing indices could be viewed by running the following query:</p> <pre><code>SELECT indexname, tablename\nFROM pg_indexes\nWHERE indexname LIKE 'pgroonga%'\nORDER BY tablename, indexname;\n</code></pre>"},{"location":"db/#custom-types","title":"Custom Types","text":""},{"location":"db/#triple-language-format","title":"Triple language format","text":"<p>Used to refer to dictionary (aka JSON)-like format with set keys <code>en</code>, <code>ru</code> and <code>uk</code> until it was converted into a custom composite type saved under name <code>triple_lang</code> in December 2023:</p> <pre><code>create type triple_lang as\n(\n    en text,\n    ru text,\n    uk text\n);\n</code></pre> <p>Custom type required some adaptations to the code base. TBD</p>"},{"location":"db/#enums","title":"Enums","text":"<ul> <li><code>relevance_status</code>. Values are:</li> <li><code>garbage</code></li> <li><code>undefined</code></li> <li><code>likely relevant</code></li> <li> <p><code>relevant</code></p> </li> <li> <p>...</p> </li> </ul>"},{"location":"db/#database-queue","title":"Database Queue","text":"<p>...</p>"},{"location":"db/#secondary-databases","title":"Secondary databases","text":"<p>The following secondary databases are maintained on the same instance as the main production database:</p> <ul> <li><code>meta</code> database was created for storing important secondary data, specifically, <code>logs</code> and <code>provenance</code> records. </li> <li>Since then, logs storage has been deprecated in favor of Logfire. </li> <li><code>provenance</code> table in <code>public</code> schema serves for storing data generated by the <code>provenance</code> flow. It is mirrored by the foreign table <code>provenance</code> in <code>service</code> schema of the production DB.</li> <li><code>infisical</code> is native database of the Infisical application</li> <li><code>authentic</code> is native database of the Authentic application.</li> <li><code>windmill</code> is native database of the Windmill application. Currently, is not used.</li> </ul> <p>The following secondary databases are maintained on the same instance as the development database:</p> <ul> <li><code>memos</code> is native database of the Memos application. </li> <li><code>penpot</code> is native database of the Penpot application. </li> <li><code>bytebase</code> is native database of the Bytebase application.</li> <li><code>ua</code> is a database for various Ukraine-related stuff.</li> <li>paperless, immich, activepieces, receiptwrangler</li> <li>minds -- check</li> <li>authentik -- check</li> <li>hedgedoc -- check then drop</li> </ul>"},{"location":"db/#foreign-tables","title":"Foreign Tables","text":"<p>By default, Postgres does not allow inter-database communication, but it can be enabled with foreign wrappers. To make it so that <code>meta</code> and <code>wapadb</code> databases could exchange data, <code>postgres_fwd</code> extention was installed. Here's the general flow for this:</p> <pre><code>-- install extention\nCREATE EXTENTION postgres_fwd;\n-- create server\nCREATE SERVER meta_server FOREIGN DATA WRAPPER postgres_fdw OPTIONS (host 'localhost', port '6002', dbname 'meta');\n-- create user mapping\nCREATE USER MAPPIN FOR warp SERVER meta_server OPTIONS (user 'warp', password 'long_and_windy_password');\n-- given that a table to be mirrored already exists\nIMPORT FOREIGN SCHEMA source_schema_ie_public LIMIT TO (target_table1) FROM SERVER meta_server INTO target_schema_ie_service;\n-- sequence for the foreign table might not get created automatically, so\nCREATE SEQUENCE target_schema_ie_service_target_table1_sequence_id START 1;\n</code></pre> <p>After this you can work with <code>target_schema_ie_service.target_table1</code> as if it was a regular table.</p>"},{"location":"db/#existing-foreign-tables","title":"Existing foreign tables","text":"Table External DB provenance meta memos memos"},{"location":"db/#installation","title":"Installation","text":"<p>See here</p>"},{"location":"infra/","title":"Infrastructure","text":""},{"location":"infra/#hetzner","title":"Hetzner","text":""},{"location":"infra/#horsey","title":"Horsey","text":"<ul> <li>Ubuntu 22.04</li> <li>8 VCPU</li> <li>32Gb RAM</li> <li>240Gb local storage</li> <li>Additional volumes:</li> <li><code>db-data</code> 200Gb</li> <li><code>docker-horse</code> 100Gb</li> <li><code>garage</code> 100Gb</li> <li><code>volume-nbg1-2</code> 160Gb</li> <li></li> </ul>"},{"location":"infra/#principal","title":"Principal","text":"<ul> <li>Ubuntu 22.04</li> <li>4 VCPU</li> <li>8Gb RAM</li> <li>80Gb local storage</li> <li>Additional volumes:</li> <li><code>docker</code> 100Gb</li> <li><code>data</code> 100Gb</li> </ul>"},{"location":"infra/#railway","title":"Railway","text":"<p>Railway is a platform for deploying applications. We are using it to host production version of the website (both frontend and backend) for reasons of reliability.</p>"},{"location":"infra/#s3-storage","title":"S3 Storage","text":"<p>TBD</p>"},{"location":"infra/#github","title":"GitHub","text":""},{"location":"infra/#wapatools","title":"wapatools","text":"<p>Repository for everything backend. It's a home for multiple applications, including the FastAPI backend for the website. It's a monorepo that uses Poetry for managing dependecies, and is structured by the <code>polylith</code> approach. Specifically, this means:</p> <ul> <li>There are 3 basic entities that together, in different combinations, comprise project applications:</li> <li><code>project</code> is used to define high-level project dependencies and set up infrastructural things. Can be only 1 per application.</li> <li><code>component</code> usually servers as the gateway into application that binds together numerous other scripts. Normally, there's one per application, but you can have more if needed.</li> <li><code>base</code> is a software module to be imported. Normally, each <code>base</code> would have a strictly limited scope of functionality, and you would have many bases per application. For example, there is a dedicated base for working with the DB, another for configuration, etc.</li> <li>Each of these have their own tree structure, starting at <code>projects</code>, <code>components</code> and <code>bases</code> respectively. </li> <li>Another important concept here is <code>namespace</code> - it is created one per repository, and then used as a qualifier in the directory structures of <code>components</code> and <code>bases</code>. In case of this repository, namespace is <code>wapaganda</code>.</li> <li>Read more here and in the documentation for Python's implementation of polylith.</li> </ul>"},{"location":"infra/#wapaganda_frontend","title":"wapaganda_frontend","text":"<p>TBD</p>"},{"location":"infra/#wdocs-subjective-agencygithubio","title":"wdocs &amp; subjective-agency.github.io","text":"<p>This documentation. It's build with MkDocs and Material for MkDocs.</p>"},{"location":"infra/#authentication","title":"Authentication","text":""},{"location":"infra/#infisical","title":"Infisical","text":"<p>Infisical is a self-hosted service for managing application secrets.</p>"},{"location":"infra/#authentik","title":"Authentik","text":"<p>Authentik is a self-hosted service for authentication and authorization flows.</p>"},{"location":"infra/#scheduling","title":"Scheduling","text":""},{"location":"infra/#ofelia","title":"Ofelia","text":"<p>Ofelia is a service for scheduling API calls to internal docker-based applications.</p>"},{"location":"infra/#docker","title":"Docker","text":""},{"location":"infra/#private-registry-vs-docker-hub","title":"Private Registry vs Docker Hub","text":"<p>TBD</p>"},{"location":"infra/#database","title":"Database","text":"<p>See here.</p>"},{"location":"infra/#logging","title":"Logging","text":"<p>Logging used to be done in many ad-hoc ways, up to storing logs to our own database, but current solution is Logfire.</p> <p>Logfire is a paid service from the authors of Pydantic. It's feature-rich, evolves constantly, and is generally, a pleasure to use.</p> <p>Current application setup flow includes step for either creating a new Logfire project and connecting the app to it, or connecting it to an existing project.</p> Historical note <p>\u200b   Before October 2023 the infrastructure consisted mainly of a Supabase project, which combined a hosted Postgres instance and a 100Gb of S3 storage. Most of the workflows were run ad-hoc, from local environments.</p> <p>\u200b   In October 2023 the infra setup was changed: 1) Supabase was dropped in favor a Hetzner server with a PostgreSQL (db) and MinIO (storage) instances running on it; 2) another PostgreSQL instance was setup on Railway to be used as dev DB.</p> <p>\u200b   Later one, 2nd server on Hetzner was set up, and dev DB was moved there; numerous other improvements were introduced</p>"},{"location":"rfc/","title":"Requests for Comments (RFC)","text":"<ul> <li> <p>RFC-1. P-Rating</p> </li> <li> <p>RFC-2. Transcripts</p> </li> <li> <p>RFC-3. About Page</p> </li> <li> <p>RFC-4. Media Segments, YouTube Channels &amp; Tables</p> </li> <li> <p>RFC-5. Pre-Launch Database Optimization</p> </li> <li> <p>RFC-7. Media Episodes Attribution Problem</p> </li> </ul>"},{"location":"todo/","title":"Outstanding challenges","text":""},{"location":"active/general_data/","title":"Active Initiatives. General Data","text":"<p><code>General Data</code> refers to a range of flows and processes around adding new personalities and other information to the database</p>"},{"location":"active/general_data/#primary-goal","title":"Primary goal","text":"<p>Formalization of process, reduction of manual parts, more automation.</p>"},{"location":"active/general_data/#progress-log","title":"Progress Log","text":""},{"location":"active/general_data/#feb-1-2023","title":"Feb 1, 2023","text":"<ul> <li>Repository <code>wapaganda_tools</code> was compiled to have all the relevant scripts in one place.</li> <li><code>update-smotrim</code> and <code>update-youtube</code> flows were developed, along with <code>transcript</code>, <code>pickle-backup</code>, <code>insert relationships</code>, <code>add person with photo</code>, and <code>upsert photo</code> flows to handle recurring tasks.</li> <li>Already existing <code>telegram-messages</code>, <code>telegram-stats</code> <code>resolve telegram</code> and <code>resolve youtube</code> flows were improved and updated.</li> <li>In addition, scripts for downloading media data from <code>smotrim</code> and <code>youtube</code> were added, too.</li> <li>Tools were isolated into 2 modules, <code>utils</code> and <code>common</code>, each with a number of submodules for various tasks.</li> <li>Every function producing a list of anything was supplemented with a decorator <code>pickle-backup</code>, which is a part of the flow of the same name.</li> <li>All relevant scripts were updated to write logs.</li> </ul>"},{"location":"active/general_data/#march-19-2023","title":"March 19, 2023","text":"<ul> <li>GitHub organization was created, and 3 repositories were moved there, including <code>wapaganda_tools</code></li> <li>@atatatko's scripts were added to the <code>utils</code> module;</li> <li>Internal modules were re-organized and functions were documented;</li> <li>Flow for adding new patients was improved to work over a list of new objects;</li> <li><code>komso</code> episodes are now taken from the komsomolka website, which offers better structure and higher bandwidth;</li> <li>Many new smotrim segments were added, including purely audio-based;</li> <li>Youtube update flow was improved to start with segments;</li> <li>Download scripts were generally improved;</li> <li><code>sql</code> folder was created to store SQL code for functions, views and certain other actions.</li> </ul>"},{"location":"active/general_data/#july-2-2023","title":"July 2, 2023","text":"<ul> <li>Yurii Cherkasov 's <code>log_helper</code> was fully integrated into the codebase;</li> <li>Full text search in the DB was implemented with <code>pgroonga</code> against multiple tables, including <code>people</code>, <code>ogranizations</code>, etc;</li> <li>Theory module was added, including corresponding section on the website, DB infrastructure, and Python scripts for adding and resolving new texts, taking into advantage newly developed OOP web parsing module;</li> <li><code>Edit markdown</code> flow was implemented to retrieve texts stored in the DB and upload the updated version;</li> <li>Scripts, <code>lookup_data</code>, <code>lookup_image</code> and <code>lookup_org_data</code> were implemented to retrieve corresponding info and display it for informational purposes;</li> <li>The script for retrieval and update of Meduza's daily news stream was developed;</li> <li>Supabase API was completely removed from the codebase (99%). Instead, the code now relies on more generic <code>psycopg</code>, <code>requests</code> and <code>requests_html</code> libraries;</li> <li>New media flows were added, for DenTV, NTV and Rodniki;</li> <li>Existing media flows were refactored and improved;</li> <li>OpenAI API was partially integrated into the code, for purposes of text translation. More purposes should be identified;</li> <li>An algorithm for initial parsing/treatment of EPUB book files was developed;</li> <li>Development of the ontology for the project has begun, which includes adoption of the existing tech stack and data storage to the OWL framework, creation and elaboration of classes, relationships etc., as well as inclusion of existing ontologies and taxonomies.</li> <li>ISCO08  taxonomy (professions) was saved to the database, with a view on setting strict order to the <code>patient</code>-to-<code>organization</code> relationship.</li> </ul>"},{"location":"active/general_data/#november-25-2023","title":"November 25, 2023","text":"<ul> <li>Database was moved off Supabase to standalone server for good. A few things fell off during the transition, and were all fixed.</li> <li>A flow for text sources was developed with the use of <code>factory</code> programming pattern. Still WIP.</li> <li>Lookup scripts were all integrated into one CLI tool.</li> <li>Logging was improved in a number of way, including structured logging, context-manager based logging, and saving logs into the dedicated db.</li> <li>Railway DB instance was set up and \"migrated to\" with the use of Snaplet.</li> <li>Screenshot-monitor script was set up to automatically parse certain data from captured screenshots.</li> <li>Windmill was introduced into the tool-stack to handle internal flows and apps.</li> <li>Databases were created for logging (<code>logs</code>), to support the tool framework (<code>tool_framework</code>), and for experimenting (<code>unrelated</code>)</li> </ul>"},{"location":"active/general_data/#left-to-do","title":"Left to do","text":"<ul> <li>[ ] Add all currently unintegrated text sources.</li> <li>[ ] Finalize auto translation flow.</li> <li>[ ] Work on the books-related scripts.</li> <li>[ ] Work with RUCR and other ontologies.</li> </ul>"},{"location":"active/transcript_factory/","title":"Active Initiatives. Transcript Factory","text":"<p><code>Transcript Factory</code> refers to the mass production of transcripts of the relevant media episodes using openai/Whisper.</p>"},{"location":"active/transcript_factory/#progress-log","title":"Progress Log","text":""},{"location":"active/transcript_factory/#july-2-2023","title":"July 2, 2023","text":"<ul> <li>Box was ousted as the storage component quite a long time ago. Instead, Supabase storage is used.</li> <li>Supabase API was removed as a dependency in favor of <code>psycopg</code> library (DB connection) and <code>requests</code> (storage operations)</li> <li>Note: using Supabase API for storage connection may have been the cause of some downloading issues on the Transfactory node side.</li> <li>The scripts underwent a number of adjustments and improvements, but the flow essentially remained unchanged since the moment it was established.</li> </ul>"},{"location":"active/transcript_factory/#november-25-2023","title":"November 25, 2023","text":"<ul> <li>The flow underwent a major refactoring:</li> <li>Transprocessor was updated with a view at running it in a Docker container on server (WIP). Clean-up functionality was integrated into the processor to simplify the setup (except for the resulting <code>.srt</code> files, for which an expiration policy was set at 1 week as a safety fallback measure).</li> <li>Transcleaner was deprecated due to its functionality being merged into the processor.</li> <li>Transsuply wasn't changed much on the code level, but the decision was made to run it no more than once a week, uploading 50-60 Gb of audiofiles in one go.</li> <li>Factory node script is undergoing the process of refactoring into a CLI.</li> <li>In addition, <code>minio</code> client is now being used to communicate with storage and not <code>requests</code>.</li> </ul>"},{"location":"active/transcript_factory/#goals","title":"Goals","text":"<ul> <li>[ ] Finish refactoring Factory node script into a CLI</li> <li>[ ] Refactor <code>transsuply</code> into a CLI</li> <li>[ ] Spin up Docker container with Transprocessor on server.</li> </ul>"},{"location":"active/web_app/","title":"Active Initiatives. Web Application","text":"<p><code>Web Application</code> refers to <code>wapaganda</code>, aka <code>W</code>, which is the web interface to the database and accompanying storage entities.</p>"},{"location":"active/web_app/#progress-log","title":"Progress Log","text":""},{"location":"active/web_app/#july-2-2023","title":"July 2, 2023","text":"<ul> <li>Endpoint for the patients is implemented; endpoints for <code>theory</code> and <code>organizations</code> are under way;</li> <li>Principal design is applied to the list of the patients; work on the patient's page design is under way, as well as an article page, and dark/light mode switcher;</li> <li>Issues with CORS were overcome, and lots of other crap.</li> </ul>"},{"location":"active/web_app/#november-25-2023","title":"November 25, 2023","text":"<ul> <li>Logo was created.</li> <li>Multiple design improvements, usually following the changes in the data structures.</li> <li>RUCR diagram was created and integrated into the UI.</li> <li></li> </ul>"},{"location":"active/web_app/#goals","title":"Goals","text":"<ul> <li>[ ] Finalize RUCR</li> <li>[ ] Finalize the translation flow of RUCR</li> </ul>"},{"location":"apps/1/","title":"ARR-1: Ave Media","text":""},{"location":"apps/1/#general","title":"General","text":"<p>Ave Media is an application for collecting new items from various media platforms.</p> <p>Process is initialized for each platform separately, which allows to run updates concurrently.</p>"},{"location":"apps/1/#code","title":"Code","text":"<p>Modules involved:</p> <ul> <li><code>bases/w/media</code></li> <li><code>bases/w/database</code></li> <li><code>bases/w/config</code></li> <li><code>components/w/ave_media</code></li> </ul> <p>Source-specific implementations are located at <code>bases/w/media/platforms</code>. File <code>platform_service</code> contains base classes <code>MediaPlatform</code> and <code>MediaTarget</code>, which are parents to platform-specific implementations in correspondingly named files in the same directory.</p>"},{"location":"apps/1/#entities-media-platforms","title":"Entities -- Media Platforms","text":"Platform URL Status Targets table Episodes table Other tables Youtube youtube.com Active <code>youtube_channels</code> <code>youtube_vids</code> Smotrim.ru smotrim.ru Active <code>smotrim_brands</code> <code>smotrim_episodes</code> Rutube rutube.ru Active <code>rutube_channels</code> <code>rutube_episodes</code> Soundstream soundstream.media In development <code>soundstream_channels</code> <code>soundstream_episodes</code> <code>soundstream_playlists</code> VK vk.com To be added TBD Komsomolka radiokp.ru To be refactored TBD Dentv dentv.ru To be refactored TBD Ntv ntv.ru To be refactored TBD"},{"location":"apps/1/#flow-diagram","title":"Flow Diagram","text":"<pre><code>flowchart TD\n    quit_on_resolve(Quit)\n    is_target_resolved(Is target resolved?)\n\n    subgraph collect_targets [Collect and init targets]\n    direction TB\n        fetch_channels(Fetch target-level entities from DB)\n        fetch_channels -- for each --&gt; init_target\n    end\n\n    subgraph init_target [Init target]\n    direction TB\n        assign_session2(Assign DB session)\n        assign_channel(Assign target-level entity)\n        set_unresolved(Set target as unresolved)\n        fetch_latest(Fetch target's latest episode)\n        set_empty(Set target for collecting all available updates\\nfetch_all = True)\n\n        assign_session2 --- assign_channel -- if `title` is missing --&gt; set_unresolved\n        assign_channel --&gt; fetch_latest -- if no latest item found --&gt; set_empty\n    end\n\n    subgraph init_platform [Init platform]\n    direction LR\n        assign_session1(Assign DB session)\n        assign_platform_specific(Assing platform-specific attributes)\n\n        assign_session1 --- assign_platform_specific --- collect_targets\n        assign_platform_specific -- if YouTube --&gt; ytb(Youtube API client)\n    end\n\n    subgraph resolve_target [Resolve target]\n    direction TB\n        query_platform(Fetch target data from the platform)\n        update_target_db(Update DB record)\n        mark_resolved(Mark target as resolved)\n        mark_dead(Mark target as defunct)\n\n        query_platform -- if found --&gt; mark_resolved --&gt; update_target_db\n        query_platform -- if not found --&gt; mark_dead\n    end\n\n    subgraph fetch_ytb_updates [ ]\n    direction TB\n        ytb_get_playlist_vids(Use 'get_playlist_videos' method)\n        ytb_limit_none(with limit = None)\n        ytb_limit_100(with limit = 100)\n        ytb_get_vids(Use 'videos' method for additional data)\n        ytb_check_record_exists(Query DB to check if this record exists)\n        ytb_add_to_updates(Add to final list)\n        ytb_parse_vids(Parse all collected videos)\n        ytb_persist_vids(Save videos to the database)\n\n        ytb_get_playlist_vids -- if fetch_all --&gt; ytb_limit_none --&gt; ytb_get_vids\n        ytb_get_playlist_vids -- if only update --&gt; ytb_limit_100 --&gt; ytb_get_vids\n\n        ytb_get_vids -- for each vid --&gt; ytb_check_record_exists -- if not --&gt; ytb_add_to_updates --&gt; ytb_parse_vids --&gt; ytb_persist_vids\n    end\n\n    subgraph fetch_smotrim_updates [ ]\n    direction TB\n        sm_set_params(Define params of update)\n        sm_fetch_all(with pages = 10)\n        sm_update(with pages = 2)\n        sm_fetch_audio_updates(Fetch audio updates)\n        sm_fetch_video_updates(Fetch video updates)\n        sm_parse_audio_updates(Parse audio updates)\n        sm_parse_video_updates(Parse video updates)\n        sm_deduplicate(Remove internal duplicates)\n        sm_check_exists(Check if record already exists in DB)\n        sm_combine_updates(Add to final list)\n        sm_persist(Save updates to the database)\n        sm_temp( )\n\n        sm_set_params -- if fetch_all --&gt; sm_fetch_all --&gt; sm_temp\n        sm_set_params -- if update --&gt; sm_update --&gt; sm_temp\n\n        sm_temp -- podcast &amp; brand --&gt; sm_fetch_audio_updates --&gt; sm_parse_audio_updates --&gt; sm_deduplicate\n        sm_temp -- brand --&gt; sm_fetch_video_updates --&gt; sm_parse_video_updates --&gt; sm_deduplicate\n\n        sm_deduplicate -- for each record --&gt; sm_check_exists -- if not --&gt; sm_combine_updates -- if any updates collected --&gt; sm_persist\n    end\n\n    subgraph fetch_rutube_updates [ ]\n    direction TB\n        rtb_set_params1(Set basic params)\n        rtb_fetch_all(with pages = 1000)\n        rtb_fetch_new(with pages = 2)\n        rtb_set_params2(Set page params)\n        rtb_fetch_updates(Fetch updates)\n        rtb_parse_updates(Parse updates)\n        rtb_deduplicate(Remove internal duplicates)\n        rtb_check_exists(Check if record already exists in DB)\n        rtb_persist(Save updates to the database)\n\n        rtb_set_params1 -- if fetch_all --&gt; rtb_fetch_all --&gt; rtb_set_params2\n        rtb_set_params1 -- if update --&gt; rtb_fetch_new --&gt; rtb_set_params2\n        rtb_set_params2 --&gt; rtb_fetch_updates --&gt; rtb_parse_updates --&gt; rtb_deduplicate -- for each record --&gt; rtb_check_exists -- if not --&gt; rtb_persist\n    end\n\n    subgraph update_target [ ]\n    direction TB\n        fetch_target_updates(Collect target's updates)\n\n        is_target_resolved -- no --&gt; resolve_target -- if resolved --&gt; fetch_target_updates -- YouTube --&gt; fetch_ytb_updates\n        mark_dead --&gt; quit_on_resolve\n        is_target_resolved -- yes --&gt; fetch_target_updates -- Smotrim --&gt; fetch_smotrim_updates\n        fetch_target_updates -- Rutube --&gt; fetch_rutube_updates\n    end\n\n    init_platform -- for each target --&gt; update_target\n</code></pre>"},{"location":"apps/1/#history","title":"History","text":"<p>Here would be some historical notes</p>"},{"location":"apps/10/","title":"ARR-10 SyncManager ::: Voiceprint Saver","text":""},{"location":"apps/10/#general","title":"General","text":""},{"location":"apps/10/#code","title":"Code","text":""},{"location":"apps/10/#entities","title":"Entities","text":""},{"location":"apps/10/#flow-diagram","title":"Flow Diagram","text":""},{"location":"apps/10/#history","title":"History","text":""},{"location":"apps/11/","title":"ARR-11 Transsuply &amp; Transsuply2","text":""},{"location":"apps/11/#general","title":"General","text":""},{"location":"apps/11/#code","title":"Code","text":""},{"location":"apps/11/#entities","title":"Entities","text":""},{"location":"apps/11/#flow-diagram","title":"Flow Diagram","text":""},{"location":"apps/11/#history","title":"History","text":""},{"location":"apps/12/","title":"ARR-12 Cooperation","text":""},{"location":"apps/12/#general","title":"General","text":""},{"location":"apps/12/#code","title":"Code","text":""},{"location":"apps/12/#entities","title":"Entities","text":""},{"location":"apps/12/#flow-diagram","title":"Flow Diagram","text":""},{"location":"apps/12/#history","title":"History","text":""},{"location":"apps/13/","title":"ARR-13 EntryBot","text":""},{"location":"apps/13/#general","title":"General","text":""},{"location":"apps/13/#code","title":"Code","text":""},{"location":"apps/13/#entities","title":"Entities","text":""},{"location":"apps/13/#flow-diagram","title":"Flow Diagram","text":""},{"location":"apps/13/#history","title":"History","text":""},{"location":"apps/14/","title":"ARR-14 Web","text":""},{"location":"apps/14/#general","title":"General","text":""},{"location":"apps/14/#code","title":"Code","text":""},{"location":"apps/14/#entities","title":"Entities","text":""},{"location":"apps/14/#flow-diagram","title":"Flow Diagram","text":""},{"location":"apps/14/#history","title":"History","text":""},{"location":"apps/15/","title":"ARR-15 Internal Review","text":""},{"location":"apps/15/#general","title":"General","text":""},{"location":"apps/15/#code","title":"Code","text":""},{"location":"apps/15/#entities","title":"Entities","text":""},{"location":"apps/15/#flow-diagram","title":"Flow Diagram","text":""},{"location":"apps/15/#history","title":"History","text":""},{"location":"apps/16/","title":"ARR-16: Voice Collection","text":""},{"location":"apps/16/#general","title":"General","text":""},{"location":"apps/16/#code","title":"Code","text":""},{"location":"apps/16/#entities","title":"Entities","text":""},{"location":"apps/16/#flow-diagram","title":"Flow Diagram","text":""},{"location":"apps/16/#history","title":"History","text":""},{"location":"apps/2/","title":"ARR-2: Periodicals","text":""},{"location":"apps/2/#general","title":"General","text":"<p>Periodicals is a data collector application that specifically targets websites of news outlets like newspapers, magazines, etc.</p> <p>The application operates over <code>Website</code>s (represented by <code>websites</code> table in the DB), referred to as <code>sources</code> in the application.</p> <p>There are 2 major parts to the flow: Fetcher and Scanner. Both are implemented as base classes from which source-specific child classes derive. In this context, adding a new source would mean defining both <code>scanner</code> and <code>fetcher</code> implementations with source-specific details.</p>"},{"location":"apps/2/#code","title":"Code","text":"<ul> <li><code>bases/w/webparse</code></li> <li><code>bases/w/database</code></li> <li><code>bases/w/config</code></li> <li><code>components/w/periodicals</code></li> </ul>"},{"location":"apps/2/#entities","title":"Entities","text":"<ul> <li><code>Webpage</code> mirrors <code>data.text_media</code> DB table;</li> <li><code>Website</code> mirrors <code>public.websites</code> DB table</li> </ul>"},{"location":"apps/2/#flow-diagram","title":"Flow Diagram","text":""},{"location":"apps/2/#scanner","title":"Scanner","text":"<p>Scanner is responsible for collecting new items from 'category' pages. </p> <pre><code>flowchart TD\n    subgraph init_update [Init scanner]\n    direction TB\n        get_target(Fetch source to scan)\n        get_latest(Fetch source's latest item)\n        get_target --&gt; get_latest --&gt; create_scanner\n    end\n    subgraph create_scanner [Invoke factory to create the scanner]\n    direction LR\n        assign_source(Assign source)\n        assign_latest(Assign latest)\n        selector(Disassemble selector)\n        assign_source --&gt; assign_latest --&gt; selector\n    end\n    subgraph parse_upd [Extract]\n        get_title(Title)\n        get_excerpt(Excerpt)\n        get_date(Publish date)\n        get_author(Authors data)\n        get_url(URL)\n        get_additional(Additional data)\n        get_title --- get_excerpt --- get_date --- get_author --- get_url --- get_additional\n    end\n    subgraph persist_upd [Check and persist]\n        check_item(Check for presence in the DB)\n        add_item(Add to the database)\n        check_item -- if not present --&gt; add_item\n    end\n    subgraph run_upd [Run Update]\n    direction TB\n        fetch_via_http(Fetch new items via HTTP)\n        extract_items(Extract items from HTTP content)\n        fetch_via_api(Fetch new items via API)\n        dedupe(Drop duplicates from final list)\n\n        fetch_via_http --&gt; extract_items -- each item --&gt; parse_upd\n        fetch_via_api -- each item --&gt; parse_upd\n        parse_upd --&gt; dedupe -- for each item --&gt; persist_upd\n    end\n\ncall_update(Trigger 'update' endpoint) --&gt; init_update --&gt; run_upd --&gt; commit(Commit updates to the database)</code></pre>"},{"location":"apps/2/#fetcher","title":"Fetcher","text":"<p>Fetcher is responsible for getting content of relevant articles (which is missing from the 'category' pages most of the time).</p> <pre><code>flowchart TD\n    subgraph create_fetcher [Invoke factory to create the fetcher]\n    direction LR\n        assign_page(Assing page)\n    end\n    subgraph init_fetch [Init fetcher]\n    direction LR\n        get_page(Get page for update)\n    end\n    subgraph parse_page [Extract]\n    direction TB\n        merge_vals(Combine new and old values)\n        get_title(Title) --&gt; merge_vals\n        get_img(First image) --&gt; merge_vals\n        get_date(Publish date) --&gt; merge_vals\n        get_author(Authors data) --&gt; merge_vals\n        get_excerpt(Excerpt) --&gt; merge_vals\n        get_url(URL) --&gt; merge_vals\n        get_additional(Additional data) --&gt; merge_vals\n    end\n    subgraph resolve_page [ ]\n        fetch_httpx(Niquests)\n        fetch_pl(Playwright)\n        extract_content(Extract content)\n        fetch_httpx --&gt; extract_content --&gt; parse_page\n        fetch_pl --&gt; extract_content\n    end\n\n\ncall_fetch(Trigger 'fetch' endpoint) --&gt; init_fetch --&gt; create_fetcher --&gt; resolve_page --&gt; commit(Commit updates to the database)\n</code></pre>"},{"location":"apps/3/","title":"ARR-3: Transfactory","text":""},{"location":"apps/3/#general","title":"General","text":"<p>Transfactory is an app that transforms massive amounts of video/audio files into text using OpenAI's Whisper. It operates in conjunction with Transsuply around a special database table which serves as a queue: the supply app adds new jobs to it for the factory to pick up.</p> <p>The necessity for update is due to change in the principal algorithm: where the old one used official Whisper model with no optimization, the new one uses WhisperX, a highly optimized version that offers a significant increase in processing speed, as well as improved output.</p> <p>See also Transsuply docs.</p>"},{"location":"apps/3/#code","title":"Code","text":"<ul> <li><code>bases/w/database</code></li> <li><code>bases/w/storage</code></li> <li><code>bases/w/config</code></li> <li><code>bases/w/transfactory_common</code></li> <li><code>components/w/trans_fast_factory</code></li> </ul>"},{"location":"apps/3/#entities","title":"Entities","text":"<ul> <li><code>Transcript</code> mirrors <code>data.transcripts</code> DB table</li> <li><code>TranscribedLine</code> mirrors <code>data.transcribed_content2</code> DB table</li> <li><code>TransfactoryAnomaly</code> mirrors <code>service.transfactory_anomalies</code> table</li> <li><code>Prabyss</code> mirrors <code>service.prabyss</code> DB table</li> <li><code>FactoryJob</code> mirrors <code>service.factory_jobs_run_details</code> DB table</li> <li><code>TransfactoryNode</code> mirrors <code>enums.transfactory_nodes</code> DB table</li> <li><code>VoiceprintTemp</code> mirrors <code>service.voiceprint_temp</code> DB table</li> </ul>"},{"location":"apps/3/#flow-diagram","title":"Flow Diagram","text":"<pre><code>flowchart TB\n    classDef exit_red fill:#cd0d3b\n    subgraph check_lines [Check for existing lines]\n        check_transcribed(Check if transcription exists)\n        set_need_transcription(Set 'need_transcription' to False)\n        check_translated(Check if translation exists)\n        set_need_translation(Set 'need_translation' to False)\n        check_transcribed -- if exists --&gt; set_need_transcription\n        check_translated -- if exists --&gt; set_need_translation\n    end\n\n    subgraph assign_transcript [Get &amp; assign transcript]\n    direction TB\n        check_existence(Check if Transcript for 'path' already exists)\n        get_transcript(Get existing transcript)\n        create_transcript&gt;Create new transcript]\n        assign_transcript_id(Assing transcript ID)\n\n        check_existence -- if found --&gt; get_transcript --&gt; assign_transcript_id\n        check_existence -- if not found --&gt; create_transcript --&gt; assign_transcript_id\n    end\n\n    subgraph assign_initial [ ]\n        assign_db_session(Assign Database session)\n        assign_machine_id(Assign machine ID)\n        assign_trans_settings(Assign transcription settings)\n        assign_storage_client(Assign storage client)\n        assign_auth_settings(Assign internal auth settings)\n        assign_db_session --- assign_machine_id --- assign_trans_settings --- assign_storage_client --- assign_auth_settings\n    end\n\n    subgraph assign_new_job [ ]\n        new_job(Get new job from PRABYSS table)\n        exit_after_new_job(Exit)\n        set_in_progress&gt;Set job's status to 'in progress']\n\n        new_job -- if none found --&gt; exit_after_new_job:::exit_red\n        new_job -- if found --&gt; set_in_progress\n    end\n\n    subgraph init_service [Init]\n    direction TB\n        get_machine(Fetch additional Machine's params from DB)\n        hardware_settings(Extract settings related to hardware)\n        decide_translation(Make decision if translation is needed)\n        create_stats(Create FactoryJob object to hold job's stats)\n        exit_on_lines_check(Exit)\n        create_engine(Create WhisperX engine)\n\n        assign_initial --&gt; get_machine --&gt; hardware_settings --&gt; create_engine\n        assign_initial --&gt; assign_new_job --&gt; decide_translation --&gt; create_stats --&gt; assign_transcript --&gt; check_lines --&gt; create_engine\n        check_lines -- if both transcription\\nand translation exist --&gt; exit_on_lines_check:::exit_red\n    end\n\n    subgraph perform_whisper_job [ ]\n    direction TB\n        load_model(Load model)\n        load_audio(Load audio)\n        run_task(Produce raw Whisper result)\n        align_result(Perform alignment)\n        diarize_result(Diarize result)\n        cleanup_resources(Unload resources and clear the memory)\n        collect_timestamps(Collect timestamps)\n        return_result(Return diarized result and timestamps)\n\n        load_model --&gt; load_audio --&gt; run_task --&gt; align_result --&gt; diarize_result --&gt; cleanup_resources --&gt; collect_timestamps --&gt; return_result\n    end\n\n    subgraph handle_job [ ]\n    direction TB\n        collect_stats(Collect stats)\n        shape_lines(Convert raw output into DB objects)\n        persist_lines&gt;Add converted lines to DB session]\n        scan_anomalies(Scan output for anomalies)\n        persist_anomalies&gt;Add anomalies to DB session]\n\n        perform_whisper_job --&gt; collect_stats --&gt; shape_lines --&gt; persist_lines --&gt; scan_anomalies -- if any found --&gt; persist_anomalies\n    end\n\n    subgraph verify_job_success [ ]\n    direction TB\n        check_(Verify job success)\n        verify_only_transcribed(Check just transcription)\n        verify_transcribed_and_translated(Check transcription and translation)\n        check_ -- if lang = en --&gt; verify_only_transcribed\n        check_ -- if lang != en --&gt; verify_transcribed_and_translated\n    end\n\n    subgraph handle_voiceprints [Handle voiceprints]\n    direction TB\n        ensure_auth(Ensure auth token exists and is valid)\n        extract_raw_speakers(Collect unique speakers from transcription output)\n        extract_speaker_audio(Extract 60 seconds of voice material)\n        make_voiceprint_request&gt;Make request to voiceprint endpoint]\n\n        extract_raw_speakers -- for each --&gt; extract_speaker_audio --&gt; make_voiceprint_request\n        ensure_auth --- make_voiceprint_request\n    end\n\n    subgraph run [Run]\n    direction TB\n        handle_translation(Handle translation)\n        handle_transcription(Handle transcription)\n        download_audio(Download audio file)\n        measure_file_size(Measure file size)\n        local_cleanup(Do local cleanup)\n        drop_from_remote(Remove audio from remote storage)\n        set_job_completed&gt;Add job to DB session with status 'completed']\n        add_stats_db&gt;Add stats object to DB session]\n        persist_to_db&gt;Persist everything to the database]\n        exit_if_no_lines(Exit)\n        exit_after_verify(Exit)\n\n        download_audio -.- measure_file_size\n        download_audio -- if need_transcription = True --&gt; handle_transcription -- if need_translation = True --&gt; handle_translation --&gt; local_cleanup --&gt; verify_job_success -- Success --&gt; handle_voiceprints --&gt; set_job_completed --&gt; add_stats_db --&gt; persist_to_db --&gt; drop_from_remote\n        handle_transcription -.- handle_job\n        handle_translation -.- handle_job\n        handle_job -- if no lines produced --&gt; exit_if_no_lines:::exit_red\n        verify_job_success -- Failure --&gt; exit_after_verify:::exit_red\n\n    end\n\ninit_service --&gt; run</code></pre>"},{"location":"apps/4/","title":"ARR-4: Tgram","text":""},{"location":"apps/4/#general","title":"General","text":"<p>Tgram application periodically scans a collection of Telegram channels for updates and persists them to the database. Secondary function of the application is resolution of new channels.</p>"},{"location":"apps/4/#resolving-empty-channels","title":"Resolving empty channels","text":"<p>Normally, new channels are added to the database with just <code>handle</code> attribute, and therefore require fetching additional data from TelegramAPI and Telemetr.</p> <p>Note</p> <p>Telemetr is a platform that collects Telegram channels' statistics. Their API allows for a limited number of requests over a month (1000), which is more than sufficient for resolution, but not enough for consistent data collection.</p> <p>The process of channel resolution first queries Telegram API for channel's details, and also collects channel's first published message (which is usually a technical message about channel creation). Then Telemetr API is queried; the data is combined into a db object and persisted with a quirk.</p> <p>Warning</p> <p>The quirk has to do with Telemetr IDs and inconsistencies in how they are stored in the db. This is mainly a historical artifact that would be fully deprecated in one of future versions. It doesn't effect the flow. See the <code>fetch</code> part of the flow diagram below.</p>"},{"location":"apps/4/#scanning-for-updates","title":"Scanning for updates","text":"<p>Scanning process get the least updated channel from the db, and fetches 200 messages using Telegram API counting from the max known message id for this channel. Then messages are filtered, converted and persisted; channel parameters are updated correspondingly.</p>"},{"location":"apps/4/#known-issues-and-limitations","title":"Known issues and limitations","text":"<p>Telegram imposes limits on the API usage, which are dynamic and not strictly predictable. In practice, you get a <code>FLOODWAIT</code> exception after making a certain number of requests over a certain period of time. The exception would contain the amount of seconds you need to wait before making another request - this can be used to put the application to sleep until then.</p>"},{"location":"apps/4/#code","title":"Code","text":"<ul> <li><code>bases/w/database</code></li> <li><code>bases/w/telegram</code></li> <li><code>bases/w/config</code></li> <li><code>components/w/telegram_scanner</code></li> </ul>"},{"location":"apps/4/#entities","title":"Entities","text":"<ul> <li><code>TgMessage</code> model mirrors <code>data.telegram_messages</code> DB table.</li> <li><code>TelegramChannel</code> model mirrors <code>public.telegram_channels</code> DB table.</li> </ul>"},{"location":"apps/4/#flow-diagram","title":"Flow Diagram","text":"<pre><code>flowchart TD\n    subgraph floodwait [FLOODWAIT]\n        flood0(Elevate FloodWait exception)\n        flood1(Set 'endpoint_disabled' to TRUE)\n        flood2(Read number of seconds from the exception meta)\n        flood3(Run 're_enable_endpoint_after_delay')\n        enable_endpoints(Set 'endpoint_disabled' to FALSE)\n        wait(Wait amount of time required by Telegram API)\n        flood0 --&gt; flood1 --&gt; flood2 --&gt; flood3 --&gt; wait --&gt; enable_endpoints\n    end\n    subgraph init [Initialize service]\n    direction LR\n        step21(Create Pyrogram client)\n        step22(Create database client)\n        step23(Start the Telegram service)\n        step21 --- step22 --- step23\n    end\n    subgraph process [ ]\n        step31(Fetch most outdated channel from DB)\n        step32(Read ID of the latest channel message, or set to 1)\n        step33(Define range of messages IDs to fetch)\n        step34(Use Telegram API to fetch the messages)\n        step35(Filter out messages with 'empty' = True)\n        step36(Parse the messages into db objects)\n        step37(Save the orig ID of the most recent of the new messages)\n\n        step31 --&gt; step32 --&gt; step33 --&gt; step34 --&gt; step35 --&gt; step36 --&gt; step37\n    end\n    subgraph resolve [ ]\n        step41(Fetch unresolved channel from DB)\n        step42(Fetch channel data from Telegram API)\n        step43(Fetch channel's 1st message from Telegram API)\n        step44(Query Telemetr for channel data)\n        step45(Create instance of TgChannelResolve)\n        step46(Set status DEFUNCT on all channels with this TelemetrID)\n        step47(Update record with this handle and status ACTIVE with data collected)\n        subgraph fetch_channel_data [ ]\n            step42 --- step43 --- step44\n        end\n        step41 --&gt; fetch_channel_data --&gt; step45 --&gt; step46 --&gt; step47\n    end\n    subgraph persist [ ]\n        persist1(Persist the messages)\n        persist2(Update channel with 'last_scanned_on' and 'last_known_msg_id')\n        persist1 --- persist2\n    end\n\ntrigger_update(Trigger update endpoint)\ntrigger_resolve(Trigger resolve endpoint)\ncheck_disabled(Read 'endpoint_disabled')\nno_updates(Update channel with 'last_scanned_on')\nmark_dead(Mark channel as dead)\nresolved(Consider channel to be resolved)\n\ntrigger_update --&gt; check_disabled -- wait if needed --&gt; init -- Run update --&gt; process\nprocess -- Updates not found --&gt; no_updates\nprocess -- Updates found --&gt; persist\nprocess -- UsernameInvalid\\nUsernameNotOccupied --&gt; mark_dead\nprocess --&gt; floodwait\n\ntrigger_resolve --&gt; check_disabled\ninit -- Run resolve --&gt; resolve --&gt; resolved\nresolve -- UsernameInvalid\\nUsernameNotOccupied --&gt; mark_dead\nresolve --&gt; floodwait\n</code></pre>"},{"location":"apps/5/","title":"ARR-5 SyncManager","text":""},{"location":"apps/5/#general","title":"General","text":"<p>SyncManager is a composite application that serves to host endpoints for flows that are either temporary or ad-hoc in nature, or too small to have their own applications.</p> <p>Currently, SyncManager provides following subcomponents:</p> <ul> <li>Queue service</li> <li>Relevance service</li> <li>Smart Adder</li> <li>Theory Sync</li> <li>Voiceprint Saver</li> <li>Tranfactory migration</li> <li>Telegram migration</li> </ul>"},{"location":"apps/5/#code","title":"Code","text":"<ul> <li><code>bases/w/auth</code></li> <li><code>bases/w/database</code></li> <li><code>bases/w/storage</code></li> <li><code>bases/w/config</code></li> <li><code>bases/w/sync_management</code></li> <li><code>components/w/sync_manager</code></li> </ul>"},{"location":"apps/5/#entities","title":"Entities","text":"<p>Note</p> <p>See lists for specific subcomponents.</p>"},{"location":"apps/5/#flow-diagram","title":"Flow Diagram","text":"<p>See diagrams for specific subcomponents.</p>"},{"location":"apps/5/#history","title":"History","text":""},{"location":"apps/6/","title":"ARR-6 SyncManager ::: Queue Service","text":""},{"location":"apps/6/#general","title":"General","text":"<p>Queue service is meant to periodically scan certain DB tables looking for incomplete jobs and schedule them for other services to pick up.</p> <p>Queue service supplies following other services:</p> <ul> <li>Relevance service. Targets (channels), episodes (videos) and articles are collected to define their relevance.</li> <li>Transsuply2. Media episodes are collected to be downloaded for Transfactory.</li> </ul>"},{"location":"apps/6/#code","title":"Code","text":""},{"location":"apps/6/#entities","title":"Entities","text":""},{"location":"apps/6/#flow-diagram","title":"Flow Diagram","text":""},{"location":"apps/6/#history","title":"History","text":""},{"location":"apps/7/","title":"ARR-7 SyncManager ::: Relevance Service","text":""},{"location":"apps/7/#general","title":"General","text":""},{"location":"apps/7/#code","title":"Code","text":""},{"location":"apps/7/#entities","title":"Entities","text":""},{"location":"apps/7/#flow-diagram","title":"Flow Diagram","text":""},{"location":"apps/7/#history","title":"History","text":""},{"location":"apps/8/","title":"ARR-8 SyncManager ::: Smart Adder","text":""},{"location":"apps/8/#general","title":"General","text":""},{"location":"apps/8/#code","title":"Code","text":""},{"location":"apps/8/#entities","title":"Entities","text":""},{"location":"apps/8/#flow-diagram","title":"Flow Diagram","text":""},{"location":"apps/8/#history","title":"History","text":""},{"location":"apps/9/","title":"ARR-9 SyncManager ::: Theory Sync","text":""},{"location":"apps/9/#general","title":"General","text":""},{"location":"apps/9/#code","title":"Code","text":""},{"location":"apps/9/#entities","title":"Entities","text":""},{"location":"apps/9/#flow-diagram","title":"Flow Diagram","text":""},{"location":"apps/9/#history","title":"History","text":""},{"location":"apps/template/","title":"TEMPLATE","text":""},{"location":"apps/template/#general","title":"General","text":""},{"location":"apps/template/#code","title":"Code","text":""},{"location":"apps/template/#entities","title":"Entities","text":""},{"location":"apps/template/#flow-diagram","title":"Flow Diagram","text":""},{"location":"apps/template/#history","title":"History","text":""},{"location":"architecture/1/","title":"Context","text":"<p>A long and tedious conflict between Russia and Ukraine broke into an acute phase on February 24, 2022, when Russia started the invasion of Ukraine from several directions at the same time.</p> <p>Simultaneously (perhaps, even prior to the event), the machine of Russian propaganda significantly ramped up its activity. Political talk shows, already abundant in the Russian media space, became prevalent, almost pushing out the entertaining content entirely.</p> <p>The Russian propaganda machine is a loose amalgamation of media outlets (TV channels, radio stations, internet platforms, Telegram channels) and personalities, who apply effort at spreading, re-affirming, maintaining and developing the narratives invented by a small circle of most exclusive people, including Vladimir Putin, Sergey Lavrov, Ramzan Kadyrov, Dmitry Kiselyov, Margarita Simonyan and Vladimir Solovyov. This system is fueled by massive financial allocations, and counts over one thousand willing participants (registered as of the moment of me writing this).</p> <p>The motivation for keeping tabs on them is as follows:</p> <ul> <li>The impact of propaganda is significant in any war, and totally overwhelming in this one. Even though it is directed mainly against the native Russian population to keep them from revolting, the resulting damage to Ukrainian people is beyond imagination;</li> <li>Every single person who took part in a propagandistic activity with a media component starting with February 2022 (at least) is responsible for a share of that damage. This share is different for each, and depends on multiple factors, such as the frequency of TV appearances, social media following, etc. (a complete list is to be compiled at a later date);</li> <li>Everyone who bears a share of this responsibility must be held accountable at some point;</li> <li>To make sure it's even possible, every bit of useful information relevant to every person responsible must be collected and stored in a way that would make the retrieval and usage of this information as simple for researchers and investigators as possible.</li> </ul>"},{"location":"architecture/1/#decision","title":"Decision","text":"<p>A database was created to capture all the relevant data. It was decided to use relational model, and specifically PostgreSQL database engine for the advanced features it offers (such as <code>full text search</code>).</p> <p>Supabase was selected to host the database. It's a relatively new service that is an alternative to Firebase, is build around PostgreSQL DB, and offers a range of advanced features and services (including in-built AWS Storage).</p> <p>To provide a publicly accessible interface for the data collected, a service application was created in Python using the <code>web2py</code> framework. (web2py book)</p> <p>The application is hosted on Railway (Railway documentation).</p>"},{"location":"architecture/1/#status","title":"Status","text":"<p>Succeeded by DVE-A-52.</p>"},{"location":"architecture/1/#consequences","title":"Consequences","text":"<p>Due to Supabase being quite new, their Python wrapper does not have all the functionality of the general library, the general library itself is being actively developed, and the documentation is far from ideal. This may cause delays in solving certain kinds of problems.</p> <p>Web2py architecture, although very efficient, somewhat limits the selection of tools that could be used in the project (for example, adding <code>Poetry</code> (dependency management tool) proved to be too much of a nuisance).</p>"},{"location":"architecture/10/","title":"\ud83d\uddf8 ARD-10. Photos","text":""},{"location":"architecture/10/#context","title":"Context","text":"<p>Images are an intrinsic part of the project, and as such, they need to be properly stored and handled. Initial temporary solution, has drawbacks, such as limited scalability, hence a more robust approach is in order.</p>"},{"location":"architecture/10/#decision","title":"Decision","text":""},{"location":"architecture/10/#reference-set","title":"Reference set","text":"<ul> <li>A set of photos in their original sizes that is to serve as a reference point for the project in general.</li> <li>Reference set is to be stored on a private bucket (<code>reference-id-photos</code>). File format is <code>.png</code>.</li> <li>Images of the reference set are NOT to be used in the project directly.</li> <li>New photos are added to the reference set, following the naming format <code>000000id_fullname_en</code>, where the <code>id</code> part is patient's database ID with as many additional zeros as needed to make it 8 characters long; and <code>fullname_en</code> must be the same as the one stored in the database in the corresponding field, lowercase, with non-conventional characters replaced via the <code>unicodedata.normalize</code>.</li> </ul>"},{"location":"architecture/10/#resized-images","title":"Resized images","text":"<ul> <li>Resized images are to be stored in <code>.jpg</code> format.</li> <li>File names for the resized images are to follow this format: <code>000000id.jpg</code>, where  the <code>id</code> part is a patient's database ID with as many additional zeros as needed to make it 8 characters long;</li> <li>Resized images are to be stored in the separate <code>photos</code> bucket, in a folder containing three (3) subfolders, 2 of them corresponding to size presets (<code>thumbnails</code> for thumbnails and <code>large</code> for the primary version), and the 3rd containing the default images, which are to be shown for every patient without a photo.</li> </ul>"},{"location":"architecture/10/#adding-new-images","title":"Adding new images","text":"<ul> <li>A: When a new person is added to the database, and a face photo for them is available:</li> <li>An <code>add person with photo</code> flow includes 3 main steps:     https://github.com/betterthanever2/wapaganda_tools/blob/master/umisc/add_person.py<ul> <li>Adding a patient to the table <code>people</code>.</li> <li>Uploading reference image;</li> <li>Generating a patient photoset and uploading the resized images, also coupled with writing corresponding records to the <code>photos</code> and <code>people_on_photos</code> tables.</li> </ul> </li> <li>B: When a face photo is found for a person already in db but so far without a face photo / When a face photo needs to be replaced</li> <li>An <code>add/update person photo</code> flow includes 4 main steps:     https://github.com/betterthanever2/wapaganda_tools/blob/master/umisc/photo_upsert.py<ul> <li>Generating a patient photoset;</li> <li>Upload each of the 3 photos (including the reference one) with replacement if necessary;</li> <li>Insert corresponding records to the tables if necessary.</li> </ul> </li> <li>C: Non-face photo</li> <li>A separate ADR would be devised as an addition to this one, or a replacement thereof.</li> </ul>"},{"location":"architecture/10/#status","title":"Status","text":"<p>Accepted.</p>"},{"location":"architecture/10/#consequences","title":"Consequences","text":"<ul> <li>Since the photos are stored in Supabase Storage, their availability is subject to the availability of the service.</li> <li>Reference photo set serves as both a backup (also providing non-compressed versions ~ <code>png</code>), and, with the Supabase UI, a way to work with the photos manually, should such need ever arises.</li> </ul>"},{"location":"architecture/11/","title":"\ud83d\uddf6 ADR-11. Database Stats","text":""},{"location":"architecture/11/#context","title":"Context","text":"<p>Early version of the W features a \"Useless Stats\" block, which contained a few basic computed numbers on the database in general, and for each person specifically. The final version of the product should as well include statistical figures. This document is to serve as a primer on this subject.</p>"},{"location":"architecture/11/#decision","title":"Decision","text":"<p>Following stats are to be computed.</p>"},{"location":"architecture/11/#general","title":"General","text":"Stats Data Notes Number of patients (population) Average/Median age across the db Number of registered media hours Number of transcripted media hours Number of registered media segments Number of relevant organizations Number of Telegram channels on file Number of written works on file (articles, books) Current day of war"},{"location":"architecture/11/#per-person","title":"Per Person","text":""},{"location":"architecture/11/#status","title":"Status","text":"<p>Rejected. Not ADR material</p>"},{"location":"architecture/11/#consequences","title":"Consequences","text":"<p>New functions should be written to compute the stats.</p>"},{"location":"architecture/12/","title":"\ud83d\uddf6\u227d ADR-12. Wapaganda Structure [2]","text":""},{"location":"architecture/12/#context","title":"Context","text":"<p>DVE-A-2 describes the structure of the project as it was conceived, and that should now be referred to as <code>v0</code>. Current document succeeds it, and describes the structure of Wapaganda <code>v1</code>, which is being developed by @atatatko.</p>"},{"location":"architecture/12/#decision","title":"Decision","text":""},{"location":"architecture/12/#backend","title":"Backend","text":"<p>We use Django as a primary backend framework, and Django Rest Framework as a primary API framework. However, we do not use Django views for page generation, rather we serve the React web app as a set of static files, which serves as a client for the API, populating data about personalities, organizations and other entities.</p> <p>The API is not used for managing users, authentication, authorization, or creating/updating data, meaning the application itself is read-only. However, there is an admin panel for the API, which is used to manage data in the database, located at https://api.subjective.agency/admin/</p>"},{"location":"architecture/12/#frontend","title":"Frontend","text":"<p>We use React as a primary frontend framework, as it offers active support, large community, documentation and examples. We minimize use of 3rd party dependencies, as we don't have much time to maintain security and vulnerability updates. React and JS offer pretty much everything we need: asynchronous REST client, localization, working with local cache and database.</p>"},{"location":"architecture/12/#status","title":"Status","text":""},{"location":"architecture/12/#backend_1","title":"Backend","text":"<p>As of mid-April 2023, the backend could be considered a stable beta, REST API core is already created and tested, which reduces further development to adding new endpoints and new types of requests. However, any changes in database schema require significant work on backend side, so release could be possible only when we have a more or less stable schema.</p>"},{"location":"architecture/12/#frontend_1","title":"Frontend","text":"<p>As of mid-April 2023, frontend Single-Page Application (SPA) is a solid alpha, with partially implemented design and localization, and fully implemented caching, searching tools and HTTP client. Frontend does not depend much on any database schema changes. The core of REST API protocol is already implemented.</p>"},{"location":"architecture/12/#consequences","title":"Consequences","text":"<p>Relying on Django and React is beneficial in the sense that both frameworks are thriving ecosystems, so solving a specific issue would take less time on average and would be relatively simpler. We should keep in mind, however, that both Django and React have vulnerabilities and limitations, which are applicable to the W project.</p>"},{"location":"architecture/13/","title":"\u221e ADR-13. Media Segments Primer","text":"<p>Parent to DVE-A-27: \ud83d\uddf8 ADR-6: Transcripts, DVE-A-29: \u221e ADR-7: Media Episodes Interface, DVE-A-30: \u221e ADR-8: Transcript Mark-Up</p>"},{"location":"architecture/13/#context","title":"Context","text":"<p>The domain of media activity covers numerous media platforms, each of which has its own specific peculiarities and context and requires a custom approach. This document describes a working model that encapsulates all relevant nuances of the terrain.</p>"},{"location":"architecture/13/#decision","title":"Decision","text":""},{"location":"architecture/13/#entities","title":"Entities","text":""},{"location":"architecture/13/#media-platforms","title":"Media Platforms","text":"<p>Media platform is an online service that allows a subset of its users to publish media items such as videos or audios for public access.</p> <p>In the database, platforms are represented with <code>enums.media_platforms</code> table.</p>"},{"location":"architecture/13/#media-targets","title":"Media Targets","text":"<p>Media target, in the context of the system, is a generic term for a platform-dependent account entity, such as <code>channel</code> on YouTube, RuTube etc., or <code>brand</code> on Smotrim.ru.</p> <p>In the database, targets are represented with platform-dependent tables:</p> <ul> <li>youtube_channels</li> <li>smotrim_brands</li> <li>rutube_channels</li> <li>komso_categories</li> <li>ntv_categories</li> <li>dentv_categories</li> </ul>"},{"location":"architecture/13/#media-episodes","title":"Media Episodes","text":"<p>Media episode is a specific media item, such as video, published on a media platform, under a media target, for public access.</p> <p>In the database, media episodes are collected into platform-specific tables:</p> <ul> <li>youtube_vids</li> <li>smotrim_episodes</li> <li>komso_episodes</li> <li>ntv_episodes</li> <li>dentv_episodes</li> <li>rutube_vids</li> </ul> <p>Media episode is connected to a target in corresponding table via a foreign key.</p>"},{"location":"architecture/13/#media-segments","title":"Media Segments","text":"<p>Platforms, targets and episodes are organized into a set of similarly structured hierarchies. Media segment is a distinct series of media episodes that could be published across multiple platforms and targets while retaining sequence within the series. Since numerous platforms and targets could be involved, the implementation provides a layer of abstraction on top of that set, allowing cross-platform matching.</p> <p>In the database, media segments are collected in <code>media_segments</code> table, and connect to the platform hierarchies on the episode level via <code>segment_id</code> attribute.</p> <p>There are several distinct types of media segments:</p> <ul> <li><code>named</code> is the most common type, and represents a sequence of episodes intentionally designed to be a part of it.</li> <li><code>composite</code> type represents a situation, when a segment is published across a number of targets, usually on a single platform, and usually territory-specific.</li> <li><code>personal</code> is person-specific catch-all segment that encapsulates all media episodes that could be tied to a specific author and don't belong to any of the named segments.</li> <li><code>complex</code> is expected to be tangled with a number of other segments.</li> </ul>"},{"location":"architecture/13/#media-episodes-clusters","title":"Media Episodes Clusters","text":"<p>Clusters are another layer of abstraction required to account for duplicates accross platforms/targets without dismissing them. We don't want to dismiss them because, in the context of the project, duplicates (provided they weren't a result of our internal system's malfunction) still work to increase the propaganda coverage.</p> <p>In the database, clusters of media episodes are collected in <code>media_episodes_clusters</code> table, and connect  to the platform hierarchies on the episode level via <code>cluster_id</code> attribute.</p> <p>When first added, a media episode is not assigned to any cluster (i.e. has <code>cluster_id</code> attribute = null). If it is determined to be relevant, new entry is added to the <code>service.clustering</code> table, which serves as a job queue for the clustering service.</p>"},{"location":"architecture/13/#services","title":"Services","text":""},{"location":"architecture/13/#update-service","title":"Update Service","text":"<ul> <li><code>Media segment</code> serves as a starting point. Its database representation is loaded with <code>MediaSegmentService</code> class, which fetches all relevant <code>platform_links</code> (i.e. <code>MediaSegmentOnPlatform</code> objects, many-to-many mapping between <code>segments</code> and <code>targets</code>), iterates over them and shapes a list of <code>targets</code>, i.e. subclass instances of <code>MediaPlatformService</code>.</li> <li>The list of <code>targets</code> is iterated over, calling on each <code>fetch_and_parse_updates</code> method that would apply target-specific logic to collect the updates.</li> <li>After collecting updates, a new record is created in <code>service.media_queue</code> table with <code>job_type</code> set to <code>update</code>.</li> <li>By default, all new items are persisted with <code>cluster_id</code> set to <code>null</code> and <code>relevance_status_id</code> set to <code>2</code> (<code>undefined</code>), in order for respective services to pick them up for resolution.</li> <li>If the type of <code>segment</code> is <code>complex</code>, then <code>segment_id</code> attribute will be nullified. These episodes would later be resolved with segment detection service.</li> </ul>"},{"location":"architecture/13/#clustering-service","title":"Clustering Service","text":"<ul> <li>Clustering service picks up records from the <code>service.media_queue</code> table with <code>job_type</code> = <code>clustering</code>, and collects existing media clusters into <code>scope</code> parameter.</li> <li>Then relevant episode is compared to each item in the scope.</li> <li>If no match is detected, the service creates new <code>cluster</code>. </li> <li>If one match is found, episode's <code>cluster_id</code> is updated correspondingly. </li> <li>If more than one match is found, a new record is created in <code>service.media_conflicts</code> table, with information about the episode and all matching clusters.</li> </ul>"},{"location":"architecture/13/#matching","title":"Matching","text":"<p>Matching takes into account <code>title</code>, <code>description</code> (if present) and <code>duration</code> (if present). </p>"},{"location":"architecture/13/#stats-collection-service","title":"Stats Collection Service","text":"<p>TBD</p>"},{"location":"architecture/13/#media-queue-service","title":"Media Queue Service","text":"<p>Queue service is meant to create media jobs (records in <code>service.media_queue</code> table) where they cannot be created in a simpler way, and specifically for:</p> <ul> <li>Clustering service;</li> <li>Relevance service;</li> <li>Initial jobs for update service: in the regular flow, new jobs are schedule by the update service itself, but when a new segment is added, they need to be inserted into that flow.</li> </ul>"},{"location":"architecture/13/#relevance-service","title":"Relevance Service","text":"<p>TBD</p>"},{"location":"architecture/13/#segment-detection-service","title":"Segment Detection Service","text":"<p>TBD</p>"},{"location":"architecture/13/#interface-enabled-entrypoints","title":"Interface-Enabled Entrypoints","text":"<p>There are several points in this system where the need for manual intervention is inescapeable.</p> <ul> <li>The clustering process may produce conflicting matching results;</li> <li>Segments may not be linked to any of the targets / A target may not be assigned to any of the segments.</li> <li>Items may have been clustered together that aren't actually related;</li> <li>...</li> </ul> <p>For these, UI-enabled routes should be developed in the <code>ave_media</code> application, that would allow resolution of those issues.</p>"},{"location":"architecture/13/#status","title":"Status","text":"<p>Cooking</p>"},{"location":"architecture/13/#consequences","title":"Consequences","text":""},{"location":"architecture/14/","title":"\ud83d\uddf8 ADR-14. Printed Data","text":""},{"location":"architecture/14/#context","title":"Context","text":"<p>There is a subset of patients with a history of writing books, articles, poems, etc. many of which were subsequently published. A significant portion of what was published by them has direct relation to the mission of the W project in that they promote, in one way or another, the messages of the Russian propaganda. This printed data = an umbrella term that incorporates books, articles, blog posts, etc. - should be properly stored in the DB and used in the project.</p>"},{"location":"architecture/14/#decision","title":"Decision","text":"<p>Following DB setup was implemented to store the printed data:</p> <ul> <li>Table <code>public.printed</code> stores primary entities (books, articles, etc.);</li> <li>Table <code>public.printed_to_people_mapping</code> stores authorship relationships;</li> <li>Table <code>data.printed_content</code> stores the content of respective printed items, divided by chapters, one record per raw chapter;</li> <li>For EPUB files, raw chapter is defined as a distinct part of the archive with internal type <code>chapter</code>.     However, it should be taken into account that many EPUB files have a suboptimal structure; for example, an actual chapter in the file could be split into several separate raw chapters, or several actual chapters could be merged into a single raw chapter.</li> </ul> <p><code>printed_content</code> table also has a field <code>parsed_content</code> which is supposed to contain the edited version of the <code>raw_content</code> field. Editing in this context means 2 things: 1) either split or merge, depending on the specific situation; 2) cleaning up - removing garbage code (such as <code>&lt;p class=\"empty-line\"&gt;</code>), extra spaces, extra newlines, etc.</p> <p>When editing, additional precautions should be taken to avoid accidental alteration of the actual text of the printed piece.</p>"},{"location":"architecture/14/#required-routines","title":"Required routines","text":"<p>The following use cases should be handled in the future:</p> <ol> <li>Book was added by accident: remove all artifacts (in this order)</li> <li>items from <code>public.printed_to_people_mapping</code></li> <li>items from <code>data.printed_content</code></li> <li>items from <code>public.printed</code></li> <li>object from storage</li> <li>Author was not properly identified: indicate author manually</li> <li>Chapter is actually a piece of the previous one: when editing the raw HTML, the clean text should be merged with the clean text of the 1st record pertaining to the chapter in question, so that only one chapter record contains the full clean text.</li> </ol> <p>This initial infrastructure allows for storage of the content, but in order to use it in the project, <code>parsed_content</code> should be produced first.</p>"},{"location":"architecture/14/#status","title":"Status","text":"<p>Accepted.</p>"},{"location":"architecture/14/#consequences","title":"Consequences","text":"<p>Even more data.</p>"},{"location":"architecture/15/","title":"\ud83d\uddf6 ADR-15. Glossary","text":""},{"location":"architecture/15/#context","title":"Context","text":"<p>Continuing research into the subject of Russian propaganda and description thereof requires certain terminology that might not be immediately obvious to an external user. This could be either existing terms of some narrow application, or terms created/adapted for the purposes of the W project.</p>"},{"location":"architecture/15/#decision","title":"Decision","text":"<p>Glossary, a special document listing these special terms, must be created as part of the <code>Theory</code> section of the W web app.</p>"},{"location":"architecture/15/#status","title":"Status","text":"<p>Rejected. Not ADR material</p>"},{"location":"architecture/15/#consequences","title":"Consequences","text":"<p>Glossary is expected to improve external user's experience with regard to understanding the essence of the W approach. However, there must be an additional solution for automatically identifying terms included into the glossary and attaching corresponding links to them.</p>"},{"location":"architecture/16/","title":"\ud83d\uddf6\u227d ADR-16. Theory","text":""},{"location":"architecture/16/#context","title":"Context","text":"<p>All the textual items relevant for the W project can be put into one of the following categories:</p> <ol> <li><code>core</code> : Texts written specifically for the W project, or for the Dum Vita Est blog, its predecessor. Most of these describe various aspects of the <code>Copium Theory of Propaganda</code>.</li> <li><code>companions</code> : Texts written by independent authors that contribute to the elucidation and research of propaganda in general, of Russian propaganda instance, or other things related in any way to the subject area of the W project.</li> <li><code>copium</code> : Texts written by Russian propagandists that shed light on their line of thinking.</li> </ol>"},{"location":"architecture/16/#decision","title":"Decision","text":"<p>In the context of the W project, <code>theory</code> is a special section that consists of 3 text collections named <code>core</code>, <code>companions</code> and <code>copium</code>. <code>Theory</code> is a top-level navigation target on the W website, alongside <code>people</code>, <code>organizations</code> and <code>media segments</code>.</p> <p>The initial body of <code>companion</code> and <code>copium</code> texts was translated from Russian into English for the <code>Dum Vita Est</code> blog, and, in April 2023, translated from Russian into Ukrainian using <code>openai/gpt-3.5</code>.</p> <p>Going forward, translations from one language to another should be delegated entirely to AI, with human responsible for 1) selecting translation targets and preparing prompts; and 2) editing and verifying the AI-generated output.</p> <p>Both content and metadata for all the theory items are stored in the <code>theory</code> table (<code>public</code> schema), with the following structure:</p> <pre><code>{\n  \"title\": {\n    \"en\": \"\",\n    \"ru\": \"\",\n    \"uk\": \"\"\n  },\n  \"type\": \"\",\n  \"excerpt\": {\n    \"en\": \"\",\n    \"ru\": \"\",\n    \"uk\": \"\"\n  },\n  \"images\": [],\n  \"content\" : {\n    \"en\": {\n      \"translated_by\":  {\"i\":  \"\", \"author\":  \"\"},\n      \"markdown\":  \"\",\n      \"edit_history\": [{\"who\":  \"\", \"when\":  \"\"}]\n    },\n    \"ru\": {\"markdown\":  \"\"},\n    \"uk\": {\n      \"translated_by\":  {\"i\":  \"\", \"author\":  \"\"},\n      \"markdown\":  \"\",\n      \"edit_history\": [{\"who\":  \"\", \"when\":  \"\"}]\n    }\n  },\n  \"original_content_metadata\": [\n    {\"date_published\": \"\",\"url\": \"\", \"author\":  \"\"}\n  ]\n}\n</code></pre> <ul> <li><code>type</code> is a string, one of <code>core</code>, <code>copium</code>, <code>companion</code>, <code>system</code>;</li> <li><code>system</code> is a set of auxiliary articles, including <ul> <li>FAQ, </li> <li>About Page, and </li> <li>Glossary.</li> </ul> </li> <li><code>title</code> and <code>excerpt</code> are triple-lang JSONBs with keys containing plain text;</li> <li><code>images</code> is a list of URLs by the look of <code>https://svfizyfozagyqkkjzqdc.supabase.co/storage/v1/object/public/photos/theory/default/default-thumb.png</code></li> <li><code>original_content_metadata</code> is a list of JSONBs, each element containing at least keys</li> <li><code>date_published</code></li> <li><code>author</code>, and</li> <li><code>url</code> where possible;</li> <li><code>content</code> is a triple-lang JSONB with keys containing dictionaries. If a lang key refers to the original text, it contains only <code>markdown</code> key; otherwise, it also contains <code>translated_by</code> and <code>edit_history</code> keys.</li> <li><code>markdown</code> contains complete text of the item in Markdown format;</li> <li> <p><code>translated_by</code> is a dictionary with 2 required keys:</p> <ul> <li><code>i</code> stands for <code>intelligence</code> and can be either <code>a</code> (<code>artificial</code>) or <code>h</code> (<code>human</code>);</li> <li><code>author</code> contains original name of the author or reference to the AI;</li> </ul> <p>If this key is absent, this is an original (unless something went terribly wrong).   * <code>edit_history</code> is meant to serve as a minimalistic edit log. It's a list of dictionaries with (preliminary) two required keys: * <code>who</code> contains name of the editor; * <code>when</code> - date of the edit.</p> <p>If this key is absent, the text was not edited. If this key is present but <code>translated_by</code> key is absent, something went wrong.</p> </li> </ul>"},{"location":"architecture/16/#status","title":"Status","text":"<p>Cooking.</p>"},{"location":"architecture/16/#consequences","title":"Consequences","text":""},{"location":"architecture/17/","title":"\ud83d\uddf8 ADR-17. Taxonomies. ISCO-08","text":""},{"location":"architecture/17/#context","title":"Context","text":"<p>As the project aims to ground its methods and approaches in existing and accepted practices, it makes sense to adopt some relevant taxonomies that have been developed by the world community. The first of these would be ISCO-08 taxonomy that provides a hierarchical description of occupations and professions.</p> <p>ISCO-08 was adopted in 2008; it replaced ISCO-88, the previous version. ISCO-08 is currently under revision, with the updated version to be released some time between 2028 and 2030. It is maintained by the ILO (International Labor Organization).</p> <p>ISCO standard is based on the concept of skill, which is numerical and can be between 1 (lowest) and 4 (highest). ISCO-08 is composed of 10 major groups, 43 sub-major groups, 130 minor groups and 436 unit groups, i.e. has 4 levels in the hierarchy (note: the hierarchy levels DO NOT correspond directly to the skill levels).</p> <p>Each group in the hierarchy is indicated with an ISCO code. Code for major groups is 1 digit (i.e. 0 through 9), for sub-major groups - 2 digits, for minor groups - 3 digits, and for unit groups - 4 digits.</p> <p>For example, here's a part from the beginning of the hierarchy:</p> <pre><code>1   Managers\n\n11  Chief Executives, Senior Officials and Legislators\n111 Legislators and Senior Officials\n1111    Legislators\n1112    Senior Government Officials\n1113    Traditional Chiefs and Heads of Villages\n1114    Senior Officials of Special-interest Organizations\n112 Managing Directors and Chief Executives\n1120    Managing Directors and Chief Executives\n</code></pre> <p><code>Managers</code> is a major (aka, root) group indicated with <code>1</code>, <code>Chief Executives, Senior Officials and Legislators</code> is a sub-major group indicated with <code>11</code>, <code>Legislators and Senior Officials</code> and <code>Managing Directors and Chief Executives</code> are minor groups (<code>111</code> and <code>112</code> respectively), and all the rest are unit groups, indicated with 4-digit codes.</p> <p>A few things to note:</p> <ul> <li>Terms could be identical. The idea is that every branch must end with a unit group, so in the case when there is no way to determine subcategories, like with <code>112</code>, the unit group code will have <code>0</code> at the end and the term will be the same as with the parent category;</li> <li>Codes also start with <code>0</code>, so when processing, they should be always treated as strings, not numbers. FYI, category <code>0</code> is about <code>Armed Forces Occupations</code>.</li> </ul>"},{"location":"architecture/17/#decision","title":"Decision","text":"<p>In order to use the taxonomy efficiently, it should be stored in the database. There are several approaches to this: adjacency list model, path enumeration model, nested set model, and closure table model.</p> <p>Closure table model was selected for the project. This approach involves the use of an additional table to represent the relationships between nodes in the hierarchy. The closure table captures all the ancestor-descendant relationships between nodes, allowing for efficient querying and traversal of the hierarchy.</p> <p>Here's how it works:</p> <ol> <li>Main table: The main table represents the nodes in the hierarchy. It contains the primary key column and other attributes related to each node.</li> <li>Closure table: The closure table is an auxiliary table that stores the ancestor-descendant relationships between nodes. It consists of two columns, usually named <code>ancestor</code> and <code>descendant</code>, which are foreign keys referencing the primary key of the main table.</li> </ol> <p>For the ISCO-08 taxonomy, the following tables were created in the <code>enums</code> schema:</p> <ul> <li><code>isco08_taxonomy</code> - main table that contains term names, definitions and other attributes;</li> <li><code>isco08_taxonomy_closure</code> - closure table with all the relationships. It has 2 columns, <code>ancestor</code> and <code>descendant</code>, which combination serves as the primary key (i.e. must be unique);</li> <li><code>isco08_index</code> - a list of ~7,000 professions with assigned category codes. This table is intended for reference, when defining a patient's position in an organization.</li> </ul> <p>In addition, table <code>org_roles</code> in the same schema will be maintained for professions outside the taxonomy/index.</p> <p>Table <code>people_in_orgs</code> of the <code>public</code> schema would undergo following updates:</p> <ul> <li>existing column <code>role</code> is to be deprecated once the processing of existing data is finished;</li> <li>new column is added <code>role_category</code> that points to <code>enums.isco08_taxonomy</code>;</li> <li>new column is added <code>role_ref1</code> that points to <code>enums.isco08_index</code>;</li> <li>new column is added <code>role_ref2</code> that points to <code>enums.org_roles</code>;</li> <li>new column is added <code>role_details</code> (JSONB) with content being a dictionary with keys <code>of</code>, <code>for</code>, <code>at</code>, <code>from</code>, <code>with</code>, etc. (all optional), each of which, in its turn, holds a triple-lang structure (<code>{\"en\": \"\", \"ru\": \"\", \"uk\": \"\"}</code>). Example:</li> </ul> <pre><code>{\"at\": {\"en\": \"world history department\", \"ru\": \"\u0444\u0430\u043a\u0443\u043b\u044c\u0442\u0435\u0442 \u043c\u0438\u0440\u043e\u0432\u043e\u0439 \u0438\u0441\u0442\u043e\u0440\u0438\u0438\", \"uk\": \"\u0444\u0430\u043a\u0443\u043b\u044c\u0442\u0435\u0442 \u0441\u0432\u0456\u0442\u043e\u0432\u043e\u0457 \u0456\u0441\u0442\u043e\u0440\u0456\u0457\"}\n</code></pre> <p>The algorithm for populating the closure table would be saved in the <code>wapatools</code> repository (<code>development/onto/isco08.py</code>).</p>"},{"location":"architecture/17/#status","title":"Status","text":"<p>In progress.</p>"},{"location":"architecture/17/#consequences","title":"Consequences","text":"<p>The flow for adding a person-to-organization relationship becomes somewhat more complicated, since it requires identification of the profession code. To simplify this, a SQL function <code>search_in_isco8_index</code> was implemented that allows full text search on the <code>isco_08_index</code> table returning <code>term_id</code> (i.e. category ID in <code>isco_08_taxonomy</code>), <code>index_id</code> (i.e. ID of the profession in <code>isco_08_index</code>), as well as codes and definitions for reference.</p> <p>Existing reference (content of the <code>role</code> column) is to be reworked into the updated format (in progress).</p> <p>Taxonomy terms should be translated into Russian and Ukrainian.</p> <p>In the long term, rooting in the existing taxonomies elaborated this deeply, would bring a little more order to the data, allowing to query it more efficiently.</p>"},{"location":"architecture/18/","title":"Context","text":"<p>This set of rules is a meaningful subset of Google's and Mozilla's coding standards.</p>"},{"location":"architecture/18/#decision","title":"Decision","text":"<p>In order to keep codebase clean and organized, the following rules are suggested as a general guideline.</p>"},{"location":"architecture/18/#files","title":"Files","text":""},{"location":"architecture/18/#file-structure","title":"File structure","text":"<ul> <li>Use\u00a0<code>src</code>\u00a0folder for source files;</li> <li>Use\u00a0<code>test</code>\u00a0folder for test files;</li> <li>Use\u00a0<code>src/js</code>\u00a0folder for JS files;</li> <li>Use\u00a0<code>src/css</code>\u00a0folder for CSS files.</li> </ul>"},{"location":"architecture/18/#file-content","title":"File content","text":"<ul> <li>Use UTF-8 encoding for all files.</li> </ul>"},{"location":"architecture/18/#import","title":"Import","text":"<ul> <li>Use following order for import statements:</li> <li>import statements for React components;</li> <li>import statements for libraries;</li> <li>import statements for project files;</li> <li>Exactly 1 (one) blank line must separate existing sections;</li> <li>Import statements must not be line-wrapped;</li> <li>The <code>.js</code> file extension is  not  optional in import paths and must always be included:</li> </ul> <p><pre><code>// Wrong!\nimport '../directory/file';\n\n// Right\nimport '../directory/file.js';\n</code></pre> * Technically, ECMAScript specification allows import cycles between ES modules, but  don't do it .</p>"},{"location":"architecture/18/#export","title":"Export","text":"<ul> <li>Use named exports everywhere. The <code>export</code> keyword can be applied to a declaration, or used in the\u00a0<code>export {name};</code>\u00a0syntax:</li> </ul> <p><pre><code>// Wrong!\nexport default function () {\n  // ...\n}\nimport name from './file.js';\n\n// Right\nexport function name() {\n  // ...\n}\nimport {name} from './file.js';\n</code></pre> * Exported variables must not be mutated outside of module initialization.</p> <p><pre><code>// Wrong!\nimport {name} from './file.js';\nname = 'new name';\n</code></pre> * Do not use default exports. Importing modules must give a name to these values, which can lead to inconsistencies in naming across modules.</p> <pre><code>// Wrong!\nexport default function () {\n  // ...\n}\n</code></pre>"},{"location":"architecture/18/#file-content_1","title":"File content","text":""},{"location":"architecture/18/#naming","title":"Naming","text":"<ul> <li>Use\u00a0<code>FileName.js</code>\u00a0notation for JS files,\u00a0<code>file-name.css</code>\u00a0and\u00a0<code>file-name.html</code>\u00a0for CSS and HTML files</li> <li>Use\u00a0<code>variableName</code>\u00a0notation for JS variables and functions</li> <li>Use\u00a0<code>ClassName</code>\u00a0notation for JS classes</li> <li>Use\u00a0<code>CONSTANT_NAME</code>\u00a0notation for JS constants</li> </ul> <p>Give as descriptive a name as possible,\u00a0within reason.\u00a0Do not worry about saving horizontal space, as it is far more important to make your code immediately understandable by a new reader.\u00a0Do not use abbreviations that are ambiguous or unfamiliar to readers outside your project,\u00a0and do not abbreviate by deleting letters within a word.</p> <pre><code>// Incorrect\nn                   // Meaningless.\nnErr                // Ambiguous abbreviation.\nnCompConns          // Ambiguous abbreviation.\nwgcConnections      // Only your group knows what this stands for.\npcReader            // Lots of things can be abbreviated \"pc\".\ncstmrId             // Deletes internal letters.\nkSecondsPerDay      // Do not use Hungarian notation.\n\n// Correct\nerrorCount          // No abbreviation.\ndnsConnectionIndex  // Most people know what \"DNS\" stands for.\nreferrerUrl         // Ditto for \"URL\".\ncustomerId          // \"Id\" is both ubiquitous and unlikely to be misunderstood.\n</code></pre>"},{"location":"architecture/18/#formatting","title":"Formatting","text":"<ul> <li>Use clang-format. The JavaScript community has invested effort to make sure\u00a0<code>clang-format</code>\u00a0\"does the right thing\" on JavaScript files.\u00a0<code>clang-format</code>\u00a0has integration with several popular editors.</li> <li>Braces are required for all control structures (i.e.\u00a0<code>if</code>,\u00a0<code>else</code>,\u00a0<code>for</code>,\u00a0<code>do</code>,\u00a0<code>while</code>, as well as any others), even if the body contains only a single statement. The first statement of a non-empty block must begin on its own line.</li> </ul> <pre><code>// Incorrect\nif (test)\n  return false;\n\n// Incorrect\nif (test) return false;\n\n// Correct\nif (test) {\n  return false;\n}\n</code></pre> <ul> <li>One statement per line</li> <li>Each statement is followed by a line-break</li> <li>Semicolons are required</li> <li>Horizontal alignment: discouraged. Consider a future change that needs to touch just one line. This change may leave the formerly-pleasing formatting mangled, and that is allowed. More often, it prompts the coder (perhaps you) to adjust whitespace on nearby lines as well, possibly triggering a cascading series of re-formattings.</li> </ul> <pre><code>// Incorrect\nconst a     = 1;\nconst bb    = 2;\nconst ccc   = 3;\n\n// Correct\nconst a = 1;\nconst bb = 2;\nconst ccc = 3;\n</code></pre>"},{"location":"architecture/18/#variables","title":"Variables","text":"<ul> <li>Use\u00a0<code>const</code>\u00a0for all of your references by default</li> <li>Use\u00a0<code>let</code>\u00a0if needed</li> <li>Don't use\u00a0<code>var</code></li> <li>One variable per declaration</li> </ul> <pre><code>// Incorrect\nconst items = getItems(), goSportsTeam = true, dragonball = 'z';\n\n// Correct\nconst items = getItems();\nconst goSportsTeam = true;\nconst dragonball = 'z';\n</code></pre> <ul> <li>Declared when needed, initialized as soon as possible</li> <li>Local variables are not habitually declared at the start of their containing block or block-like construct. Instead, local variables are declared close to the point they are first used (within reason), to minimize their scope.</li> </ul>"},{"location":"architecture/18/#arrays","title":"Arrays","text":"<ul> <li>Do not use the variadic Array constructor. The constructor is error-prone if arguments are added or removed. Use a literal instead.</li> </ul> <pre><code>// Incorrect\nconst a1 = new Array(x1, x2, x3);\nconst a2 = new Array(x1, x2);\nconst a3 = new Array(x1);\nconst a4 = new Array();\n\n// This works as expected except for the third case: if x1 is a whole number \n// then a3 is an array of size x1 where all elements are undefined. \n// If x1 is any other number, then an exception will be thrown, \n// and if it is anything else then it will be a single-element array.\n\n// Correct\nconst a1 = [x1, x2, x3];\nconst a2 = [x1, x2];\nconst a3 = [x1];\nconst a4 = [];\n</code></pre> <ul> <li>Explicitly allocating an array of a given length using new Array(length) is allowed when appropriate.</li> </ul> <pre><code>const a1 = new Array(foo.length);\n</code></pre>"},{"location":"architecture/18/#objects","title":"Objects","text":"<ul> <li>Do not mix quoted and unquoted keys</li> <li>Object literals may represent either structs (with unquoted keys and/or symbols) or dictionaries (with quoted and/or computed keys). Do not mix these key types in a single object literal.</li> </ul> <pre><code>// Incorrect\n{\n  width: 42, // struct-style unquoted key\n  'maxWidth': 43, // dict-style quoted key\n}\n\n// Correct\n{\n  width: 42, // struct-style unquoted key\n  maxWidth: 43, // struct-style unquoted key\n}\n</code></pre> <ul> <li>Enumerations are defined by adding the @enum annotation to an object literal. Additional properties may not be added to an enum after it is defined. Enums must be constant, and all enum values must be deeply immutable.</li> </ul> <pre><code>/**\n * Supported temperature scales.\n * @enum {string}\n */\nconst TemperatureScale = {\n  CELSIUS: 'celsius',\n  FAHRENHEIT: 'fahrenheit',\n};\n</code></pre>"},{"location":"architecture/18/#strings","title":"Strings","text":"<ul> <li>Use single quotes for strings</li> <li>Use template strings instead of concatenation</li> </ul> <pre><code>// Incorrect\nconst name = 'Bob Parr';\nconst time = 12;\nconst greeting = 'Good afternoon, ' + name + ', it is ' + time + ' now.';\n\n// Correct\nconst name = 'Bob Parr';\nconst time = 12;\nconst greeting = `Good afternoon, ${name}, it is ${time} now.`;\n</code></pre> <ul> <li>Do not use line continuations (that is, ending a line inside a string literal with a backslash) in either ordinary or template string literals. Even though ES5 allows this, it can lead to tricky errors if any trailing whitespace comes after the slash, and is less obvious to readers.</li> </ul> <pre><code>// Incorrect\nconst longString = 'This is a very long string that \\\n    far exceeds the 80-column limit. It unfortunately \\\n    contains long stretches of spaces due to how the \\\n    continued lines are indented.';\n\n// Correct\nconst longString = 'This is a very long string that ' +\n    'far exceeds the 80-column limit. It does not ' +\n    'contain long stretches of spaces since the ' +\n    'continued lines are indented.';\n</code></pre>"},{"location":"architecture/18/#functions","title":"Functions","text":"<ul> <li><code>Parameter name</code> comments should be used whenever the value and method name do not sufficiently convey the meaning, and refactoring the method to be clearer is infeasible. Preferred format: 1) before the value, 2) includes <code>=</code> sign:</li> </ul> <pre><code>someFunction(obviousParam, /* shouldRender= */ true, /* name= */ 'hello');\n</code></pre> <ul> <li>Functions may contain nested function definitions. If it is useful to give the function a name, it should be assigned to a local constant</li> <li>Arrow functions provide a concise function syntax and simplify scoping <code>this</code> for nested functions. Prefer arrow functions over the function keyword, particularly for nested functions</li> </ul> <pre><code>/**\n * Arrow functions can be documented just like normal functions.\n * @param {number} numParam A number to add.\n * @param {string} strParam Another number to add that happens to be a string.\n * @return {number} The sum of the two parameters.\n */\nconst moduleLocalFunc = (numParam, strParam) =&gt; numParam + Number(strParam);\n</code></pre> <ul> <li>Default parameters are supported but discouraged</li> <li>Optional parameters are permitted using the <code>=</code> operator in the parameter list. Optional parameters must include spaces on both sides of the<code>=</code> operator, be named exactly like required parameters (i.e., not prefixed with\u00a0<code>opt_</code>), use the\u00a0<code>= suffix</code>\u00a0in their JSDoc type, come after required parameters, and not use initializers that produce observable side effects. All optional parameters for concrete functions must have default values, even if that value is undefined. In contrast to concrete functions, abstract and interface methods must omit default parameter values.</li> </ul>"},{"location":"architecture/18/#classes","title":"Classes","text":"<ul> <li>Type names are typically nouns or noun phrases. For example,\u00a0<code>Request</code>,\u00a0<code>ImmutableList</code>, or\u00a0<code>VisibilityMode</code></li> <li>Interface names may sometimes be adjectives or adjective phrases instead (for example,\u00a0<code>Readable</code>)</li> <li>Method names are typically verbs or verb phrases. For example,\u00a0<code>sendMessage</code>\u00a0or\u00a0<code>stop</code>_</li> <li>Getter and setter methods for properties are never required, but if they are used they should be named\u00a0<code>getFoo</code>,\u00a0<code>isFoo</code>\u00a0or\u00a0<code>hasFoo</code>\u00a0for booleans</li> <li>Constructor, static and prototype methods, and properties must be documented with the <code>@constructor</code>, <code>@static</code>, <code>@method</code>, and <code>@type</code> annotations respectively.</li> <li>Constructors are required for all ES6 classes. The constructor may be omitted only if the class is a mixin or a base class.</li> <li>Set all of a concrete object\u2019s fields (i.e. all properties other than methods) in the constructor. Otherwise, VMs\u2019 ability to optimize is constrained.</li> </ul> <pre><code>class Foo {\n  constructor() {\n    /** @private @const {!Bar} */\n    this.bar_ = computeBar();\n\n    /** @protected @const {!Baz} */\n    this.baz = computeBaz();\n  }\n}\n</code></pre> <ul> <li>Where it does not interfere with readability, prefer module-local functions over private static methods</li> <li>Do not use JavaScript <code>get</code> and <code>set</code> class properties. They are potentially surprising and difficult to reason about, and have limited support in the compiler. Provide ordinary methods instead.</li> </ul> <pre><code>// Incorrect\nclass C {\n  get foo() {\n    return this.foo_;\n  }\n  set foo(val) {\n    this.foo_ = val;\n  }\n}\n\n// Correct\nclass C {\n  getFoo() {\n    return this.foo_;\n  }\n  setFoo(val) {\n    this.foo_ = val;\n  }\n}\n</code></pre>"},{"location":"architecture/18/#jsdoc","title":"JSDoc","text":"<ul> <li>JSDoc is used on all classes, fields, and methods.</li> </ul> <pre><code>/**\n * Multiple lines of JSDoc text are written here,\n * wrapped normally.\n * @param {number} arg A number to do something to.\n */\nfunction doSomething(arg) { }\n</code></pre> <p>or, a one-liner:</p> <pre><code>/** @const @private {!Foo} A short bit of JSDoc. */\nthis.foo_ = foo;\n</code></pre> <p>JSDoc type annotations are encouraged for all variables and return values, and required for all parameters.</p> <pre><code>/** @type {number} */\nconst foo = 1;\n\nconst /** number */ foo = 1;\n\nconst /** !Array&lt;number&gt; */ data = [];\n\n/**\n * Some description.\n * @type {!Array&lt;number&gt;}\n */\nconst data = [];\n</code></pre> <ul> <li>JSDoc type support Markdown syntax. Use it to add links, emphasis, and other formatting to your comments.</li> <li>Classes, interfaces and records must be documented with a description and any template parameters, implemented interfaces, visibility, or other appropriate tags</li> <li>The class description should provide the reader with enough information to know how and when to use the class, as well as any additional considerations necessary to correctly use the class</li> <li>Textual descriptions may be omitted on the constructor. @constructor and @extends annotations are not used with the class keyword unless the class is being used to declare an <code>@interface</code> or it extends a generic class.</li> </ul> <pre><code>/**\n * A fancier event target that does cool things.\n * @implements {Iterable&lt;string&gt;}\n */\nclass MyFancyTarget extends EventTarget {\n  /**\n   * @param {string} arg1 An argument that makes this more interesting.\n   * @param {!Array&lt;number&gt;} arg2 List of numbers to be processed.\n   */\n  constructor(arg1, arg2) {\n    // ...\n  }\n};\n\n/**\n * Records are also helpful.\n * @extends {Iterator&lt;TYPE&gt;}\n * @record\n * @template TYPE\n */\nclass Listable {\n  /** @return {TYPE} The next item in line to be returned. */\n  next() {}\n}\n</code></pre> <ul> <li>All enums and typedefs must be documented with appropriate JSDoc tags (<code>@typedef</code> or <code>@enum</code>) on the preceding line</li> <li>Public enums and typedefs must also have a description</li> <li>Individual enum items may be documented with a JSDoc comment on the preceding line</li> </ul> <pre><code>/**\n * A useful type union, which is reused often.\n * @typedef {!Bandersnatch|!BandersnatchType}\n */\nlet CoolUnionType;\n\n\n/**\n * Types of bandersnatches.\n * @enum {string}\n */\nconst BandersnatchType = {\n  /** This kind is really frumious. */\n  FRUMIOUS: 'frumious',\n  /** The less-frumious kind. */\n  MANXOME: 'manxome',\n};\n</code></pre> <ul> <li>In methods and named functions, parameter and return types must be documented, except in the case of same-signature</li> <li>Return type may be omitted if the function has no non-empty return statements</li> <li>Method, parameter, and return descriptions (but not types) may be omitted if they are obvious</li> <li>If a method overrides a super class method, it must include an <code>@override</code> annotation</li> </ul> <pre><code>/** A class that does something. */\nclass SomeClass extends SomeBaseClass {\n  /**\n   * Operates on an instance of MyClass and returns something.\n   * @param {!MyClass} obj An object that for some reason needs detailed\n   *     explanation that spans multiple lines.\n   * @param {!OtherClass} obviousOtherClass\n   * @return {boolean} Whether something occurred.\n   */\n  someMethod(obj, obviousOtherClass) {  }\n\n  /** @override */\n  overriddenMethod(param) {  }\n}\n\n/**\n * Demonstrates how top-level functions follow the same rules.  This one\n * makes an array.\n * @param {TYPE} arg\n * @return {!Array&lt;TYPE&gt;}\n * @template TYPE\n */\nfunction makeArray(arg) {  }\n</code></pre> <ul> <li>Always specify template parameters. This way, the compiler can do a better job, and it makes it easier for readers to understand what code does.</li> </ul> <pre><code>// Incorrect\nconst /** !Object */ users = {};\nconst /** !Array */ books = [];\nconst /** !Promise */ response = null;\n\n// Correct\nconst /** !Object&lt;string, !User&gt; */ users = {};\nconst /** !Array&lt;!Book&gt; */ books = [];\nconst /** !Promise&lt;!Response&gt; */ response = null;\n</code></pre>"},{"location":"architecture/18/#status","title":"Status","text":"<p>Accepted.</p>"},{"location":"architecture/18/#consequences","title":"Consequences","text":"<p>Hopefully, better JavaScript code.</p>"},{"location":"architecture/19/","title":"\ud83d\uddf8 ADR-19. Taxonomies. Organization Types","text":""},{"location":"architecture/19/#context","title":"Context","text":"<p><code>Organization</code> is one of the major entities of the project. The initial way of clustering the organizations on record was implemented with the <code>organization_type</code> table, which included 33 types derived from available data. This, of course, soon proved to be suboptimal, as it wasn't properly reflecting the actual situation.</p>"},{"location":"architecture/19/#decision","title":"Decision","text":"<p>It was decided to create a proper taxonomy following the closure pattern, for which purpose tables <code>org_taxonomy</code> and <code>orgs_taxonomy_closure</code> were created in <code>enums</code> schema.</p> <p>The new taxonomy is based on the ISIC Rev. 4 (International Standard Industrial Classification of All Economic Activities), with economic activities converted into organization types, and some important missing ones added, - such as <code>criminal</code> or <code>military</code> organizations. Overall, 297 types are identified as of October 10, 2023.</p> <p>The overall structure is hierarchical with following nuances:</p> <ul> <li>Top-level types are as abstract as possible. The <code>term</code> includes word <code>organizations</code>, for example, <code>Advertising and market research organizations</code> or <code>Delivery organizations</code>.</li> <li>Type's place in the hierarchy is reflected in the length of the code, adding 2 characters per level. In effect, top-level concepts have the shortest codes ( <code>A1</code>, <code>D5</code>, etc), where the letter comes from the type's name, and digit is a consequtive number of top-level types for that letter. Subsequent levels are coded with digits only.</li> <li>This implies a restriction on the number of top-level types that could be added - no more than 9 per letter. Admittedly, this is quite a lot (top-level types are hard to miss, and max we have currently is <code>S7</code>, with the other letter being significantly less occupied), but this is still a drawback.</li> <li>There is a special top-level catch-all category, <code>Other organizations</code> with code <code>O0</code> and no subtypes.</li> <li>Every level starting with 2nd may optionally have an <code>Other</code> category, which should be indicated with <code>00</code> level code. For example, <code>Criminal organizations</code> type has code <code>C2</code> with a number of subtypes ( <code>Gangs</code> is <code>C201</code>, etc.) and a dedicated catch-all type <code>Other criminal organizations</code> with code <code>C200</code>.</li> <li>Initial placement of types/subtypes is largely alphabetical, but the new ones should be added strictly at the end of a relevant category with no regard for the alphabet.</li> <li>Codes can be used to figure out the relationships between types, but <code>closure</code> table provides a better way of doing it. Consult DVE-A-78 for information on how to work with closure tables.</li> </ul> <p>New types should not be added into the <code>taxonomy</code> table manually, because closure relationships won't be registered then. Use script <code>orgs_taxonomy_append</code> in the <code>taxo</code> module of <code>wapatools</code> repo to do it properly.</p>"},{"location":"architecture/19/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/19/#consequences","title":"Consequences","text":"<p>The described approach provides a better way of handling different kinds of organizations than the previous one, but it's far from perfect. This should be revisited in the future, preferrably with a team of people.</p>"},{"location":"architecture/2/","title":"Context","text":"<p>Security is always quite a concern these days, and it is particularly important because the project may become a subject of interest from unfriendly people. As one measure to enhance security, a secrets management solution is to be introduced into the project setup.</p>"},{"location":"architecture/2/#decision","title":"Decision","text":"<p>Doppler.com is an established service provider, with existing integrations with GitHub Actions, Supabase, etc., and exceptionally nice tech support, according to whom integration with Railway would be planned as soon as they release their API (some time in the next couple of months). UPD: Integration with Railway has, indeed, been released, and worked fine ever since.</p> <p>Account was opened on a free tier, and necessary integrations were set up with GitHub, Railway and PyCharm.</p>"},{"location":"architecture/2/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/2/#consequence","title":"Consequence","text":"<p>It might be a pain if there's ever a need to switch to a different service provider. See DVE-A-5</p>"},{"location":"architecture/20/","title":"\u221e ADR-20. Ruskymir Creed [RUCR]","text":""},{"location":"architecture/20/#context","title":"Context","text":"<p>Russian propaganda scene is pretty diverse, with several thousand individuals commenting the everchanging stream of events in a variety of voices and accents, offering all kinds of emotional and intellectual fillers to the audience - yet, a closer examination shows that the entirety of its content can be reduced to several hundred rather specific ideas (or, concepts, as they would be called here) that are repeated in various forms and combinations over and over again.</p> <p>At the same time, the project requires a structure against which the body of accumulated text could be analyzed. The main idea here is that since every media fragment (appearance on a talk-show, a published article, etc.) contributes to at least one of these concepts (usually, more than one), we can map existing (proven) contributions to known concepts. And, because we treat these concepts as first-grade citizens in the system, we can develop them further, providing extensive definitions and notes, as well as numerical measure of harm.</p>"},{"location":"architecture/20/#decision","title":"Decision","text":"<p>RUCR, aka Ruskymir Creed, is an attempt to exhaustively list all these concepts, and to define possible relationships between them.</p>"},{"location":"architecture/20/#skos","title":"SKOS","text":"<p>RUCR from the start was being developed with the SKOS framework in mind. SKOS (Simple Knowledge Organization System) is an OWL-based data model for sharing and linking knowledge organization systems via the Web; it's been a W3 Consortium recommendation for quite a while now. For the purposes of this document, let it be known that SKOS works with entities such as <code>Concept</code>, <code>ConceptScheme</code> and <code>Collection</code>, and 2 major types of relationships between them: the hierarchical is defined with <code>broader</code>, <code>narrower</code>, <code>broaderTransitive</code> and <code>narrowerTransitive</code> properties; the 2nd kind is defined with a symmetric <code>related</code> property.</p>"},{"location":"architecture/20/#xml","title":"XML","text":"<p>However, the initial take on it was made using a visualization tool https://app.diagrams.net, which allows, on the one hand, creation of nicely looking diagrams, and, on the other, saving those diagrams in XML format. A special flow was developed around these XML diagrams, allowing automatic generation of RU and UK versions from the EN version. These diagrams would then be presented for public access on the project's website.</p> <p>The initial versions of the diagrams offer a limited visualizations experience, in the sense that only primary <code>broader-narrower</code> relationships are shown, to avoid visual clutter.</p>"},{"location":"architecture/20/#database","title":"Database","text":"<p>Table <code>rucr_taxonomy</code> was created in the <code>enums</code> schema to provide persistent storage. Each concept has 3 lang-based content versions, set of tags (correspond to <code>Collection</code>s in SKOS), <code>xml_id</code> and <code>xml_data</code> for tying DB entities to XML entities, and <code>status</code>, which is <code>verified</code> for all proper concepts.</p>"},{"location":"architecture/20/#owlready2","title":"owlready2","text":"<p><code>owlready2</code> is a Python library that allows working with OWL ontologies.</p> <p>General SKOS entities and properties are defined in the <code>skos_general.py</code> script as Python classes inheriting from either <code>Thing</code> or <code>Property</code> base class, and defining a relationship to a corresponding SKOS entity (SKOS itself is loaded and attached to the primary ontology as <code>imported</code>). Names of some classes are altered (with respect to original SKOS) to better reflect the nuances of the relationship, specifically, <code>broader</code> is defined as <code>HasBroader</code>, <code>narrower</code> - as <code>HasNarrower</code>, <code>related</code> - as <code>IsRelatedTo</code> and so on.</p> <p>RUCR concepts are explicitly listed in the <code>rucr_skos.py</code> file with the following basic structure:</p> <ul> <li><code>Collections</code> are defined at the top; </li> <li>then follow <code>top</code> concepts; </li> <li>then the rest of the concepts. Primary <code>broaderTransitive</code> relationships (they are used instead of simple <code>broader</code> everywhere) are defined immediately after the concept declaration, which forces their order to some degree. </li> <li>At the bottom, <code>related</code> connections are defined in the same order concepts are defined above.</li> </ul>"},{"location":"architecture/20/#status","title":"Status","text":"<p>In progress</p>"},{"location":"architecture/20/#consequences","title":"Consequences","text":""},{"location":"architecture/21/","title":"\ud83d\uddf8 ADR-21. Data Provenance","text":""},{"location":"architecture/21/#context","title":"Context","text":"<p>In the context of data management, provenance refers to the metadata that describes the origin and processing history of a dataset. It is helpful for establishing the reliability and trustworthiness of the data. This is something that should've been introduced from the beginning, but better late than never.</p>"},{"location":"architecture/21/#decision","title":"Decision","text":""},{"location":"architecture/21/#persistent-metadata","title":"Persistent metadata","text":"<p>Table <code>provenance</code> was created in the <code>service</code> schema of the master DB with the following structure:</p> Field name Data type Notes <code>uid</code> UUID <code>operation</code> TEXT <code>tbl</code> TEXT <code>schm</code> TEXT <code>agent</code> TEXT <code>happenned_at</code> TIMESTAMP <code>affected_row_id</code> INTEGER <code>updated</code> JSONB Relevant for updates only. Contains names of updated keys and their old value <code>inserted</code> TEXT[] Relevant for inserts only. Contains an array of non-null column names"},{"location":"architecture/21/#mechanism","title":"Mechanism","text":"<p>To populate the <code>provenance</code> table automatically, the following generic trigger function was developed using PL/Python procedural language:</p> <pre><code>CREATE OR REPLACE FUNCTION trg_change()\nRETURNS TRIGGER AS $$\n    import json\n    operation = TD[\"event\"].lower()\n    old_rec = TD[\"old\"] if operation in(\"update\", \"delete\") else None\n    new_rec = TD[\"new\"] if operation in(\"update\", \"insert\") else None\n    tbl = TD[\"table_name\"]\n    sch = TD[\"table_schema\"]\n\n    current_data_q = plpy.prepare(\"SELECT current_user, current_timestamp;\")\n    current_data = plpy.execute(current_data_q)\n    current_user, current_timestamp = current_data[0].values()\n\n    if operation == \"insert\":\n        rec_id = new_rec.get(\"id\") or new_rec.get(\"uid\")\n        notnull = [k for k,v in new_rec.items() if v]\n        query = plpy.prepare(\"INSERT INTO service.provenance (operation, tbl, schm, agent, happenned_at, affected_row_id, inserted) VALUES($1, $2, $3, $4, $5, $6, $7)\", [\"tbl_operation\", \"text\", \"text\", \"text\", \"timestamp\", \"int\", \"text[]\"])\n        plpy.execute(query, [operation, tbl, sch, current_user, current_timestamp, rec_id, notnull])\n    elif operation == \"update\":\n        rec_id = new_rec.get(\"id\") or new_rec.get(\"uid\")\n        updated = dict()\n        for k, val in old_rec.items():\n            if new_rec[k] != val:\n                updated[k] = val\n        query = plpy.prepare(\"INSERT INTO service.provenance (operation, tbl, schm, agent, happenned_at, affected_row_id, updated) VALUES($1, $2, $3, $4, $5, $6, $7)\", [\"tbl_operation\", \"text\", \"text\", \"text\", \"timestamp\", \"int\", \"jsonb\"])\n        plpy.execute(query, [operation, tbl, sch, current_user, current_timestamp, rec_id, json.dumps(updated)])\n    elif operation == \"delete\":\n        rec_id = old_rec.get(\"id\") or old_rec.get(\"uid\")\n        query = plpy.prepare(\"INSERT INTO service.provenance (operation, tbl, schm, agent, happenned_at, affected_row_id) VALUES($1, $2, $3, $4, $5, $6)\", [\"tbl_operation\", \"text\", \"text\", \"text\", \"timestamp\", \"int\"])\n        plpy.execute(query, [operation, tbl, sch, current_user, current_timestamp, rec_id])\n$$ LANGUAGE plpython3u;\n</code></pre> <p>With this, any table that requires data provenance can be hooked up by creating a single trigger for it:</p> <pre><code>CREATE OR REPLACE TRIGGER trg_&lt;table_name&gt;\nAFTER UPDATE OR DELETE OR INSERT ON &lt;schema_name.table_name&gt;\nFOR EACH ROW\nEXECUTE FUNCTION trg_change();\n</code></pre> <p>And, as an additional safety mechanism, the following meta-trigger was set up, as well:</p> <pre><code>CREATE OR REPLACE TRIGGER trg_prov\nAFTER UPDATE OR DELETE ON service.provenance\nFOR EACH ROW\nEXECUTE FUNCTION trg_change();\n</code></pre> <p>Meaning, that if anybody would attempt to alter or remove a row from the <code>provenance</code> table, it would be registered in the <code>provenance</code> table via the usual flow. Note, that <code>insert</code> operation is omitted from this one. If added, it would create an infinite loop of database transactions.</p>"},{"location":"architecture/21/#insert-exceptions","title":"INSERT Exceptions","text":"<p>There are other tables, for which <code>INSERT</code> operation is not registered in the <code>provenance</code> table: lists of media, such as <code>youtube_vids</code>, and data tables, such as <code>telegram_messages</code> - because inserts here are done by automated scripts which have their own logs and usually insert in bulk. Here's the full list of such tables:</p> <ul> <li>public: dentv_episodes, komso_episodes, ntv_episodes, rutube_vids, smotrim_episodes, text_media, youtube_vids</li> <li>data: printed_content, telegram_channel_stats, telegram_messages, transcribed_content, transcribed_content_translation_en, transcripts</li> <li>future: meduza_dow_stream, rodniki, e_mash_mapdata</li> <li>service: factory_jobs_run_details, media_segments_stats</li> <li>enums: orgs_taxonomy_closure, isco08_taxonomy_closure (see below)</li> </ul>"},{"location":"architecture/21/#closures","title":"Closures","text":"<p>Relevant for tables whose names end with <code>_closure</code>.</p> <p>Closure tables don't have <code>id</code> columns, so the standard trigger won't work. Another version of the above trigger function was customized and saved under name <code>trg_change_closure</code>.  It is different in 2 respects:</p> <ol> <li>It doesn't have code for handling <code>INSERT</code> operations;</li> <li>For <code>update</code> operation, instead of updated columns, the entire <code>OLD</code> record is saved.</li> </ol>"},{"location":"architecture/21/#test-rows","title":"Test rows","text":"<p>During the development of the flow, 5 rows were inserted into the <code>provenance</code> table that do not conform to the established standard:</p> uid reasoning what's wrong with it 71a5cefc-d84b-469f-88bb-d4ed67b71835 This is a genuine update made for testing purposes. Updated field: <code>known_for</code>. Previous value was <code>null</code> <code>user: postgres</code> - generic superuser should not be used for regular operations` updated: null` - this record was inserted by an earlier version of the trigger function that didn't account for updated columns 84942cc2-0bab-4fc8-847a-2bfdb4e892fa This is a genuine update made for testing purposes. Updated field: <code>known_for</code>. Previous value was <code>null</code> <code>user: postgres</code> - generic superuser should not be used for regular operations` updated: null` - this record was inserted by an earlier version of the trigger function that didn't account for updated columns e742763a-dc19-45c0-b715-2323531eddc5 This is a genuine update made for testing purposes. Updated field: <code>known_for</code>. Previous value was <code>null</code> <code>user: postgres</code> - generic superuser should not be used for regular operations a2f7165f-4a4a-4305-b6f7-78e0afd59c32 This is a dummy update: a test record was inserted into the table <code>organizations</code> from under the <code>ata</code> user; the insert operation triggered creation of the provenance record. Was inserted purely for testing purposes 6f4ae1a6-bb43-400c-840c-db02e711cf0e This is a dummy update, and refers to the removal of the test record inserted into the <code>organizations</code> table (see a2f7165f-4a4a-4305-b6f7-78e0afd59c32) Was inserted purely for testing purposes <p>They were not removed, because nothing should be removed from the <code>provenance</code> table.</p>"},{"location":"architecture/21/#status","title":"Status","text":"<p>The complete list of tables for which provenance will be enabled can be found in DVE-266 https://subjective.youtrack.cloud/issue/</p> <p>As of the moment of submitting this, triggers are set up for <code>people</code> and <code>organizations</code> tables in the <code>public</code> schema of the master DB.</p>"},{"location":"architecture/21/#consequences","title":"Consequences","text":"<p>Provenance mechanism is for the people to trust the project data a little bit more.</p>"},{"location":"architecture/22/","title":"\ud83d\uddf8 ADR-22. TripleLang Composite Type","text":""},{"location":"architecture/22/#context","title":"Context","text":"<p>Due to tri-language nature of the project, where English, Russian and Ukrainian languages are treated as top-level citizens, a common pattern arouse in the database usage, where tables would have lang variants of entity names, either saved as text in different columns (like <code>fullname_en</code>, <code>fullname_ru</code>, <code>fullname_uk</code> in <code>people</code>), or in a JSONB column with <code>en</code>, <code>ru</code> and <code>uk</code> keys.</p>"},{"location":"architecture/22/#decision","title":"Decision","text":"<p>Composite type with name <code>triple_lang</code> was created in December 2023:</p> <pre><code>create type triple_lang as\n(\n    en text,\n    ru text,\n    uk text\n);\n</code></pre> <p>Custom type required following adaptations in the code:</p>"},{"location":"architecture/22/#django","title":"Django","text":"<p>Django handles composite type with a dedicated library <code>django-postgres-composite-types</code>. Given that it's installed and added to Django modules, following classes should be added to the code:</p> <pre><code>from postgres_composite_types import CompositeType\n\n# to models\nclass TripleLang(CompositeType):\n    # https://github.com/danni/django-postgres-composite-types\n    \"\"\"Text value in 3 languages: en, uk, ru\"\"\"\n    en = models.CharField()\n    ru = models.CharField()\n    uk = models.CharField()\n\n    class Meta:\n        db_type = 'triple_lang'\n\n# to serializers\nclass TripleLangSerializer(serializers.Serializer):\nen = serializers.CharField(allow_null=True)\nru = serializers.CharField(allow_null=True)\nuk = serializers.CharField(allow_null=True)\n\nclass Meta:\n    model = models.TripleLang\n    fields = ('en', 'ru', 'uk')\n</code></pre> <p>Then it can be used like so:</p> <pre><code>class EnumsISCOIndex(models.Model):\n    id = models.BigAutoField(primary_key=True)\n    created_at = models.DateTimeField(blank=True, null=True)\n    isco08 = models.ForeignKey(EnumsISCOTaxonomy, models.DO_NOTHING, blank=True, null=True)\n    name = TripleLang.Field(blank=True, null=True)\n    appended = models.BooleanField(blank=True, null=True)\n\n    class Meta:\n        managed = True\n        db_table = 'enums_isco08_index'\n</code></pre>"},{"location":"architecture/22/#psycopg","title":"psycopg","text":"<p>With <code>psycopg</code> the only thing necessary is registration of the type after creating a connection object:</p> <pre><code>from psycopg.types.composite import CompositeInfo, register_composite\n\ninfo = CompositeInfo.fetch(connection, \"triple_lang\")\nregister_composite(info, connection)\n</code></pre> <p>This functionality can be added to the connection creation procedure.</p>"},{"location":"architecture/22/#sqlalchemy-pydantic","title":"SQLAlchemy + Pydantic","text":"<p>With the combination of <code>SQLAlchemy</code> and <code>pydantic</code>, extention package <code>sqlalchemy_utils</code> should be used to define <code>CompositeType</code> columns. Given a <code>pydantic</code> model</p> <pre><code>from pydantic import BaseModel\n\nclass TripleLang(BaseModel):\n    en: Optional[str]\n    ru: Optional[str]\n    uk: Optional[str]\n</code></pre> <p>A table with <code>triple_lang</code> fields can be defined like so:</p> <pre><code>from datetime import datetime\nfrom typing import Optional\nfrom pydantic import BaseModel, ConfigDict\nfrom sqlalchemy import Column, Integer, Text, DateTime, MetaData\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy_utils import CompositeType\nfrom wapaganda.database.models.base import Base, TripleLang\n\nclass BundleTypeDB(Base):\n    metadata = MetaData(schema=\"enums\")\n    __tablename__ = 'bundle_types'\n    __table_args__ = {'extend_existing': True}\n\n    id = Column(Integer, primary_key=True)\n    created_at = Column(DateTime)\n    code = Column(Text)\n    updated_on = Column(DateTime)\n    description = Column(CompositeType(name=\"triple_lang\", columns=[Column(\"en\", Text), Column(\"ru\", Text), Column(\"uk\", Text)]))\n\n    bundles = relationship(\"BundleDB\", back_populates=\"bundle_type_\")\n\nclass BundleTypeDTO(BaseModel):\n    model_config = ConfigDict(from_attributes=True, arbitrary_types_allowed=True)\n\n    # id: Optional[int] = None\n    created_at: datetime\n    code: str\n    updated_on: Optional[datetime]\n    description: TripleLang\n</code></pre> <p>With setup like this, on the app level, one needs to register custom types like so:</p> <pre><code>from sqlalchemy_utils import register_composites\nfrom wapaganda.database.db_client import DBClient\n\nclient = DBClient(driver='psycopg2')\nwith client.get_connection() as conn:\n    register_composites(conn)\n</code></pre> <p>As of January 2024, <code>psycopg2</code> needs to be used for <code>CompositeType</code> class; newer <code>psycopg</code> isn't supported.</p>"},{"location":"architecture/22/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/22/#consequences","title":"Consequences","text":"<p>Custom types can be tricky to use, which makes them a possible source of bugs. On the other hand, they enable more flexible work with the db.</p>"},{"location":"architecture/23/","title":"\ud83d\uddf8 ADR-23. Wapaganda Localization [2]","text":""},{"location":"architecture/23/#context","title":"Context","text":"<p>This document succeeds and replaces ADR-9 DVE-A-33, and describes approach to maintaining the website in 3 languages (En, Ru, Uk).</p>"},{"location":"architecture/23/#decision","title":"Decision","text":""},{"location":"architecture/23/#data","title":"Data","text":"<p>Composite database type <code>triple_lang</code> was created to hold values that are either translatable or transcriptable; it has 3 <code>TEXT</code> fields corresponding the the 3 supported languages. Type adaptation was implemented for both Django and FastAPI-based backends. See more in DVE-A-120.</p>"},{"location":"architecture/23/#frontend","title":"Frontend","text":"<p>Website components are translated using native React tools (<code>useTranslation</code> function), with file <code>i18n.js</code> containing translatable options in <code>resources</code> constant.</p>"},{"location":"architecture/23/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/23/#consequences","title":"Consequences","text":"<p><code>triple_lang</code> type allows for more reliable and manageable approach to handling multilang data, both retrival and update. On the other hand, frontend keeping all text in a single file would make it hard to manage as the volume of content grows. TODO: Research possible options.</p>"},{"location":"architecture/24/","title":"\ud83d\uddf8 ADR-24. Polylith Projects & Dockerization","text":""},{"location":"architecture/24/#context","title":"Context","text":"<p>Primary project's repository, <code>wapatools</code>, is a shared monorepo that follows the polylith structure. This structure is not directly consistent with the Docker approach, and requires certain additional steps.</p>"},{"location":"architecture/24/#decision","title":"Decision","text":"<p>Services in the W ecosystem can be run either on Railway, or on our own infrastructure. Railway's upside is simplicity and reliability; its downside is only the cost.</p>"},{"location":"architecture/24/#general-build-procedure","title":"General Build Procedure","text":"<p>A shell-script can be used to perform all actions necessary. Approximate procedure includes the following steps:</p> <ol> <li>Define paths to the files required for the project (healthcheck script, log config etc.);</li> <li>Drop old <code>wheel</code> file if it exists;</li> <li>Copy files to the project's directory (<code>projects/{project_name}</code>);</li> <li>Run <code>poetry build-project</code> command from the project directory.    This would create <code>dist</code> folder in the projects' directory with files that can be used to install the project, including a new <code>wheel</code> file;</li> <li>Get the name of the new <code>wheel</code> file;</li> <li>Modify <code>Dockerfile</code> with the new <code>wheel</code>'s name.    This solution relies on that the name of the <code>wheel</code> file is located on a specific line in the Dockerfile;</li> </ol> <p>:::warning    With services that are supposed to run on Railway, the process stops here. Railway can build their own docker images, thank you very much.    :::</p> <p>:::info    The rest of the steps are only required for services that are supposed to run on our own infrastructure.    ::: 7. Build Docker image; 8. Tag new image; 9. Push new image to Docker Hub or private repository.</p>"},{"location":"architecture/24/#dockerfile","title":"Dockerfile","text":"<p>:::info Dockerfile is almost the same for both deployment cases. The 2 main differences are:</p> <ul> <li>You should install Infisical CLI in non-Railway Dockerfile;</li> <li>When copying wheel file, in Railway one should start in the repository's root, and in self-hosted case - in the project's root. :::</li> </ul> <p>Project's Dockerfile must be composed with these things in mind:</p> <ul> <li>Index of the line on which the name of the <code>wheel</code> is defined must be aligned with the builder. If using a template, no changes are necessary;</li> <li>Final image must include Infisical CLI for any application that requires supply of secrets;</li> <li>When using multistage builds, watch out for what components should be copied to the final image. This depends on what exactly was installed;</li> <li>As a rule, last line in the Dockerfile switches work directory to where the main executable is located;</li> <li>As a rule, Dockerfile doesn't include a <code>CMD</code> or <code>entrypoint</code> command, mainly because it would be overridden in the <code>compose</code> file anyway.</li> </ul>"},{"location":"architecture/24/#deployment","title":"Deployment","text":""},{"location":"architecture/24/#self-hosted-with-docker-compose","title":"Self-Hosted with <code>docker compose</code>","text":"<p><code>compose.yml</code> or <code>docker-compose.yml</code> file is used to launch the service which may be a single container or multiple.</p> <ul> <li>Main container's <code>command</code> must start with <code>infisical run --</code> followed by the path to Python's interpreter + service executable;</li> <li><code>env_file</code> must be supplied with Infisical's <code>server address</code> and <code>access token</code>.</li> </ul>"},{"location":"architecture/24/#on-railway","title":"On Railway","text":"<ol> <li>Make sure the corresponding Infisical project has secret named <code>RAILWAY_DOCKERFILE_PATH</code> set to <code>/projects/{project}/Dockerfile</code>, given that the name of the Dockerfile is not modified.</li> <li>Create a new project on Railway.<ul> <li>Set <code>source repo</code> to <code>https://github.com/subjective-agency/wapatools</code></li> <li>Select <code>development</code> branch or whatever may be the case.</li> <li>In the <code>build</code> section, set <code>Watch Paths</code> to <code>projects/{project_name}</code>.</li> <li>In the <code>deploy</code> section, set <code>Custom Start Command</code> to <code>python {primary_component_name}/core.py</code></li> <li>Set <code>Region</code> to anywhere in Europe, <code>Runtime</code> to <code>V2</code>, and <code>Replicas</code> to 1. :::warning At this point Railway would likely attempt to deploy, but fail for lack of secrets. :::</li> </ul> </li> <li>Set up integration with Infisical, then re-deploy.</li> </ol>"},{"location":"architecture/24/#development-flow","title":"Development Flow","text":"<p>Suppose, in the course of development, you finished an important update to the app, and now want to deploy a new version. Here are the steps for this:</p> <ol> <li>Commit changes with a meaningful message, as usual.</li> <li>Go to <code>projects/{project_name}/pyproject.toml</code> and bump up the version.</li> <li>Run <code>builder</code> shell script for the application.</li> <li>Commit artifacts created by this, with message <code>{project_name}-{version}</code>, for example, <code>ave-media-0.2.1.8</code>.</li> <li>Push these 2 commits to the remote origin.</li> </ol> <p>At this point, Railway would detect updates in the <code>project</code> directory, and initiate a new build &amp; deploy.</p>"},{"location":"architecture/24/#status","title":"Status","text":"<p>Active</p>"},{"location":"architecture/24/#consequences","title":"Consequences","text":"<p>Setting up a consistent flow around apps dockerization opens a straight path to running apps in our infrastructure, or on Railway.</p>"},{"location":"architecture/25/","title":"\ud83d\uddf8 ADR-25. Secrets Management with Infisical","text":""},{"location":"architecture/25/#context","title":"Context","text":"<p>This document succeeds and replaces ADR-2. It describes the integration of self-hosted Infisical instance as the primary tool for communicating sensitive data to the applications.</p>"},{"location":"architecture/25/#decision","title":"Decision","text":"<p>Infisical is a feature-rich open-source secrets management platform with well-detailed documentation.</p> <p>Wapaganda instance is hosted on a Hetzner server with <code>docker compose</code>. It is accessible on port <code>50789</code> at <code>principal</code>'s IP address; its HTTPS host is https://asdfghjkl.subjective.agency (reverse proxied with Caddy). </p> <p>Normally, Infisical's docker compose stack includes 4 services: <code>infisical</code> itself, <code>redis</code>, <code>postgres</code> and <code>db-migration</code>, where <code>db-migration</code> starts with all the rest, and exits once done migrating. In W setup, <code>postgres</code> service is disabled; instead, a dedicated <code>infisical</code> database is set up on the production instance.</p>"},{"location":"architecture/25/#setup","title":"Setup","text":"<p>Each application that requires secrets injection has a <code>project</code> in Infisical. Every project can have multiple environments, but the simplest setup is to have a single <code>dev</code> environment with all secrets required for an app.</p>"},{"location":"architecture/25/#railway-integration","title":"Railway integration","text":"<p>Infisical has native support for Railway. To set it up,</p> <ol> <li>Go to Railway, and obtain a new API key. Name it the same as the application.</li> <li>Go to Infisical -&gt; a project you're setting up -&gt; <code>Integrations</code>. Find <code>Railway</code>, click on it, and submit the new API key.</li> <li>In the pop-up, select appropriate environments on both sides. Make sure environments are aligned: you want to connect Prod to Prod, and Dev to Dev.</li> </ol> <p>Once the integration is saved, you should see a bunch of secrets in Railway, in the service's <code>Shared Secrets</code> section.</p> <p>Railway integration means, you don't need to add <code>infisical run --</code> to the start command.</p>"},{"location":"architecture/25/#docker-apps","title":"Docker apps","text":"<p>With non-Railway docker-based applications, integration is enabled with the following procedure:</p> <ol> <li><code>Dockerfile</code> for the application must include instruction for installation of Infisical CLI. For example, here's the corresponding line from the backup service's file (that also installs PostgreSQL and a bunch of other stuff):</li> </ol> <p><pre><code>RUN apt-get update &amp;&amp; apt-get install -y bash curl tar coreutils postgresql-client &amp;&amp; curl -1sLf \\\n'https://dl.cloudsmith.io/public/infisical/infisical-cli/setup.deb.sh' | bash \\\n&amp;&amp; apt-get update &amp;&amp; apt-get install -y infisical\n</code></pre> 2. Application's start command, whether in <code>Dockerfile</code> or in <code>docker-compose.yml</code> must be modified to start with <code>infisical run --</code> like so:</p> <p><pre><code>infisical run -- python backup/core.py\n</code></pre> 3. In order to the application to connect with Infisical, it needs 2 things: address of the host (since we're self-hosting) and a token. You can generate token for a project in the Access Control tab. These 2 values must be saved in <code>.env</code> file in the same folder where you have app's primary <code>docker compose</code> file. </p>"},{"location":"architecture/25/#local-apps","title":"Local apps","text":"<p>If a locally-running application needs to be integrated with Infisical, you must have the CLI installed in the corresponding system; and launch the app in the previously described manner, by prefixing the start command with <code>infisical run --</code>.</p> <p>Instead of tokens, you authorize the application by explicitly logging in (run <code>infisical login</code> and follow the flow). This login is persisted for some period of time (about a week), after which you'd have to re-login. </p> <p>:::danger It has been noticed that if a Redis container exits for whatever reason, Infisical instance won't be operational, but instead of failing, it would be urging you to re-login. :::</p>"},{"location":"architecture/25/#status","title":"Status","text":"<p>Active</p>"},{"location":"architecture/25/#consequences","title":"Consequences","text":"<p>Some refactoring was necessary, otherwise, it works better than Doppler.</p>"},{"location":"architecture/27/","title":"\u221e ADR-27. Wapaganda Structure [3]","text":""},{"location":"architecture/27/#context","title":"Context","text":"<p>:::info Previous version of the system is described in ADR-12. :::</p> <p>This document describes the structure of the W project.</p>"},{"location":"architecture/27/#decision","title":"Decision","text":""},{"location":"architecture/27/#diagram","title":"Diagram","text":"<p>TBD</p>"},{"location":"architecture/27/#git-repositories","title":"Git repositories","text":"<p>All repositories here are hosted under Subjective Agency organization on GitHub.</p> <ul> <li><code>wapatools</code>. Primary repository, home for all applications except <code>web</code>.</li> <li><code>wapaganda_frontend</code>. React-based frontend for the <code>web</code> application.</li> <li><code>whisperX</code>. Fork required in order for WhisperX to run properly (support for <code>transfactory</code> application).</li> </ul>"},{"location":"architecture/27/#database","title":"Database","text":"<p>There are 2 PostgreSQL instances set up for the project on different servers, <code>prod</code> and <code>dev</code>.</p> <p>Production database has tables distributed across following schemas:</p> <ul> <li>public;</li> <li>service;</li> <li>enums;</li> <li>data.</li> </ul> <p>There are several external tables synchronized to the database, all of them in <code>service</code> schema:</p> <ul> <li><code>provenance</code>. Primary table for the data provenance mechanism. See ADR-21 for details;</li> <li><code>logs</code>. The log system that required this table was deprecated in favor of Logfire. See ADR-31 for details;</li> <li><code>memo</code> and <code>memo_relation</code> are required to support the <code>theory</code> subsystem. See ADR-30 for details.</li> </ul> <p>Custom <code>TripleLang</code> type was created for the project. See ADR-22 for details.</p> <p>Dev database is maintained in sync with the prod with the help of Snaplet. See [ADR-32] for details.</p>"},{"location":"architecture/27/#internal-applications","title":"Internal Applications","text":"<p>All internal applications are Docker-based. Latest version of images can be found in our private repository at <code>registry.subjective.agency</code>.</p> <ul> <li>Backup. Regularly exports project's database to JSON files. Works in 2 modes: incremental creates daily dumps of only new or updated rows; full creates weekly dumps of all relevant tables.</li> <li>Transfactory. Takes audiofiles as input and returns structured text (and metadata) as output.</li> <li>Transsuply. Pre-processing and registration of new jobs for Transfactory. TBD.</li> <li>Periodicals. Scans text-based media, news agencies etc for relevant updates, and collects them. TBD</li> <li>Ave Media. Scans audio and video-based media such as Smotrim.ru, Youtube.com etc. for relevant updates, and collects them. TBD</li> <li>Tgram. Scans target Telegram channels using official API, and collects data and statistics. TBD</li> <li>Printed. TBD</li> <li>Web. Provides a public view on the existing data.</li> <li>Entry Bot. Telegram bot that serves as input channel for new data.</li> <li>SyncManager. General-purpose application for misc backgrounds sync tasks, and handles requests forwarded from the Entry Bot.</li> </ul>"},{"location":"architecture/27/#supporting-applications-integrations","title":"Supporting Applications &amp; Integrations","text":"<ul> <li>Infisical. Used to communicate secrets to applications. See ADR-25 for details.</li> <li>Authentik. Used for authentication and authorization in other tools.</li> <li>Memos. Notes-taking application. Used in the <code>theory</code> flow. </li> <li>HedgeDoc. Hosts documentation of the project.</li> <li>Youtrack. Used to be a ticket-tracker of the project, where also documentation was stored. Docs were moved away to HedgeDoc. Ticket-tracker has not been restored for now, for lack of need.</li> <li>Bytebase. Operational interface on top of the databases. Allows to explore the database via a rather convenient interface, as well as to run update and insert queries of all kinds: either in admin mode, or through a ticket-creation flow.</li> <li>Ofelia. TBD</li> <li>dbmate. TBD</li> </ul>"},{"location":"architecture/27/#network-hosting","title":"Network &amp; Hosting","text":"<p>Domain <code>subjective.agency</code> is hosted on <code>domain.com</code> and managed via Cloudflare. Mailservice is managed via <code>namecheap.com</code>. Secondary domain <code>subjective.place</code> is hosted directly on Cloudflare.</p>"},{"location":"architecture/27/#hetzner-principal","title":"Hetzner Principal","text":"<p>Hetzner Principal server hosts:</p> <ul> <li>dev database,</li> <li>Infisical,</li> <li>Authentik,</li> <li>Hedgedoc,</li> <li>Bytebase,</li> <li>Memos.</li> </ul>"},{"location":"architecture/27/#hetzner-horsey","title":"Hetzner Horsey","text":"<p>Hetzner Horsey server hosts:</p> <ul> <li>production database, </li> <li>Caddy as reverse proxy server, </li> <li>Garage-operated storage (100 Gb), and </li> <li>all the internal applications, except <code>Web</code>.</li> </ul>"},{"location":"architecture/27/#railway","title":"Railway","text":"<p>Railway hosts:</p> <ul> <li>FastAPI-based backend, and </li> <li>React-based frontend </li> </ul> <p>Together they constitue Web internal application. </p> <p>Railway is also used for test deployment of new internal applications.</p>"},{"location":"architecture/27/#status","title":"Status","text":"<p>Active</p>"},{"location":"architecture/27/#consequences","title":"Consequences","text":""},{"location":"architecture/28/","title":"\ud83d\uddf8 ADR-28. Backup Service","text":""},{"location":"architecture/28/#context","title":"Context","text":"<p>The W project is build around a database, so backing up data is crucial. The core of the application was built by Yurii Cherkasov in the <code>wapaganda_backend</code> repository in 2022-2023, including the principal functionality for export into JSON files, resuming, and batching. This document describes how that code was modified and adapted to be run in automated fashion.</p>"},{"location":"architecture/28/#decision","title":"Decision","text":""},{"location":"architecture/28/#application","title":"Application","text":"<p>The application uses <code>sqlmodel</code> database models to communicate with the database and serialize data. At the start of the process, all relevant models are loaded and collected into a list. Each table is queried for number of rows to export first, which decides the export strategy (in one go, or in batches). Then data is dumped into JSON file(s). Once all the tables are exported, the data is archived and compressed; a schema file (generated by <code>dbmate</code> outside of the scope of this application) and database config files are also added to the archive, which is then uploaded to 2 destinations, in-company Minio server, and Backblaze storage. The outcome of the process is communicated to the application owner via Telegram.</p>"},{"location":"architecture/28/#modes","title":"Modes","text":"<p>Two modes are available for use: <code>full</code> and <code>incremental</code>.</p>"},{"location":"architecture/28/#full-mode","title":"Full mode","text":"<p>In full mode, the application queries the tables directly without any additional logic. Default batch size is 100K records.</p>"},{"location":"architecture/28/#incremental-mode","title":"Incremental mode","text":"<p>In incremental mode, the application operates via the <code>volatility</code> table, which serves as a temporary storage for new and updated rows. Each database table, for which backup has been enabled, has a set of triggers associated with it, including one that adds new records to <code>volatility</code> every time a row is added to that table, updated or removed. The removed records also have a JSON payload containing all non-null values of the removed row.</p> <p>The application queries the <code>volatility</code> table for the rows of the target table. If none are found, the target is skipped. If records are found, they get processed in the following manner:</p> <ul> <li>Removed records are separated, and later dumped into a file with <code>_deleted</code> suffix.</li> <li>Non-removed records are analyzed to get a unique set. This is a bottleneck: large amount of records may take a long time to parse through.</li> </ul> <p>Once the set of records to be exported is defined, their tables are queried using explicit lists of the record IDs. This is the reason why default batch size in incremental mode is 50K (there is a limit on the Postgres side on how many such IDs you can send in a query).</p> <p>Note: by default, table <code>telegram_messages</code> is excluded from incremental backup because it seems to hang the entire process.</p>"},{"location":"architecture/28/#arguments","title":"Arguments","text":"<p>The application accepts 3 arguments:</p> <ul> <li><code>mode</code> defines which backup mode to use;</li> <li><code>export_dir</code> allows user to set the directory for exported files;</li> <li><code>exclude</code> allows user to exclude certain tables from backup. It's a list of schema-qualified strigyfied table names, for example, <code>[\"data.telegram_messages\",\"data.text_media\"]</code>.</li> </ul> <p>These arguments can be supplied to the app directly in <code>docker-compose.yaml</code> file used to launch it, or in the <code>.env</code> file indicated there.</p>"},{"location":"architecture/28/#flow","title":"Flow","text":"<p>The app is built as a Docker image, and it meant to be launched with context-specific <code>docker-compose.yaml</code> files.</p> <p>Application relies on the following:</p> <ul> <li>a schema file has been produced by <code>dbmate</code> in the recent days, and saved to a particular directory;</li> <li>database configuration files can be located.</li> </ul> <p>The application is scheduled to run:</p> <ul> <li>in <code>full</code> mode: once a week, ETC nighttime;</li> <li>in <code>incremental</code> mode: once a day, ETC nighttime, except for the day when full dump is done.</li> </ul>"},{"location":"architecture/28/#storage","title":"Storage","text":"<p>The archive produced by the export is then uploaded to the storage:</p> <ul> <li>Minio storage located on the same server as the database;</li> <li>Backblaze cloud storage.</li> </ul>"},{"location":"architecture/28/#logging","title":"Logging","text":"<p>The application logs to a dedicated <code>backup</code> project on Logfire.</p>"},{"location":"architecture/28/#status","title":"Status","text":"<p>Active</p>"},{"location":"architecture/28/#consequences","title":"Consequences","text":"<p>Data is safe and consistent.</p>"},{"location":"architecture/29/","title":"\u221e ADR-29. Storage","text":""},{"location":"architecture/29/#context","title":"Context","text":"<p>The project requires storage in order to exchange media files, store assets, etc. For the first period of its existense, we used Supabase, which offered 100Gb of storage in addition to the database hosting service \"for free\". When we dropped Supabase, Minio-operated storage was set up on the same Hetzner server that hosts the database. This document describes the current implementation.</p>"},{"location":"architecture/29/#decision","title":"Decision","text":""},{"location":"architecture/29/#status","title":"Status","text":""},{"location":"architecture/29/#consequenses","title":"Consequenses","text":""},{"location":"architecture/3/","title":"\ud83d\uddf6 ADR-3. Git Flow","text":""},{"location":"architecture/3/#context","title":"Context","text":"<p>As the project gradually picking up steam, it would be appropriate to introduce a stricter Git flow, to make the process of development more organized and structured. Many different approaches are possible here, the most widely used of them are git-flow and GitHub flow.</p>"},{"location":"architecture/3/#decision","title":"Decision","text":"<p>Given that the project is rather small, and is not expected to require significant workforce, GitHub flow seems more appropriate. The components of the approach are as follows:</p> <ul> <li>There is a persistent branch called <code>master</code> that serves as a production;</li> <li>Development goes on in <code>feature</code> branches that may or may not have YouTrack issues linked to them;</li> <li>Feature branch must have a <code>feature-</code> prefix in the name.</li> <li>Once a feature is deemed complete, a PR should be created. This would spin up a Railways PR environment for this feature branch. Once the PR is closed, the Railway env would be removed automatically.</li> <li>If PR is approved, the feature branch should be merged into <code>master</code> and then removed.</li> </ul>"},{"location":"architecture/3/#status","title":"Status","text":"<p>In revision. The workflow would be modified towards CI/CD.</p> <p>[UPD] [17.02.2024] This ADR is simply cancelled for now.</p>"},{"location":"architecture/3/#consequence","title":"Consequence","text":"<p>Git happiness.</p>"},{"location":"architecture/30/","title":"\ud83d\uddf8 ADR-30. Theory","text":""},{"location":"architecture/30/#context","title":"Context","text":"<p>Website's Theory section is intended to serve as a window into the supporting materials related to the purpose of the project. These are: current-time articles and blog posts written by either friends of Ukraine or relatively objective observers (subtype <code>companions</code>); a special tiny collection of particularly enlightening texts written by the Russian propaganda actors (subtype <code>copium</code>); my own theoretical reasonings (subtype <code>core</code>); and a multitude of notes on various research subjects, from the so-called geopolitics to linguistic nuances of Russian language.</p> <p>The initial body of companion and copium texts was translated from Russian into English for the Dum Vita Est blog, and, in April 2023, translated from Russian into Ukrainian using <code>openai/gpt-3.5</code>.</p>"},{"location":"architecture/30/#decision","title":"Decision","text":""},{"location":"architecture/30/#memos","title":"Memos","text":"<p>Memos is a self-hosted application for taking notes. What makes this particular app suitable for the purpose of the W project are the following nuances:</p> <ul> <li>Memos supports storing notes in the PostgreSQL database;</li> <li>App's system of tags is sufficiently intricate, and supports nested tags;</li> <li>There is a feature allowing to link notes with each other. </li> </ul> <p>As such, Memos provides a convenient interface for adding new notes.</p>"},{"location":"architecture/30/#technical-details","title":"Technical Details","text":"<p>As of mid August 2024, Memos instance is set up on a Hetzner server. HTTPS domain is https://memos.subjective.agency (reverse proxied with Caddy). Dedicated postgres database was created on Dev instance. This database is managed by the Memos application and should not be tempered with manually.</p>"},{"location":"architecture/30/#adding-new-memo","title":"Adding New Memo","text":"<p>Most of the metadata is added to the note in the frontematter component, which must always come first. It is followed by the note's content, and then, at the end, exactly 1 line of tags.</p>"},{"location":"architecture/30/#frontmatter","title":"Frontmatter","text":"<p>Frontmatter is designated with <code>---</code> before and after a number of YAML-formatter key-value pairs, and must always come as the first component on the canvas. </p> <p>Depending on whether a note is a full-fledged article, or just a remark, there could be different amount of metadata. Generally speaking, <code>author</code> and <code>urls</code> should be definitely preserved, plus anything else that makes sense in context.</p> <p>With smaller notes, definitely set <code>sequence</code> if note is a start of a new sequence.</p> <p>With larger articles, following keys are requred:</p> <ul> <li><code>author</code></li> <li><code>subtype</code>. This could be one of: <code>companion</code>, <code>core</code> or <code>copium</code>.</li> <li><code>excerpt</code>. Short summary of / intro into the article.</li> <li><code>date_published</code></li> <li><code>url</code> (if singular) or <code>urls</code>. In the latter case, the value must be surrounded with square brackets <code>[]</code>, have each URL inside wrapped in single quotes <code>'</code>, and use coma <code>,</code> as a delimiter.</li> <li><code>translated_to_en</code> | <code>translated_to_uk</code> | <code>translated_to_ru</code>: a JSON structure with keys <code>i</code> (for intelligence, could be <code>h</code> (human) or <code>a</code>), and <code>author</code>. Example: <code>{\"i\": \"h\", \"author\": \"shoomow\"}</code></li> </ul>"},{"location":"architecture/30/#content","title":"Content","text":"<ul> <li>Must be clean markdown.</li> </ul>"},{"location":"architecture/30/#tags","title":"Tags","text":"<ul> <li>Tags must be put on the last line of the note, and must all be on one line.</li> <li>Tags must be separated with spaces. </li> <li>Tags can be multilevel, in which case levels must be separated with forward slash <code>/</code> and no spaces.</li> <li>Every singular tag must be preceded with <code>#</code>; with multilevel tags, only the root must have <code>#</code>.</li> <li>If a tags has to be more than 1 word, words within tags must be separated with underscore <code>_</code>.</li> </ul> <p>Example:</p> <pre><code>#research/geopolitics #mackinder #book/democratic_ideals_and_reality\n</code></pre> <p>:::info In case a newly saved memo is a part of a sequence (e.g. yet another observation taken in the course of reading a book), once it has been added, find the previous note of this sequence, and add the link to the new note to it. This is how the sequences are maintained. :::</p>"},{"location":"architecture/30/#secondary-tables-and-synchronization-flow","title":"Secondary Tables and Synchronization Flow","text":""},{"location":"architecture/30/#database","title":"Database","text":"<p>Two of the original Memos tables are mirrored to the primary database via <code>foreign tables</code> mechanism. They can be found in the <code>service</code> schema, under names <code>memo</code> and <code>memo_relation</code>.</p> <p>Before the data can be served to the final user, it needs to be processed. For this purpose, following secondary tables were set up:</p> <ul> <li><code>public.theory2</code>. Primary content (notes) is stored here.</li> <li><code>service.theory_tags</code>. Tags. Includes 1-to-many <code>parent</code> relationship.</li> <li><code>service.theory_tags_mapping</code>. Mapping between notes (<code>theory2</code>) and tags (<code>theory_tags</code>).</li> <li><code>service.theory_sequences</code>. Sequences.</li> <li><code>service.theory_sequence_mapping</code> Mapping between notes and sequences. Includes <code>spot_in_sequence</code> to be explicit about the order of notes.</li> <li><code>enums.theory_types</code></li> </ul>"},{"location":"architecture/30/#synchronization","title":"Synchronization","text":"<p>Synchronization is set up as 2 routes in the FastAPI web application (under <code>theory</code> tag): <code>theory/sync</code> and <code>theory/update_sequences</code>. Both are background processes to be run at schedule.</p> <p>The primary <code>sync</code> process collects new <code>memos</code>, uses python markdown libraries to properly parse it, and converts it into a DB object (<code>theory2</code>). </p> <p><code>Frontmatter</code> fields listed explicitly in this section would be parsed into proper columns, everything else would be merged into a JSON structure and saved in <code>additional</code> field, with the exception of <code>sequence</code> tag, which indicates first note in a new sequence, --- in this case, a new <code>sequence</code> object would be created.</p> <p>Tags would be parsed; they will be mapped to the note, and all new ones would be inserted into the <code>theory_tags</code> table. </p> <p>The secondary sync process (<code>update_sequences</code>) iterates over the known sequences (i.e. present in the <code>theory_sequences</code> table), finds an <code>edge</code> note, and checks if there is a previously non-existent relationship there.</p>"},{"location":"architecture/30/#status","title":"Status","text":"<p>Active</p>"},{"location":"architecture/30/#consequences","title":"Consequences","text":"<p>The new setup required significant refactoring of the frontend.</p>"},{"location":"architecture/31/","title":"\ud83d\uddf8 ADR-31. Logging with Logfire","text":""},{"location":"architecture/31/#context","title":"Context","text":"<p>Historically, logging application events and sharing them among the team members has been a challenge. Logfire, which is created and maintained by the team behind Pydantic, makes this whole ordeal a breeze.</p>"},{"location":"architecture/31/#decision","title":"Decision","text":"<p>Logfire allows for very versatile setup. Normally, a standalone application would require a dedicated <code>project</code> in Logfire, but in certain cases multiple applications can write to the same project.</p> <ol> <li>Create new project via Logfire web UI</li> <li>Go to Settings and generate a new <code>write token</code>. </li> <li>Save the token as <code>LOGFIRE_TOKEN</code> in the corresponding Infisical project (should already be created by then).</li> </ol> <p>Optionally, alerts can be configured in the corresponding section of the Logfire project.</p> <p>In Python code logs are recorded via the <code>logfire</code> library, which, in its basic functions, serves as an almost complete drop-in replacement for Python's standard logging -- just replace <code>logging.info</code> with <code>logfire.info</code> etc, and you're good to go. The only exception here is <code>warning</code> which is shortened to <code>warn</code> with Logfire.</p> <p>In addition, Logfire offers a number of additional features:</p> <ul> <li>you can organize logs into <code>spans</code> (custom groups) for better visibility;</li> <li>you can configure the app to write into different environments;</li> <li>you can scrub sensitive information;</li> <li>you can track system metrics, or create custom metrics;</li> <li>you can <code>instrument</code> various integrations, like <code>requests</code>, <code>httpx</code>, <code>sqlalchem</code> etc. (should be installed explicitly)</li> </ul>"},{"location":"architecture/31/#status","title":"Status","text":"<p>At the moment, Logfire only supports Python applications, but it is based on OpenTelemetry, which means it can be, with some tweaks, enabled for JavaScript applications, too.</p>"},{"location":"architecture/31/#consequences","title":"Consequences","text":"<p>Very neat, informative and accessible logs.</p>"},{"location":"architecture/32/","title":"ADR-32 Database Backup with PgBackRest","text":""},{"location":"architecture/32/#context","title":"Context","text":""},{"location":"architecture/32/#decision","title":"Decision","text":""},{"location":"architecture/32/#status","title":"Status","text":"<p>Active.</p>"},{"location":"architecture/32/#consequences","title":"Consequences","text":""},{"location":"architecture/33/","title":"\ud83d\uddf8 ADR-33. Private Docker Registry","text":""},{"location":"architecture/33/#context","title":"Context","text":"<p>It is more than possible to use DockerHub for exchanging Docker images, but it doesn't feel secure, since the data can be, in theory, available to random people, even if the repo is private. On the other hand, setting up private registry is as simple as running a regular docker <code>compose</code> application. </p>"},{"location":"architecture/33/#decision","title":"Decision","text":"<ul> <li>Registry was set up on the Horsey server at port 50000, and secured behind HTTPS at <code>registry.subjective.agency</code>. </li> </ul> <p>Note</p> <p>Without a dedicated domain for the registry, one would have to add its address to <code>insecure-registries</code> key in <code>/etc/docker/daemon.json</code>, but because the registry is accessible via https, there is no need.</p> <ul> <li>Registry was set up as a system service, with the service file at <code>/etc/systemd/system/docker-registry.service</code>.</li> <li>Data directory is <code>/mnt/docker/registry</code>     Example of creating data directory and applying permissions     <pre><code>sudo mkdir -p /mnt/docker/registry\nsudo chown -R root:root /mnt/docker/registry\nsudo chmod -R 755 /mnt/docker/registry\n</code></pre></li> <li>Authentication is set up with <code>htpasswd</code>. Auth directory is <code>/mnt/docker/registry/auth</code>. Auth config is at <code>/mnt/docker/registry/config.yml</code></li> </ul>"},{"location":"architecture/33/#see-also","title":"See Also","text":"<ul> <li>Private Docker Registry Auth</li> </ul>"},{"location":"architecture/33/#status","title":"Status","text":"<p>Currently (Nov 27, 2024) only smaller images can be stored in the registry; larger ones fail to push with <code>Cloudflare entity too large</code> exception, which is why private repository on Docker Hub is used for those. This issue should be fixed at some point.</p>"},{"location":"architecture/33/#consequences","title":"Consequences","text":"<p>Only a few minor things need to change when using private registry; namely, you need to indicate the domain when pushing and pulling; and you need to authorize with the repository in order to work with it.</p>"},{"location":"architecture/4/","title":"\ud83d\uddf6\ud83d\uddf8 ADR-4. REST API","text":""},{"location":"architecture/4/#context","title":"Context","text":"<p>There are two major approaches to Frontend architecture: traditional web applications that perform most of the application logic on the server, and single-page applications (SPAs) that perform most of the user interface logic in a web browser, communicating with the web server primarily using web APIs. A hybrid approach is also possible, the simplest being host one or more rich SPA-like subapplications within a larger traditional web application.</p>"},{"location":"architecture/4/#decision","title":"Decision","text":"<p>Traditional Web applications could be required to function in browsers without JavaScript support. As this is not our case, we opt out of such a solution. Besides, most of the modern Frontend frameworks assume SPA as an architectural choice.</p> <p>SPA provides users with a speeded experience because it takes the shortest time to load. What usually makes applications load for longer is if they need to reload each page's HTML every time the user is interacting with it. With single-page apps, on the other hand, there are no additional queries to download pages. SPA application expose a rich user interface with many responsive features, e.g. search with intelligent autocomplete.</p> <p>As a frontend solution, we choose a React-based Single Page Application (SPA) - quite a mature solution with lots of tools, libraries, and a vast community.</p>"},{"location":"architecture/4/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/4/#consequences","title":"Consequences","text":"<p>Communication between SPA and service is usually performed using REST API protocol. It contains only data, with no page representation. The request usually includes either search conditions for some entity (people, organizations, media sources), or blank condition to load all the available data, when it matches the application logic.</p> <p>The response contains data with no connection to the data source (e.g. it contains connected organizations connected to a particular person, rather than database IDs of such an organization. SPA doesn't need to know anything about data representation and the database itself.</p>"},{"location":"architecture/5/","title":"\ud83d\uddf6 ADR-5. MongoDB","text":""},{"location":"architecture/5/#context","title":"Context","text":"<p>Project's growth is expressed not only in the evolution of the code base, but also in including new data sources. Recently it became clear that some of these sources are too abundant and not consistent enough in terms of data schema for a regular relationships-based database.</p>"},{"location":"architecture/5/#decision","title":"Decision","text":"<p>MongoDB provides a solution to this issue, as it is JSON-based and does not care about the schema. A serverless instances was set up with Mongo Atlas, currently holding 1 database (<code>wapa</code>) and 1 collection (<code>telegram-messages</code>). Primary search index will be by <code>channel_id</code>.</p> <p>The <code>telegram-messages</code> collection is intended to store JSON-like objects representing messages published in the <code>relevant</code> telegram channels. Each such object contains a key <code>channel_id</code> which value corresponds to the channel's <code>id</code> in the <code>telegram_channels</code> table on Supabase. In addition, <code>telegram_channels</code> table acquires a new column called <code>history_count</code> which should contain the number of message objects in <code>telegram-messages</code> collection with corresponding <code>channel_id</code>.</p>"},{"location":"architecture/5/#status","title":"Status","text":"<p>Deprecated due to MongoDB not being used anymore. All the data previously stored in Mongo was migrated to Postgres (schema <code>data</code>).</p>"},{"location":"architecture/5/#consequences","title":"Consequences","text":"<p>Mongo does not add too much overhead to the engineering process, as its high-level API follows typical conventions and is relatively easy to fuse into the project. Some updates to the code base would be required. Quite a lot of data analysis and visualization opportunities are expected to open up.</p>"},{"location":"architecture/6/","title":"\ud83d\uddf6\u227d ADR-6. Transcripts","text":""},{"location":"architecture/6/#context","title":"Context","text":"<p>One of the W's most important parts has to do with the various media segments (i.e. talk shows, streams, YouTube videos etc.). Until recently, the main focus in this area was on indicating participants, which is just barely enough to make it useful.</p> <p>Transcripts - text representation of the specific episodes of the relevant media segments - make it significantly more useful. With whisper, an open-source, free-to-use software doing speech recognition of high quality, it is possible to produce transcripts for every relevant media episode in (semi-)automated fashion.</p> <p>Whisper is a machine-learning tool that uses pre-compiled downloadable models (<code>tiny</code> \u2192 <code>small</code> \u2192 <code>medium</code> \u2192 <code>large</code>). The choice of model effects 2 things: processing time and result quality.</p> <p>The quality of the output is pretty great even with <code>medium</code> model (~95%), and <code>large</code> model does an almost perfect job (~99%). Processing time is approximately 3x and 10x on the media duration, respectively. These figures are not exact measurements, but rather a subjective estimation. Processing time is measured on a relatively advanced but still a regular PC.</p> <p>Whisper produces 3 text files: <code>.txt</code>, <code>.str</code>, and <code>.vtt</code>. The <code>.vtt</code>, while could contain additional metadata (such as chapters), in this particular case does not contain anything but timestamps, which makes it no different from the <code>.srt</code> version. <code>.txt</code> can be easily generated from <code>.srt</code>, so essentially only the <code>srt</code> file should be saved.</p>"},{"location":"architecture/6/#decision","title":"Decision","text":"<p>The following structure is implemented around the transcript manufacturing process.</p>"},{"location":"architecture/6/#transcript-flow","title":"Transcript Flow","text":"<p>Transcript flow consists of 4 inter-related scripts:</p> <ul> <li>transfactory, which produces transcripts from audio files;</li> <li>transsuply, which sees to it that there were always enough files for processing;</li> <li>transprocessor, which downloads the <code>factory</code>'s output and saves it to final storage ;</li> <li>transcleanup, which removes processed audio and text files.</li> </ul> <p>The flow is described in more detail in DVE-A-40.</p>"},{"location":"architecture/6/#storage","title":"Storage","text":"<p>The following storage units are used for temporary and permanent storage:</p> <ul> <li>MongoDB (cluster <code>wapaless</code>, collection <code>transcripts</code>) that contains objects of the structure:</li> </ul> <p><pre><code>{\n  \"supa_table\": str \"{segment_name}_vids\",\n  \"supa_id\": int,\n  \"whisper_model\": str \"l\" | \"s\" | \"m\",\n  \"duration\": dict {\"total\": int}, # in seconds\n  \"text\": str \"full_text\",\n  \"srt\": dict {(\"chunk_start_time-chunk_end_time\"): \"chunk_text\"}\n}\n</code></pre> * Supabase storage, bucket <code>prabyss</code>. Used for temporary storage of audio files and resulting <code>.srt</code> files * Supabase storage, bucket <code>back-backups</code>. Used to store all the backups (not just transcripts) as <code>pickle</code> objects.</p>"},{"location":"architecture/6/#auxiliary-database","title":"Auxiliary database","text":"<ul> <li>Supabase DB, schema <code>service</code>, table <code>prabyss</code>. Used to store information on the media episodes to be transcribed, as well as a data exchange point between different parts of the transcript flow.</li> </ul> <p>Additional technical information can be found here and here.</p>"},{"location":"architecture/6/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/6/#consequence","title":"Consequence","text":"<p>This infrastructure is relatively strict and predictable, but also adjustable. In the near future, it would allow an easier implementation of full text search over the large amounts of text. In the more distant future, it would also allow NLP-based analysis and/or translation into English.</p>"},{"location":"architecture/7/","title":"\u221e ADR-7. Media Episodes Interface","text":""},{"location":"architecture/7/#context","title":"Context","text":"<p>Wapaganda DB contains data that is currently not used in the project, or access to it is limited. One such cluster is the media episodes data, including dates (limited availability), metadata (limited availability), participants (work in progress) and transcripts produced by Whisper. The project will benefit from unlocking this data and making it as accessible to the user as possible.</p>"},{"location":"architecture/7/#decision","title":"Decision","text":"<p>In addition to the <code>people</code> interface, a parallel interface for the media episodes should be developed. On the highest level, this should be a list of available <code>media segments</code>.</p> <p>A level deeper - a list of all existing episode per segment (may or may not be limited by dates - for example, to show only starting with Feb 24). For each episode in the list, a list of participants should be available on this level, along with a date, duration and a link to the <code>episode page</code>.</p> <p>Another level deeper is an <code>episode page</code>, which contains the same basic data plus possible additional data, and a CTA to the <code>transcript page</code>.</p> <p><code>transcript page</code> should be a separate thing to enable future <code>mark-up</code> development.</p>"},{"location":"architecture/7/#status","title":"Status","text":"<p>Cooking.</p>"},{"location":"architecture/7/#consequences","title":"Consequences","text":"<p>With this initiative completed, there should be 2 points of entry to a <code>transcript page</code>: apart from the one described here, a user should be able to get a link to this page by visiting a <code>person</code> page \u2192 <code>airtime</code> tab \u2192 <code>date</code> selection \u2192 <code>episode</code> selection.</p>"},{"location":"architecture/8/","title":"\u221e ADR-8. Transcript Mark-Up","text":""},{"location":"architecture/8/#context","title":"Context","text":"<p>Media transcripts represent significant amounts of textual data that, at some point, will be analyzed via the NLP flow (not developed). This flow will produce additional data that would require appropriate representation in the UX/UI.</p>"},{"location":"architecture/8/#decision","title":"Decision","text":"<p>There should be, on the one hand, a tool to 1) create custom tags, and to 2) easily mark specific parts of the text with these tags; and on the other, a special user view that would display these tags appropriately.</p> <p>There should be an approximate list of potentially useful tags.</p>"},{"location":"architecture/8/#status","title":"Status","text":"<p>Cooking.</p>"},{"location":"architecture/8/#consequences","title":"Consequences","text":"<p>Undetermined.</p>"},{"location":"architecture/9/","title":"\ud83d\uddf6\u227d ADR-9. Wapaganda Localization [1]","text":""},{"location":"architecture/9/#context","title":"Context","text":"<p>Given the origins and the nature of the project, 3 languages need to be available for the end user: Ukrainian (primary), English (to make it accessible to as many researchers as possible) and Russian (because most of the source material is in Russian).</p> <p>Localization in all 3 languages must be supported on 2 levels:</p> <ol> <li>Website components</li> <li>Data.</li> </ol>"},{"location":"architecture/9/#data-layer","title":"Data Layer","text":"<p>To support localization on the data level, most of the relevant and translatable data pieces in the DB have infrastructure for all 3 languages, which is implemented in 1 of 2 possible ways:</p> <ol> <li>Separate columns, such as <code>fullname_en</code>, <code>fullname_uk</code>, <code>fullname_ru</code> etc in the <code>people</code> table.</li> <li>Single column of <code>JSONB</code> type, holding a dictionary with structure <code>{\"en\": \"eng_version\", \"ru\": \"ru_version\", \"uk\": \"uk_version\"}</code></li> </ol> <p>NOTE: While infrastructure is almost guaranteed, the data pieces themselves may be (in fact, quite likely to be) missing in various combinations.</p>"},{"location":"architecture/9/#website-components-layer","title":"Website Components Layer","text":"<p>See DVE-A-34.</p>"},{"location":"architecture/9/#decision","title":"Decision","text":"<ul> <li>Identify localization options given that accepted frontend framework is React (DVE-83)</li> <li>Work out a transition plan, in order to phase out the <code>web2py</code>-native approach to localizing website components (DVE-84).</li> <li>Work out an approach to switching supported languages for both relevant layers simultaneously (DVE-42).</li> </ul>"},{"location":"architecture/9/#status","title":"Status","text":"<p>Cooking.</p>"},{"location":"architecture/9/#consequences","title":"Consequences","text":"<p>Unclear.</p>"},{"location":"architecture/template/","title":"ADR-XX TEMPLATE","text":""},{"location":"architecture/template/#context","title":"Context","text":""},{"location":"architecture/template/#decision","title":"Decision","text":""},{"location":"architecture/template/#status","title":"Status","text":"<p>Active.</p>"},{"location":"architecture/template/#consequences","title":"Consequences","text":""},{"location":"db/additional_schemas/","title":"Additional Schemas","text":"<p>All the tables actively used in the project belong to the <code>public</code> schema. To support storage of additional data that might be useful in the future for the current project, or a related one, a number of additional schemas were created: </p> <ul> <li><code>future</code> </li> <li><code>service</code></li> <li><code>data</code></li> <li><code>enums</code></li> <li><code>auth</code></li> <li><code>inferred</code></li> </ul>"},{"location":"db/additional_schemas/#service","title":"Service","text":"<p><code>service</code> schema is meant to contain additional data that is not explicitly related to the project but is required for some related purpose. Notable tables include:</p> <ul> <li><code>prabyss</code>. </li> <li>TBD</li> </ul>"},{"location":"db/additional_schemas/#future","title":"Future","text":"<p><code>future</code> schema contains a number of tables that may become a part of the current project in the future, or may span out a separate project. As of the time of creation, the following tables were attributed to the <code>future</code> schema:</p> <ul> <li><code>meduza_dow_stream</code>    Contains parsed messages of the Meduza daily text streams on the ongoing events. Parsed here means that empty/technical messages are filtered out, and the data format is standardized, but the content is not verified for relevance. Most recently updated on May 2, 2023, with data up to May 1, 2023.</li> <li><code>modrf_briefings</code> outdated Data on the MOD RF briefings has been collected from various sources, including Telegram, VK and YouTube. Some of the data is cleaned and JSONified, but only to a certain point.</li> <li><code>tryvoga_alerts</code> outdated Data on the air-raid alerts in localities of Ukraine. Obtained by parsing Telegram channel.</li> <li>TBD</li> </ul>"},{"location":"db/additional_schemas/#data","title":"Data","text":"<p><code>data</code> schema contains data-heavy entities, namely:</p> <ul> <li><code>telegram_messages</code>. Related to <code>telegram_channels</code> in <code>public</code> schema. Contains posts published in <code>relevant</code> telegram channels. Constitutes ~90% of the DB size.</li> <li><code>telegram_channels_stats</code></li> <li><code>printed_content</code>. Related to <code>printed</code> in <code>public</code> schema. Contains chapters of books, articles, blog posts.</li> <li><code>transcripts</code> &amp; <code>transcribed_content</code>. Table <code>transcripts</code> contains basic info on the transcribing jobs completed by <code>transfactory</code> (related to various tables in <code>public</code> such as <code>smotrim_episodes</code>). Table <code>transcribed_content</code> contains lines from the resulting <code>srt</code> files processed into convenient format (related to <code>transcripts</code>)</li> <li>TBD</li> </ul>"},{"location":"db/additional_schemas/#using-tables-in-alternative-schemas","title":"Using Tables in Alternative Schemas","text":"<p>To allow using an additional schema called <code>schemaname</code>, the following steps must be complete:</p> <ol> <li> <p>Create schema <code>schemaname</code>;</p> </li> <li> <p>Run these 2 queries:</p> </li> </ol> <pre><code>GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA schemaname TO &lt;db_role&gt;;\nGRANT USAGE ON SCHEMA schemaname TO &lt;db_role&gt;;\n</code></pre> <p>where <code>&lt;db_role&gt;</code> is the name of the database user you wish to enable the usage for.</p>"},{"location":"db/additional_schemas/#moving-table-to-another-schema","title":"Moving table to another schema","text":"<pre><code>alter table tablename set schema schemaname;\n</code></pre>"},{"location":"db/additional_schemas/#querying-data-in-other-schemas","title":"Querying data in other schemas","text":"<p>TBD</p>"},{"location":"db/install/","title":"Installation Details","text":""},{"location":"db/install/#database","title":"Database","text":"<p>...</p>"},{"location":"db/install/#updating-db-ssl-certificate","title":"Updating DB SSL certificate","text":"<p>...</p> <pre><code>openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout /etc/ssl/private/ssl-cert-snakeoil.key -out /etc/ssl/certs/ssl-cert-snakeoil.pem\n</code></pre>"},{"location":"db/install/#extensions","title":"Extensions","text":""},{"location":"db/install/#pgroonga","title":"pgroonga","text":"<p>Starting page on the installation in general. For installing on Ubuntu-based service, follow <code>How to install for the official PostgreSQL</code> section on this page. Copy of the instruction:</p> <pre><code>$ sudo apt install -y software-properties-common\n$ sudo add-apt-repository -y universe\n$ sudo add-apt-repository -y ppa:groonga/ppa\n$ sudo apt install -y wget lsb-release\n$ wget https://packages.groonga.org/ubuntu/groonga-apt-source-latest-$(lsb_release --codename --short).deb\n$ sudo apt install -y -V ./groonga-apt-source-latest-$(lsb_release --codename --short).deb\n$ echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release --codename --short)-pgdg main\" | sudo tee /etc/apt/sources.list.d/pgdg.list\n$ wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\n$ sudo apt update\n$ sudo apt install -y -V postgresql-15-pgdg-pgroonga # update to actual version of Postgres\n</code></pre> <pre><code>CREATE EXTENSION pgroonga;\n</code></pre>"},{"location":"db/install/#pg_cron","title":"pg_cron","text":"<p>Official page.</p> <pre><code>sudo apt-get -y install postgresql-16-cron # update to actual version of Postgres\n</code></pre> <p>Update DB configuration (<code>postgresql.conf</code>):</p> <pre><code>shared_preload_libraries = 'pg_cron'\ncron.database_name = 'postgres' # set to actual name of the db\ncron.timezone = 'PRC' # optionally set the TZ\n</code></pre> <pre><code>CREATE EXTENSION pg_cron\n</code></pre>"},{"location":"db/install/#pg_prewarm","title":"pg_prewarm","text":"<p>TBD</p>"},{"location":"db/install/#plpython3u","title":"plpython3u","text":"<pre><code>sudo apt-get update\nsudo apt-get install postgresql-server-dev-all\nsudo apt-get install python3 python3-dev\nsudo apt-get install postgresql-plpython3\n</code></pre> <pre><code>CREATE EXTENSION plpython3u;\n</code></pre>"},{"location":"db/install/#vector","title":"vector","text":"<p>...</p>"},{"location":"db/install/#cube","title":"cube","text":"<p>...</p>"},{"location":"db/new_table_checklist/","title":"New Table Checklist","text":"<p>When adding a new table to the W database, one must consider, first, whether it is a central core table, or a satellite table.</p> <p> </p> <p>\u200b   Central core table is a table that stores data directly usable in the project one way or another. Most tables in the database belong to this group. Satellite table contains secondary data, most often related to internal processes, such as maintaining jobs queue or storing quality control information.</p>"},{"location":"db/new_table_checklist/#steps","title":"Steps","text":"<ul> <li>Before creating table in the db, create a SQLModel in the repository - this would allow you to contemplate on the table structure without committing changes to the db until it's finalized.</li> <li>Before creating table in the db, make sure table contains common fields, as is relevant for each specific case:</li> <li><code>added_at</code> -- for a timestamp indicating moment of creating this record.</li> <li><code>updated_on</code> -- for a timestamp indicating the most recent moment this record was updated.</li> <li><code>aired_on</code> -- Media-episode tables only for a timestamp when a media episode was published to the platform.</li> <li><code>was_downloaded</code> -- Media-episode tables only for a boolean indicating if the media item represented by this record was downloaded.</li> <li><code>media_segment_id</code> -- Media-episode tables only foreign key to <code>public.media_segments</code>.</li> <li><code>cluster_id</code> -- Media-episode tables only foreign key to <code>data.media_episodes_clusters</code>.</li> <li><code>relevance_status</code> -- ENUM type indicating relevance of this record to the project's essence.</li> <li><code>is_defunct</code> -- boolean type to indicate whether or not entity represented by this record is still active.</li> <li>...</li> <li>When creating table in the db</li> <li>Make sure to use <code>TEXT</code> and not <code>VARCHAR</code></li> <li>Use <code>SERIAL</code> IDs as primary keys where possible; composite keys if needed. Use UUIDs only as a last resort.</li> <li>If the table is expected to have translatable or transliteretable values, use <code>TripleLang</code> composite type for them. More details here.</li> <li> <p>...</p> </li> <li> <p>Create additional indexes for fields that must be unique and/or fields that you expect to query often.</p> </li> <li>If you expect to be doing full-text search against the table, create <code>pgroonga</code> index in addition to indexes in the step above.</li> <li>After creation, enable provenance mechanism. As a general rule, you want it enabled for a central core table, but not for a satellite table.</li> <li>Make sure all relevant db roles can query and/or update the table.</li> </ul>"},{"location":"db/new_table_checklist/#enabling-provenance","title":"Enabling provenance","text":"<p>Enabling <code>provenance</code> flow for a table, means creating for this table one of the following triggers:</p> <pre><code>-- applicable to most tables\nCREATE OR REPLACE TRIGGER trg_TABLE_NAME\nAFTER UPDATE OR DELETE OR INSERT ON SCHEMA_NAME.TABLE_NAME\nFOR EACH ROW\nEXECUTE FUNCTION trg_change();\n\n-- applicable to data-heavy tables where a lot of insert operation are expected to happen in automated fashion\nCREATE OR REPLACE TRIGGER trg_TABLE_NAME\nAFTER UPDATE OR DELETE ON SCHEMA_NAME.TABLE_NAME\nFOR EACH ROW\nEXECUTE FUNCTION trg_change();\n</code></pre> <p>where <code>TABLE_NAME</code> and <code>SCHEMA_NAME</code> should be replaced with actual table and schema names.</p>"},{"location":"db/new_table_checklist/#creating-indexes","title":"Creating Indexes","text":"<p>TBD</p>"},{"location":"db/new_table_checklist/#additional-checks","title":"Additional Checks","text":"<p>TBD</p>"},{"location":"db/people_bundles/","title":"People Bundles","text":"<p>Bundles are custom groups of people. 4 types of bundles are defined (as of November 27, 2022).</p>"},{"location":"db/people_bundles/#subjective","title":"Subjective","text":"<p>Subjective bundle is a virtual group where membership is defined by purely subjective speculations.</p> <ul> <li>Alias: <code>s</code></li> <li>Example: <code>{\"en\": \"Mean Old Jerks Club\", \"ru\": \"\u041a\u043b\u0443\u0431 \u0441\u0442\u0430\u0440\u044b\u0445 \u0437\u043b\u043e\u0431\u043d\u044b\u0445 \u043c\u0443\u0434\u0430\u043a\u043e\u0432\"}</code></li> </ul> ID Name Reasoning 1 Mean Old Jerks Club A person is aged, and behaves like a mean jerk 6 Likely random people A person is probably not related to propagandistic activities 58 Sociopaths A person has spoken at least once in favor of either public executions, mass executions or death squads, or have demonstrated sociopathic tendencies otherwise 63 Opportunists A person is likely to not have solid convictions of their own"},{"location":"db/people_bundles/#objective","title":"Objective","text":"<p>Objective bundle is a virtual group where membership could be defined via objectively existing features.</p> <ul> <li>Alias: <code>o</code></li> <li>Example: <code>{\"en\":\"Solovyov pack\",\"ru\":\"\u0421\u043e\u043b\u043e\u0432\u044c\u0435\u0432\u0441\u043a\u0430\u044f \u0441\u0442\u0430\u044f\"}</code></li> </ul> ID Name Reasoning 53 Own correspondents abroad A person works for a Russian media in the capacity of foreign correspondent 52 Religious figure A person is a priest, a shaman, a mufti or some other kind of professional god servant 47 Military correspondent A person is a correspondent (journalist) and has worked in the war zone is this capacity 45 International surveyors A person is a foreign national (towards Russia) and has participated in at least one RF electoral episode as a surveyor 42 Diplomats A person has served as a professional diplomat 25 PhD A person has a doctorate 24 Afghan veterans A person has participated in the Soviet-Afghan war 1980-1989 22 Communists A person has expressed their allegiance to the idea of communism 19 Influencers A person actively works towards gaining larger audience online 21 Religious fanatics A person has expressed extreme ideas of religious essence 20 Politicians A person has participated in formal political process 75 Soldiers A person is a part of an army, with any kind of rank 2 Solovyov pack A person has at least once appeared on air on Solovyov Live TV channel in the capacity of a host, or a reporter 54 Cultural figure A person is a professional singer, writer, poet, performer, musician etc. and acquired some fame in that capacity Nazi A person has expressed at least one of the following convictions:- that Russian nation is superior;- that Ukrainian nation is inferior;- that Ukrainian nation is not a nation (i.e. did not occur naturally but was somebody's project);- TBD"},{"location":"db/people_bundles/#expert","title":"Expert","text":"<p>Expert bundle is a virtual group where membership is defined by claimed expertise in any field of activity/knowledge.</p> <ul> <li>Alias: <code>e</code></li> <li>Example: <code>{\"en\":\"Political science\",\"ru\":\"\u041f\u043e\u043b\u0438\u0442\u043e\u043b\u043e\u0433\u0438\u044f\",\"uk\":\"\u041f\u043e\u043b\u0456\u0442\u043e\u043b\u043e\u0433\u0456\u044f\"}</code></li> </ul>"},{"location":"db/people_bundles/#national","title":"National","text":"<p>National bundle is a virtual group where membership is defined by either [claimed | actual] belonging to an ethnic group, or citizenship.</p> <ul> <li>Alias: <code>n</code></li> <li>Example: <code>{\"en\":\"Ukrainians\",\"ru\":\"\u0423\u043a\u0440\u0430\u0438\u043d\u0446\u044b\",\"uk\":\"\u0423\u043a\u0440\u0430\u0457\u043d\u0446\u0456\"}</code></li> </ul>"},{"location":"db/popular_stats/","title":"Popular Stats","text":"<p>Popular Stats is a nickname for a layer of components showing current stats related to the project. </p> <p>The following pieces of statistics should be calculated and displayed:</p> <ul> <li>Number of patients (population)</li> <li>Average/Median age across the db</li> <li>Gender distribution (male v. female)</li> <li>Number of registered media hours</li> <li>Number of transcribed media hours</li> <li>Number of registered and relevant media segments</li> <li>Number of relevant organizations</li> <li>Number of relevant Telegram channels</li> <li>Number of written works on file (articles, books)</li> <li>TBD</li> </ul> <p>UI TBD</p>"},{"location":"db/psycopg/","title":"psycopg In a Gist","text":"<p><code>psycopg</code> is an advanced Python adapter/connector for PostgreSQL databases. The library is at v3, but goes without a version in the name, i.e. simply <code>psycopg</code>, as opposed to <code>psycopg2</code>, which is a previous generation. The documentation can be found on a dedicated website, and is relatively full.</p>"},{"location":"db/psycopg/#installation","title":"Installation","text":"<p>Installation is more or less straightforward:</p> <pre><code>poetry add psycopg[binary]\n</code></pre>"},{"location":"db/psycopg/#usage","title":"Usage","text":"<p>Main objects are <code>connection</code> and <code>cursor</code>.</p> <p>You would usually create a <code>connection</code>, and then a <code>cursor</code> that you'd use to perform transactions. In the previous version of the library, <code>cursor</code> was the primary object you dealt with. With the new version, you don't need to create a cursor (though you still can), because <code>connection</code> object has the same <code>execute</code> method, which creates a <code>cursor</code> object implicitly when called.</p> <p>The library is rather versatile, and allows various approaches:</p> <ul> <li>you can use main objects as context managers;</li> <li>there is an async version for every function;</li> <li>the format of the return result can be customized with <code>row_factories</code>;</li> <li>To get dictionaries, pass <code>row_factory=psycopg.rows.dict_row</code> when creating connection;</li> <li>you can configure your own types if you need to.</li> </ul>"},{"location":"db/psycopg/#caveats","title":"Caveats","text":"<p>Psycopg has a behaviour that may seem surprising compared to psql: by default, any database operation will start a new transaction. As a consequence, changes made by any cursor of the connection will not be visible until <code>Connection.commit()</code> is called, and will be discarded by <code>Connection.rollback()</code>. The following operation on the same connection will start a new transaction.</p> <p>If a database operation fails, the server will refuse further commands, until a <code>rollback()</code> is called.</p> <p>This can be circumvented by setting <code>autocommit = True</code> when creating connection, or using transaction context.</p> <p>To access table in a schema other than <code>public</code>, use construct <code>sql.Identifier(\"schema_name\", \"table_name\")</code>.</p>"},{"location":"db/psycopg/#types-adaptation","title":"Types Adaptation","text":"<ul> <li>With <code>boolean</code>s, <code>numeric</code>s, <code>string</code>s, <code>uuid</code>s or <code>binary</code> types are converted automatically and predictably;</li> <li>With <code>date</code>s <code>datetime</code>s etc, it should be <code>datetime</code> objects on Python side;</li> <li>With <code>json</code>, standard serializers are used by default, but it can be customized;</li> </ul>"},{"location":"db/psycopg/#sql-query-composition","title":"SQL Query Composition","text":"<p>Simple queries can be passed as regular strings, but it is advisable to default to the safe way by using the <code>sql</code> module of the library (mostly, to establish a habit, since more complex types require it anyway). A few examples:</p> <pre><code>from psycopg import sql\n\nselect_all = sql.SQL(\"SELECT * FROM {tbl};\").format(tbl=sql.Identifier(\"people\"))\nreq = connect.execute(select_all )\n\none_field = sql.SQL(\"SELECT {id} FROM {tbl};\").format(tbl=sql.Identifier(\"people\"), id=sql.Identifier(\"id\"))\nreq = connect.execute(one_field )\n\nseveral_fields_w_condition = sql.SQL(\"SELECT {fields} from {tbl} WHERE {condition_field} = 'complete';\").format(\n    tbl=sql.Identifier(\"prabyss\"),\n    fields=sql.SQL(\",\").join([sql.Identifier(\"id\"), sql.Identifier(\"title\")]),\n    condition_field=sql.Identifier(\"status\")\n)\nreq = connect.execute(several_fields_w_condition)\n\nselect_key_of_jsonb_column = sql.SQL(\"SELECT {fld} FROM {tbl} WHERE {fld2} = %s\").format(\n    fld=sql.SQL(\"jsonb_extract_path({content, 'en', 'markdown'})\"),\n    tbl=sql.Identifier(\"theory\"),\n    fld2=sql.Identifier(\"id\")\n)\nreq = connect.execute(select_key_of_jsonb_column , (record_id,)).fetchone()\n\nupdate_key_in_jsonb_column = sql.SQL(\"UPDATE {tbl} SET {fld} = {jset} WHERE {fld2} = %s\").format(\n    fld=sql.Identifier(\"content\"),\n    tbl=sql.Identifier(\"theory\"),\n    fld2=sql.Identifier('id'),\n    jset=sql.SQL(\"jsonb_set({fld}, '{pth}', %s)\").format(fld=sql.Identifier(\"content\"), pth = sql.SQL('{en}'))\n)\nconnect.execute(update_key_in_jsonb_column , (json.dumps({\"markdown\": new_version}), record_id))\n</code></pre> <p>A few things to notice:</p> <ul> <li>There are several different objects used in query composition, the most common of which are <code>Identifier</code> and <code>SQL</code>. Use first to point to columns and tables; second - to wrap all other portions of the query;</li> <li>Curly brackets are used for variable placeholders. This is similar to how <code>f-strings</code> are formed in Python, and this similarity may be confusing. The rule of thumb would be to assume that <code>f-strings</code> cannot be used at all when shaping queries, even though in pieces of SQL that do not contain variable placeholders, they are still possible.</li> <li>A special <code>%s</code> placeholder is used to point at values. At execution stage, values must be passed in an iterable (even if there's just one): <code>(record_id,)</code> and not simply <code>record_id</code>.</li> <li>It is possible to use named placeholders, i.e. <code>%(title)s</code>, in which case, the iterable must be a dictionary where keys correspond to the names of placeholders.</li> </ul>"},{"location":"db/psycopg/#copy","title":"Copy","text":"<p>For massive inserts, <code>copy</code> operation is preferable. Below is an example of inserting objects from a Python iterable <code>parsed_records</code>, which is a list of dictionaries:</p> <pre><code>from psycopg import sql\nfrom wapaganda.database.core2 import create_connection\n\nparsed_records = [...] # this list of full of dictionaries\nconnect = create_connection()\ncur = connect.cursor()\n\nupload_query = sql.SQL(\"COPY {tbl} ({fields}) FROM STDIN\").format(\n    tbl=sql.Identifier(\"schema\", \"table_in_schema\"),\n    fields=sql.SQL(\",\").join([sql.Identifier(x) for x in parsed_records[0].keys()])\n)\n\nwith cur.copy(upload_query) as copy:\n    for rec in parsed_records:\n        copy.write_row(list(rec.values()))        \n</code></pre> <p>Things to notice:</p> <ul> <li><code>create_connection</code> function returns <code>connection</code> object with proper auth elements;</li> <li><code>copy</code> operation is performed on the <code>cursor</code> object, not the <code>connection</code> object;</li> <li>list comprehension is applied to the 1st record in the list to obtain field names. Note: this requires for the <code>parsed_records</code> list to contain at least one record, i.e. the query has to be declared after you collected the data;</li> <li>when using dictionaries, pass <code>.values()</code> to <code>write_row()</code> method and don't forget to convert it into <code>list</code>.</li> </ul> <p><code>copy</code> can be used for other things, such as loading data from a CSV file, or copying data between tables. Consult Postgres documentation for full functionality.</p>"},{"location":"historical/clusterfuck1/","title":"The 221223 Clusterfuck Postmortem","text":"<p>On Friday, December 22, 2023 files from the data folder of the production database got removed due to negligence on the part of the project's owner.</p>"},{"location":"historical/clusterfuck1/#context","title":"Context","text":"<p>On Monday, December 18, 2023 I started a major refactoring work on the database (TODO: some references). The day before, on Dec 17, 2023, a backup of data was created by @atatatko on my request. The refactoring progressed quite well, although not without bumps and obstacles. Part of refactoring involved updates to the largest table we have, <code>telegram_messages</code>, in particular - fixing an error with missing subtype, and optimizing <code>content</code> field by dropping unnecessary data. Since the latter operation had to be performed on every record in the table, and the process was taking rather long due to its size, a function was developed to perform the necessary processing with PL\\Python and then scheduled to run every 30 seconds, multiple times.</p>"},{"location":"historical/clusterfuck1/#event","title":"Event","text":"<p>At a certain point, I had 100 of these crawlers updating <code>telegram_messages</code>. Then, in a moment of madness, I decided that the type of field for <code>telegram_channel_id</code> just has to be <code>bigint</code> instead of some measly <code>integer</code>. Launching this update led to a deadlock and inevitable corruption of the index associated with the <code>content</code> field, and to the database going unresponsive.</p> <p>I tried restarting the service, but discovered that the main drive is all out of space, which was not normal, given the size of the database at the time at around 45Gb (the drive is ~150Gb). I lockated the directory that occupied the most space; it was Postgres' data directory. In the second moment of madness, I removed everything from it indiscriminately.</p> <p>This went down at about 16:00 Kyiv time.</p>"},{"location":"historical/clusterfuck1/#damage-control-discoveries","title":"Damage Control &amp; Discoveries","text":"<p>The data was removed in a particularly nasty way, as it not only led to the loss of data, but made the database unaccessible via regular means. It was still possible to connect to it directly with <code>sudo -u postgres psql</code> This is how I discovered that only <code>wapadb</code> database was killed, while the secondary databases ( <code>logs</code>, <code>windmill</code>) were pretty much intact. I exported data from all their tables from within <code>psql</code>. </p> <p>Attempt to recover the deleted files with Linux utility <code>testdisk</code> &gt; <code>photorec</code> did produce some files, but nothing that looks like db data fiiles.</p> <p>Then I saw no other choice but to set up instance from scratch. This involved installing Postgres anew and restoring 1) schema; 2) data.</p> <p>Version 16 of the database was installed instead of v15, because why not.</p> <p>For database schema, we had a dump dated October 31, 2023. Using the fact that Pycharm still had the db metadata saved locally, I manually updated the dump with current definitions. Then I took multiple attempts on running the migration, fixing errors (with missing schema qualification, tables placed in the wrong place, etc.) along the way, until I finally got it right. Some missing pieces were restored from project files and documentation afterwards.</p> <p>The data backup, which was supposed to have been made on December 17, was missing some tables ( <code>dentv_episodes</code>), and for others, the most recent records were dated Nov 3, 2023. Data from the backup was restored without much issue, however due to the gap between most recent records and the clusterfuck event, restoration of data from local files (such as <code>.srt</code> files produced by <code>transfactory</code>) and/or updating tables via regular means, apparently, led to some mixups (specifically noticed on <code>smotrim_episodes</code>). The cause of mixup is unclear.</p> <p>By EOD Saturday, December 23, access to the database was restored, most of the crucial data was restored, and web application was back online.</p>"},{"location":"historical/clusterfuck1/#conclusions-and-updates","title":"Conclusions and Updates","text":"<ol> <li>Instead of <code>logs</code> database, <code>meta</code> database was created to hold logs, provenance records and other similar stuff.</li> <li>Backup procedure must be established in the nearest future to avoid data loss / mixup.</li> <li>Id field on <code>provenance</code> table is now <code>bigint</code>, not <code>uid</code>.</li> <li>Lost data:</li> <li><code>public.dentv_episodes</code></li> <li><code>service.prabyss</code></li> <li><code>service.factory_jobs_run_details</code></li> <li><code>service.provenance</code></li> <li><code>future</code> ?</li> <li>TBD</li> </ol>"},{"location":"historical/onboarding1/","title":"Quick Onboarding. General Information and Setting Up","text":"<p>Warning</p> <p>This is nowhere near current</p>"},{"location":"historical/onboarding1/#architecture","title":"Architecture","text":"<p>Web application is implemented with React on frontend and Django on backend; in addition, <code>wapatools</code> is a Python repo with various tools, some of which are used by the web app, while others have to do with collection, parsing and cleaning the data. More details on the web app could be found in DVE-A-52.</p>"},{"location":"historical/onboarding1/#python-dependency-management","title":"Python Dependency Management","text":"<p>Dependencies for the web app [???????]</p> <p><code>wapatools</code> is handled with Poetry and follows the polylith approach with regard to repository structure. See DVE-A-59 for details of transitioning the repo to this structure.</p>"},{"location":"historical/onboarding1/#database","title":"Database","text":"<p>The project is built around a database, which is hosted on Supabase, - a service providing an alternative to Firebase (a Google product). More information on it could be found here DVE-A-11. Important: as a general rule, tables and columns have comments that aim to be as descriptive as possible. They can be retrieved with a SQL function (or a Supabase API call that launches the same function).</p>"},{"location":"historical/onboarding1/#snaplet","title":"Snaplet","text":"<p>Snaplet is a tool that allows you to create and use database snapshots. We need it so that we don't have to develop the codebase against the prod DB thus risking all kinds of stuff. See Snaplet 101 for more details.</p>"},{"location":"historical/onboarding1/#service","title":"Service","text":"<p>The application is hosted on GitHub (repository) and Railway. Railway is a platform for hosting applications of all sorts. Secrets are handled by Doppler. See DVE-A-5 for setup instructions.</p>"},{"location":"historical/original_setup/","title":"Original Setup","text":"<p>This document describes the configuration of the Wapaganda project as set up for the first time. Versions are current as of the moment of writing this (October 5, 2022).</p>"},{"location":"historical/original_setup/#hardware","title":"Hardware","text":"<p>Processor   Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz, 2592 Mhz</p>"},{"location":"historical/original_setup/#software","title":"Software","text":""},{"location":"historical/original_setup/#operating-system","title":"Operating system","text":"<p>Windows 10 (Version 10.0.19044 Build 19044) x64</p>"},{"location":"historical/original_setup/#python","title":"Python","text":"<p>3.10.5 (3.10 is the minimum)</p>"},{"location":"historical/original_setup/#ide","title":"IDE","text":"<p>PyCharm 2022.2.2 (CE) Build #PC-222.4167.33, built on September 15, 2022</p>"},{"location":"historical/original_setup/#runtime-version","title":"Runtime version","text":"<p>17.0.4+7-b469.53 amd64</p>"},{"location":"historical/original_setup/#vm","title":"VM","text":"<p>OpenJDK 64-Bit Server VM by JetBrains s.r.o.</p>"},{"location":"historical/original_setup/#other","title":"Other","text":"<ul> <li>Scoop</li> <li>Doppler CLI (installed through <code>scoop</code>)<ul> <li><code>doppler-env</code> (Python dependency; installed through <code>pip</code>)</li> </ul> </li> <li>Railway CLI (installed through <code>scoop</code>)</li> <li>WSL + Ubuntu 20.04.5 LTS Canonical (v. 2004.5.11.0)</li> <li>curl (installed through <code>WSL</code>)</li> <li>Snaplet CLI (installed through <code>WSL</code> -&gt; <code>curl</code>)</li> <li>Browsers (to access the interface locally):</li> <li>Firefox 105.0.1 (64-bit)</li> <li>Chrome Version 106.0.5249.91 (Official Build) (64-bit)</li> </ul>"},{"location":"historical/personal_info_addendum/","title":"Personal Information Addendum","text":"<p>Some of the records in the <code>people</code> table contain additional information that might be considered sensitive by some. This document is a list of fields containing such information.</p> <p><code>raw</code> \u00a0usually refers to strings, and means that it was taken from the source as is, i.e. a string may contain spaces and other unwanted characters</p>"},{"location":"historical/personal_info_addendum/#address","title":"<code>address</code>","text":"<p>List of addresses that are possibly associated with the person.</p>"},{"location":"historical/personal_info_addendum/#type","title":"Type","text":"<p><code>List</code> of strings (raw)</p>"},{"location":"historical/personal_info_addendum/#contact","title":"<code>contact</code>","text":""},{"location":"historical/personal_info_addendum/#type_1","title":"Type","text":"<p><code>JSONB</code></p>"},{"location":"historical/personal_info_addendum/#possible-keys","title":"Possible keys","text":"<ul> <li><code>phones</code> List of strings (raw)</li> <li><code>emails</code> List of strings</li> <li><code>telegram</code> List of strings (raw). Refers to personal account in Telegram. Maybe in different formats</li> </ul>"},{"location":"historical/personal_info_addendum/#associates","title":"<code>associates</code>","text":"<p>List of people known to be associated with the person.</p>"},{"location":"historical/personal_info_addendum/#type_2","title":"Type","text":"<p><code>List</code> of <code>JSONB</code></p>"},{"location":"historical/personal_info_addendum/#required-keys","title":"Required keys","text":"<ul> <li><code>relationship</code>. String. Must always be present. Indicates the type of relationship connecting person with the associate. Can be <code>wife</code>, <code>mother</code>, <code>son</code>, <code>partner</code>, <code>collegue</code> etc.</li> <li><code>id</code>. Integer. Either this or <code>name</code> must be present. Refers to the ID of a person in the <code>people</code> table. If present, no other keys are required except <code>relationship</code>.</li> <li><code>name</code>. JSON object - triple language format. Either this or <code>id</code> must be present.</li> </ul>"},{"location":"historical/personal_info_addendum/#possible-keys_1","title":"Possible keys","text":"<ul> <li><code>dob</code>. String. The format is usually <code>YYYY-MM-DD</code></li> <li><code>contact</code>. JSON object similar in structure to the <code>contact</code> field.</li> <li><code>additional</code>. JSON object similar in structure to the <code>additional</code> field.</li> <li><code>address</code>. List of strings (raw). Similar in structure to the <code>address</code> field.</li> <li><code>social</code>. List of strings. URLs of person's profile in social networks. Similar in structure to the <code>social</code> field.</li> <li><code>raw</code>.</li> </ul>"},{"location":"historical/personal_info_addendum/#example-object","title":"Example object","text":"<pre><code>[\n  {\n    \"name\": {\"en\": \"Veronica\", \"ru\": \"\u0412\u0435\u0440\u043e\u043d\u0438\u043a\u0430\", \"uk\": \"\u0412\u0454\u0440\u043e\u043d\u0456\u043a\u0430\"}, \n    \"relationship\": \"daughter\"\n  },  \n  {\n    \"dob\": \"1996-08-01\", \n    \"name\": {\"en\": \"Olga Savchenkova\", \"ru\": \"\u041e\u043b\u044c\u0433\u0430 \u0421\u0430\u0432\u0447\u0435\u043d\u043a\u043e\u0432\u0430\", \"uk\": \"\u041e\u043b\u044c\u0433\u0430 \u0421\u0430\u0432\u0447\u0435\u043d\u043a\u043e\u0432\u0430\"}, \n    \"social\": [\"http://polynkova.tilda.ws/#rec167782553\", \"https://kinolift.com/24633\"], \n    \"contact\": \n      {\n        \"emails\": [\"o.polynkova@mail.ru\"], \n        \"phones\": [\"+79002280187\"],\n        \"telegram\": [\"@war_criminal\"]\n      },\n    \"alias\": \n      [\n        {\"en\": \"Captain Nakedbottom\", \"ru\": \"\u041e\u043a\u0441\u0430\u043d\u0430 \u0412\u043f\u0441\u0438\u043d\u0443\u0433\u0440\u044b\u0437\", \"uk\": \"\u041e\u043a\u0441\u0430\u043d\u0430 \u0420\u0430\u0448\u0438\u0441\u0442\u043a\u0430\"}\n      ],\n    \"additional\": \n      {\n        \"passport\": \n          {\n            \"number\": \"66 10 556462\",\n            \"issued by\": \"a russian state agency\",\n            \"issued on\": \"some time in the past\"\n          },\n        \"tin\": \"5131654614654163\",\n        \"edrpou\": \"24563541\",\n      }, \n      \"relationship\": \"wife\"\n      \"raw\": {\"biography\": \"She was born and still lives today\", \"work\": \"that company over there\"}\n  }\n]\n</code></pre>"},{"location":"historical/personal_info_addendum/#additional","title":"<code>additional</code>","text":""},{"location":"historical/personal_info_addendum/#type_3","title":"Type","text":"<p><code>JSONB</code></p>"},{"location":"historical/personal_info_addendum/#possible-keys_2","title":"Possible keys","text":"<ul> <li><code>passport</code>. List of JSON objects. Possible keys:</li> <li><code>number</code>. String (raw)</li> <li><code>issued by</code>. String (raw)</li> <li><code>issued on</code>. String (raw)</li> <li><code>edrpou</code>. String (raw). Refers to a Ukrainian legal entity ID.</li> <li><code>tin</code>. List of strings. Refers to a person's tax identifier. Could be either Russian or Ukrainian, which are differently structured.</li> <li><code>drivers license</code>. String.</li> <li><code>auto</code>. List of JSON objects. Refers to the vehicles associated with the person. Possible keys:</li> <li><code>plate</code>. String</li> <li><code>vin</code>. String</li> <li><code>make</code>. String</li> <li><code>year</code>. String</li> <li><code>urls</code>. List of strings. Dumpster for URLs that have anything to do with the person.</li> </ul>"},{"location":"historical/snaplet/","title":"Snaplet 101","text":"<p>Snaplet was a tool for creating and using database snapshots for development purposes. Snaplet has been decommissioned in 2024</p>"},{"location":"historical/snaplet/#setting-up","title":"Setting Up","text":""},{"location":"historical/snaplet/#prerequisites","title":"Prerequisites","text":"<ol> <li>Two Postgres instances, one of which is the <code>production</code> database, and the other is <code>development</code> database (empty);</li> <li>A read-only role in the production Postgres project;</li> <li>Superuser access to the development DB Postgres project;    <code>ALTER USER postgres WITH superuser;</code></li> <li>Connection strings from both DBs (use read-only role for the <code>production</code> one).</li> <li>Snaplet requires PSQL to be installed</li> <li>Windows: <code>choco install psql</code></li> <li>MacOS: <code>brew install libpq</code></li> <li>Linux: <code>apt install postgresql-client</code></li> <li>PSQL <code>bin</code> directory should be accessible through the <code>PATH</code></li> <li>Windows: <code>set PATH=%PATH%;\"%LOCALAPPDATA%\\Programs\\pgAdmin 4\\v6\\runtime\"</code></li> <li>MacOS: <code>export PATH=\"/usr/local/opt/libpq/bin:$PATH\"</code></li> </ol>"},{"location":"historical/snaplet/#process","title":"Process","text":"<ol> <li> <p>Go to https://www.snaplet.dev/, log into the account; set up the team and project. Use <code>CONNECTION_STRING</code> for the production database and establish a connection.</p> </li> <li> <p>In the 2nd step exclude schemas that you don't want to save; then follow through and make sure the process finishes without issues.</p> </li> <li> <p>Install Snaplet CLI with <code>curl -sL https://app.snaplet.dev/get-cli/ | bash</code> To use on Windows, you have two options: install WSL and an appropriate Linux distribution (for example, canonical Ubuntu 20.04.5) or use Bash which comes with any Windows Git distribution.</p> </li> <li> <p>In the terminal, navigate to your project's local directory and run</p> </li> </ol> <pre><code>snaplet config setup\n</code></pre> <ol> <li> <p>Enter the <code>CONNECTION_STRING</code> for the development database connection string when prompted (it also can be found in the Supabase Dev panel: Project Settings -&gt; Database Settings -&gt; Connections String -&gt; URI). This command creates a <code>.snaplet</code> directory and the configuration files used by the CLI</p> </li> <li> <p>Later, during making changes to the development database, you may want to switch between the development and production URL. The safest option to do that is through pre-configured Doppler secrets. Use <code>SNAPLET_SOURCE_DATABASE_URL</code>for creating the snapshot, and <code>SNAPLET_DATABASE_URL</code> for restoring one. We'll be using exactly this method below.</p> </li> </ol> <p></p>"},{"location":"historical/snaplet/#using-snaplet","title":"Using Snaplet","text":""},{"location":"historical/snaplet/#to-create-a-snapshot","title":"To create a snapshot","text":"<p>Documentation</p> <p>We assume, all requirements are met, including the <code>SNAPLET_SOURCE_DATABASE_URL</code> secret in Doppler. To access the secret from the production environment, you should use the <code>Snaplet prod</code> Service token. You generate the token once, and keep it in a password manager. More about Sevice Tokens see in a related article or a Doppler official documentation</p> <pre><code># Prevent command with Service Token being recorded in bash history\nexport HISTIGNORE='doppler run*'\ndoppler run --token='dp.st.prd.xxxx' -- snaplet snapshot capture\n</code></pre> <p>Snapshots are automatically stored in <code>$HOME/.snaplet/snapshots</code>, but you can specify a path:</p> <pre><code>doppler run --token='dp.st.prd.xxxx' -- snaplet snapshot capture /path/to/stored-snapshot\n</code></pre>"},{"location":"historical/snaplet/#to-restore-a-snapshot","title":"To restore a snapshot","text":"<p>You restore the snapshot with another simple command</p> <pre><code>snaplet snapshot restore\n</code></pre> <p>For snapshot restoring, you should write some permissions to a target database.</p> <p>Make sure you don't write into a production database!</p> <p>Even  though, multiple measures were taken to prevent it:</p> <ul> <li>Secret <code>SNAPLET_DATABASE_URL</code> in a production Doppler configuration deliberately left empty. There's a zero chance we will use Snaplet to overwrite the Production database.</li> <li>If we restore the snapshot through the CI script <code>tools/snapshot_tool.py</code>, it prevents restoring the database if the URL looks like the Production</li> </ul> <pre><code># Prevent command with Service Token being recorded in bash history\nexport HISTIGNORE='doppler run*'\ndoppler run --token='dp.st.dev.xxxx' -- snaplet snapshot restore\n</code></pre>"},{"location":"historical/snaplet/#change-schema-after-restoring-a-snapshot","title":"Change schema after restoring a snapshot","text":"<p>After restoring the snapshot, it usually overwrites all tables except Database Schema. This prevents accessing the restored database through PostgREST or API with HTTP Error 401. In order to fix it, we must run the PSQL script <code>tools/schema_permission.sql</code> in a newly created database. This should fix the access rights and restore the schema.</p>"},{"location":"historical/snaplet/#other-commands","title":"Other commands","text":"<p>To list existing snapshots</p> <pre><code>snaplet snapshot list\n</code></pre> <p>To list existing configs</p> <pre><code>snaplet config list\n</code></pre> <p>To save config locally</p> <pre><code>snaplet config pull\n</code></pre> <p>More CLI commands are here.</p>"},{"location":"historical/tg_message_pgroonga_postmortem/","title":"Telegram Message pgroonga Index Postmortem","text":"<p>Table <code>telegram_messages</code> in <code>data</code> schema has been an issue due to its size (~11M records and ~14Gb of disk space). For a long time, creating <code>pgroonga</code> index on the <code>content</code> column was impossible due to Supabase limitations. However, the task proved non-trivial even with those limitations removed.</p> <ol> <li>Attempt to create the index on the existing data failed (see DVE-264 for details)</li> <li>Unsuccessful solutions :</li> </ol> <p>Create temp table, move all the data there, create index on empty or almost empty table, move data back.    Reason for failure: All steps except for the last one went just fine, however moving the data onto the indexed table turned out to be extremely slow, and because the process was initiated for the entire data set from a local client, it failed when the client stopped working (for unrelated reasons).    As a result, data moved to the temp table was lost. Luckily, we have it backed up. 3. Data was restored from backup into a regular table mimicing the original structure of <code>telegram_messages</code>. After that, I attempted to move it in batches. The process was very-very slow, so it promted me to introduce [queueing functionality] so that this process can take place directly on the server in automatic fashion. 4. In the middle of one of the jobs, an unrelated request to <code>search_in_people_skim</code> stored function with empty input cause the database to panic and restart. This led to corruption of the <code>pgroonga</code> index. 5. By that point, ~2M records were already copied over. I created a new index (took ~15 minutes) and dropped the old one (client hang on this operation, but the index was indeed removed). 6. Copying operation resumed after this, turned out to have much faster pace: less than 24 hours later ~80-85% of the data was already moved, while the first 15-20% took several days.</p>"},{"location":"infra/install_postgres_server/","title":"Installing Postgres Server","text":""},{"location":"infra/install_postgres_server/#vps-server-setup","title":"VPS Server Setup","text":""},{"location":"infra/install_postgres_server/#connect-ubuntu-repo-with-postgres","title":"Connect Ubuntu repo with Postgres","text":"<p>Add the PostgreSQL repository to your Ubuntu server's sources list. Update version as appropriate</p> <pre><code>echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" | sudo tee /etc/apt/sources.list.d/pgdg.list\nwget -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nsudo apt -y update &amp;&amp; sudo apt -y upgrade\nsudo apt -y install postgresql-16 postgresql-contrib-16 postgresql-client-16 postgresql-common\nsudo systemctl status postgresql\nsudo systemctl start postgresql\n</code></pre>"},{"location":"infra/install_postgres_server/#give-access-to-postgres","title":"Give access to Postgres","text":""},{"location":"infra/install_postgres_server/#open-firewall-port","title":"Open firewall port","text":"<p>Enable the firewall, unless already done</p> <p><code>sudo ufw enable</code></p> <p>Open the Postgres port for incoming connections</p> <p><code>sudo ufw allow 5432/tcp</code></p> <p><code>sudo ufw allow 5432/udp</code></p> <p>Check the status of the firewall</p> <p><code>sudo ufw status</code></p>"},{"location":"infra/install_postgres_server/#restore-current-snapshot-of-the-database","title":"Restore current snapshot of the database","text":"<ul> <li>Using Snaplet create a snapshot of the Production database   <code>snaplet snapshot capture</code></li> <li>Restore snapshot on the 2nd server   <code>snaplet snapshot restore</code></li> </ul>"},{"location":"infra/install_postgres_server/#new1","title":"New1","text":"<p>This document is the collection of steps required to install Postgres server on Ubuntu machine.</p> <p>https://www.postgresql.org/download/linux/ubuntu/</p> <ul> <li><code>sudo apt install postgresql</code></li> <li><code>sudo apt install -y postgresql-common   sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh</code></li> <li>sudo sh -c 'echo \"deb [signed-by=/usr/share/postgresql-common/pgdg/apt.postgresql.org.pgp] https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" &gt; /etc/apt/sources.lis</li> </ul> <p>t.d/pgdg.list' * sudo install -d /usr/share/postgresql-common/pgdg</p> <p>echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" | sudo tee /etc/apt/sources.list.d/pgdg.list</p> <p>wget -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -</p> <p>sudo apt -y install postgresql-16 postgresql-contrib-16 postgresql-client-16 postgresql-common * sudo systemctl start postgresql</p> <p>pg_ctl -D /usr/local/pgsql/data initdb</p> <p>psql -U postgres</p> <p>pg_ctl</p>"},{"location":"infra/polylith_structure/","title":"Polylith Project Structure","text":"<p>This document is to describe the process of switching <code>wapaganda_tools</code> project to the Polylith structure.</p> <p>Useful links:</p> <ul> <li>https://davidvujic.blogspot.com/2022/08/a-simple-scalable-python-project.html</li> <li>https://davidvujic.github.io/python-polylith-docs/</li> <li>https://github.com/DavidVujic/python-polylith</li> <li>https://github.com/DavidVujic/python-polylith-example</li> <li>https://github.com/ttamg/python-polylith-microservices-example</li> <li>https://davidvujic.github.io/python-polylith-docs/migrating/</li> <li>https://python-poetry.org/docs/plugins/</li> <li>https://github.com/subjective-agency/wapatools</li> </ul>"},{"location":"infra/polylith_structure/#steps-taken","title":"Steps taken","text":"<p>These Poetry plugins need to be installed:</p> <pre><code>poetry self add poetry-multiproject-plugin\npoetry self add poetry-polylith-plugin\n</code></pre> <ol> <li> <p>Create a new project with Poetry as dependency manager.</p> </li> <li> <p>Initialize git and Poetry</p> <pre><code>git init\npoetry init\n</code></pre> </li> <li> <p>Create a workspace, with a basic Polylith folder structure</p> <pre><code>poetry poly create workspace --name wapaganda --theme loose\n</code></pre> <p>This operation creates file <code>workspace.toml</code> with workspace settings. If section <code>test</code> is enabled, a folder with the same name would be created in the root mirroring the repo structure.</p> </li> <li> <p>Create a virtual environment</p> <pre><code>poetry install\n</code></pre> </li> <li> <p>Create new <code>project</code></p> <pre><code>poetry poly create project --name wapatools\n</code></pre> </li> <li> <p>Create <code>bases</code> and <code>components</code> for each module in the old repository:</p> <pre><code>poetry poly create base --name database\n...\npoetry poly create component --name transfactory\n...\n</code></pre> <p>The concept of <code>bases</code> / <code>components</code> / <code>projects</code> does not align perfectly with the structure by which the old repo was organized. For started, I decided to migrate contents of the <code>scripts</code> folder as <code>bases</code> and contents of the <code>src</code> folder as <code>components</code>, all assigned to the single <code>project</code>. However, it would probably make sense to split functionality down into different <code>projects</code>, which would require moving things around.</p> </li> <li> <p>The operation above boils down to creating the respective element's folder structure, as well as <code>_init_</code> file and <code>core.py</code> file (which is a conventional (but not mandatory) way of code organization). Once it's in place, files could be copied over from the old repo.     </p> </li> <li> <p>Organize <code>pyproject.toml</code> files.     One of the nuances of this approach, is that <code>pyproject</code> file is required not only on the root level, covering the whole package, but also on the <code>project</code> level. This means, that every project <code>folder</code> must contain its own <code>pyproject.toml</code> with appropriate dependencies.</p> <ol> <li>Repo-level file should contain following elements:</li> </ol> <pre><code>[tool.poetry]\npackages = [\n    {include = \"wapaganda/api\", from = \"bases\"},\n    ...\n    {include = \"wapaganda/umisc\", from = \"components\"},\n]\n\n[tool.poetry.dependencies]\n{copy over all the dependencies from the old repo}\n</code></pre> <ol> <li><code>project</code>-level file should contain following elements:</li> </ol> <pre><code>[tool.poetry]\npackages = [\n    {include = \"wapaganda/core\", from = \"../../bases\"},\n    ...\n    {include = \"wapaganda/upload\", from = \"../../components\"}\n]\n{note the path in the 'from' element}\n{ideally, only part of the repo dependencies should be listed for a project}\n\n[tool.poetry.plugins.\"poetry.application.plugin\"]\npoetry-polylith-plugin = \"polylith.poetry_plugin:PolylithPlugin\"\n\n[tool.poetry.dependencies]\n{copy over all the dependencies relevant for the project}\n</code></pre> </li> <li> <p><code>__init__.py</code>     This file is required for each <code>component</code> and each <code>base</code>. By default, the structure of <code>__init__.py</code> looks like this:</p> <pre><code>from wapaganda.base_or_component_name import core\n\n__all__ = [\"core\"]\n</code></pre> <p>This structure assumes the existence of file <code>core.py</code> and imports it as a whole. Alternatively, certain elements could be imported like so:</p> <pre><code>from wapaganda.database.connection import connect_supabase\n\n__all__ = [\"connect_supabase\"]\n</code></pre> <p>It is not strictly necessary to put this in order for all the components right away.</p> </li> <li> <p>This whole thing won't work until you run the following commands:</p> <ol> <li>Since the dependencies were not introduced in the natural way:</li> </ol> <pre><code>poetry lock\n</code></pre> <p>This would write or update <code>poetry.lock</code> file.</p> <ol> <li>The same should be done to the <code>project</code>-level file:</li> </ol> <pre><code>poetry lock --directory projects/wapatools\n</code></pre> <ol> <li><code>poetry install    <pre><code>This would install the dependencies, and it would install the internal module (`wapatools` in this case), which will finally unlock the usage. Something like this should be the final output line:\n</code></pre>    Installing the current project: wapatools (0.1.0)</code></li> </ol> </li> </ol>"},{"location":"infra/polylith_structure/#using-polylith-plugin","title":"Using <code>polylith</code> plugin","text":"<p>Once the above steps are complete, the plugin could be used like so:</p> <ul> <li><code>poetry poly info</code></li> </ul> <p>Returns workspace summary that looks like this:    * <code>poetry poly libs</code></p> <p>Returns summary of libraries in use that looks like this:    * <code>poetry poly diff</code></p> <p>Returns the difference between current state and latest stable point (not sure if this is git-based or what) * <code>poetry poly check</code></p> <p>Checks the validity of the structure (I think) - basically, a boolean derivation of the <code>poetry poly libs</code> command.</p>"},{"location":"infra/polylith_structure/#final-notes","title":"Final notes","text":"<ul> <li>I tested imports of internal modules on the <code>connection</code> func, and it worked just fine. Other modules should have <code>__init__</code> files properly structured.</li> <li>All in all the experience is surprisingly positive. Of course, I had some confusion, like with all things newly learned, but it was quite easy to figure things out after all. Some time was surely spent on this, but it was comparatively small.</li> <li>In the future the structure should be further refined: some scripts would probably have to be moved from <code>bases</code> to <code>components</code> and vice versa. It would definitely make sense to split this whole thing into multiple projects, not just one. I have a feeling that this structure suits this repo extremely well.</li> </ul>"},{"location":"infra/private_docker_registry_auth/","title":"Private Docker Registry Auth","text":"<ul> <li>Make sure <code>htpasswd</code> is installed:   <code>shell     sudo apt-get update &amp;&amp; sudo apt-get install apache2-utils</code></li> <li>Create <code>htpasswd</code> file and add the first user   <code>shell     sudo mkdir -p /mnt/docker/registry/auth     htpasswd -Bc /mnt/docker/registry/auth/htpasswd &lt;username&gt;</code></li> <li>When prompted, enter password</li> <li>Create config file at <code>/mnt/docker/registry/config.yml</code> with following content:   <code>yaml    version: 0.1    log:      fields:        service: registry        environment: development    http:      addr: :5000      headers:        X-Content-Type-Options: [nosniff]    auth:      htpasswd:        realm: basic-realm        path: /auth/htpasswd</code></li> <li> <p>When starting the registry's container, make sure to mount paths to config and to the auth directory. For example, when defining a system service:   ```ini     Description=Private Docker Registry for the Wapaganda Project     After=docker.service     Requires=docker.service</p> <p>[Service] Restart=always ExecStart=/usr/bin/docker run --rm --name wapa-registry -p 50000:5000 -v /mnt/docker/registry:/var/lib/registry -v /mnt/docker/registry/auth:/auth -v /mnt/docker/registry/config.yml:/etc/docker/registry/config.yml registry:latest ExecStop=/usr/bin/docker stop registry</p> <p>[Install] WantedBy=multi-user.target ```</p> </li> </ul>"},{"location":"infra/private_docker_registry_auth/#add-new-users","title":"Add new users","text":"<pre><code>htpasswd -B /mnt/docker/registry/auth/htpasswd &lt;new_username&gt;\n</code></pre>"},{"location":"infra/private_docker_registry_auth/#authenticate","title":"Authenticate","text":"<pre><code>docker login registry.subjective.agency\n</code></pre>"},{"location":"infra/private_docker_registry_auth/#list-repositories","title":"List Repositories","text":"<pre><code>curl -u &lt;username&gt;:&lt;password&gt; -X GET https://registry.subjective.agency/v2/_catalog\n</code></pre>"},{"location":"infra/private_docker_registry_auth/#list-tags","title":"List tags","text":"<pre><code>curl -u &lt;username&gt;:&lt;password&gt; -X GET https://registry.subjective.agency/v2/wapaganda/tags/list\n</code></pre>"},{"location":"infra/ssl_lets_encrypt/","title":"SSL with Let's Encrypt!","text":"<p>For info about database SSL see [TBD].</p> <p>It's possible to get SSL certificate for free using Let's Encrypt CA.</p> <p>Limitations: Paid certificates offer enhanced encryption levels over free options such as Let\u2019s Encrypt and provide additional features such as extended validation (EV), wildcard support, and site seals to prove authenticity.</p>"},{"location":"infra/ssl_lets_encrypt/#install-certbot-if-not-yet-installed","title":"Install <code>certbot</code> if not yet installed","text":"<pre><code>apt install letsencrypt\n</code></pre>"},{"location":"infra/ssl_lets_encrypt/#generate-certificate-for-serversubjectiveagency","title":"Generate certificate for <code>server.subjective.agency</code>","text":"<pre><code>certbot certonly --standalone -d server.subjective.agency\n</code></pre>"},{"location":"infra/ssl_lets_encrypt/#check-the-generated-certs","title":"Check the generated certs","text":"<pre><code>certbot certificates\n</code></pre> <pre><code>Saving debug log to /var/log/letsencrypt/letsencrypt.log\n\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nFound the following certs:\n  Certificate Name: server.subjective.agency\n    Serial Number: 494d1e4555cb861b59d48512b2b81394bbc\n    Key Type: RSA\n    Domains: server.subjective.agency\n    Expiry Date: 2024-04-12 19:35:08+00:00 (VALID: 87 days)\n    Certificate Path: /etc/letsencrypt/live/server.subjective.agency/fullchain.pem\n    Private Key Path: /etc/letsencrypt/live/server.subjective.agency/privkey.pem\n</code></pre> <p>You can copy / add your certificates anywhere they're needed. But we have to automate it, right?</p> <p>Let's encrypt infrastructure comes with renewal hooks allowing certs and keys be deployed anywhere.</p>"},{"location":"infra/ssl_lets_encrypt/#setup-renewal-hook","title":"Setup renewal hook","text":"<pre><code>vim /etc/letsencrypt/renewal-hooks/deploy/minio\n\n# Copy or symlink the renewed certificates to Minio accessible directory\ncp /etc/letsencrypt/live/server.subjective.agency/fullchain.pem ~minio-user/.minio/certs/public.crt\ncp /etc/letsencrypt/live/server.subjective.agency/privkey.pem ~minio-user/.minio/certs/private.key\nchown -R minio-user:minio-user ~minio-user/.minio/certs\nchmod 400 ~minio-user/.minio/certs/private.key\n</code></pre> <p>And now after renewal minio will get the new certificates automatically.</p>"},{"location":"infra/ssl_lets_encrypt/#check-the-certs-are-in-place","title":"Check the certs are in-place!","text":"<pre><code>cd /etc/letsencrypt/renewal-hooks/deploy/minio \nls -al  ~minio-user/.minio/certs\n</code></pre> <pre><code>total 36\ndrwxrwxr-x 3 minio-user minio-user 4096 Jan 15 22:15 .\ndrwxrwxr-x 4 minio-user minio-user 4096 Oct 15 21:40 ..\ndrwx------ 2 minio-user minio-user 4096 Oct 12 09:58 CAs\n-r-------- 1 minio-user minio-user 1704 Jan 15 22:15 private.key\n-rw-r--r-- 1 minio-user minio-user 5538 Jan 15 22:15 public.crt\n</code></pre>"},{"location":"infra/ssl_lets_encrypt/#automate-with-cron","title":"Automate with cron","text":"<p>We cannot keep in mind all the certs dates, so we force-update our cert every 2 months let's say on day 15:</p> <pre><code>crontab -e\n</code></pre> <pre><code># m h  dom mon dow   command\n0 0 15 */2 * certbot renew --force-renew\n</code></pre>"},{"location":"misc/begging_text/","title":"Begging text","text":"<p>Wapaganda is a non-commercial project, dedicated to collecting information on Russian and pro-Russian propagandists of war.\u00a0</p> <p>Essentially, it's a catalog of the personalities involved in the state-funded propaganda machine, the most efficient and deceptive since the times of Joseph Goebbels. The goal of the project is to collect all information about them, and make it freely available, with the following purposes in mind:</p> <ol> <li>Serving justice. These people must not escape just evaluation of their deeds. They are guilty of multiple crimes against humanity, from propaganda of hate, to calls for total genocide. Dehumanizing treatment of civilians on occupied territories, mass murder, torture, rape, looting are the direct consequences of the stream of hate, produced by Russian state-supported media.</li> <li>Enabling further research. Willing researchers must be given necessary information on the subject, including raw data and ways to analyze it, so that it would be possible not only to better understand it, but also to come up with methods of efficient resistance to propaganda and provide adequate answers to the burning questions.</li> <li>Revealing the true scale of lies\u00a0produced by Russia, and the role this effort played in the hostilities. We plan to offer open data about the scale of involvement of specific individuals, organizations, media outlets, etc.; forms and patterns of propaganda, as well as objective measures of involvement, such as air time, and many more.</li> </ol> <p>While propaganda is a murky thing to study, we believe it's a very important task, as its impact on society goes somewhat underappreciated, especially against the background of the news about the war. However, given the amount of harm it causes, its importance seems to be even higher, as the war and all the war crimes are direct consequences of propaganda.</p> <p>We are a non-commercial project, with no kind of organized funding (governmental or otherwise). With this fundraising, we are looking to cover the cost of maintaining the web application we're building, and to remove impediments from the process of further development. The maintenance cost is not very high (under $100 per month), but the cost of qualified labor has become a rather serious expense recently, as we believe that wages must not be skimmed. Also: many people in Ukraine lost their jobs as a direct consequence of the war, and while we can't help everyone, but we can employ at least a few.</p> <p>In the future, we may want to cover expenses for such things as SEO or design, although it's not currently on the roadmap.</p> <p>We aren't impartial. But we know how biases work, and we are working hard to remain objective no matter what.</p> <p>Victory will be Ukraine's; justice and truth will prevail. Thank you for reading this far.</p>"},{"location":"misc/slides_revealjs/","title":"Slides With Reveal.JS","text":"<p>I'm using reveal.js for creating presentations. Follow these steps to spin up a presentation at <code>https://slides.subjective.agency</code>:</p> <ol> <li> <p>Create the deck. It could be created either via code, or using the UI at https://slides.com, which is a commercial solution on top of the technology, with the free plan allowing to create up to 5 decks. A created deck can be exported in HTML format.</p> </li> <li> <p>Add the deck to the forked repository at https://github.com/subjective-agency/reveal.js - either into <code>decks</code> directory, or as an <code>index.html</code> file.</p> </li> <li> <p>If replacing <code>index.html</code> file, DO NOT REMOVE the one that already exists, instead rename it into something like <code>_index_.html</code>.</p> </li> <li> <p>A cron is set up on the <code>principal</code> server that checks for updates every minute and rebuilds the docker image if any are detected</p> </li> <li> <p>If it doesn't, manual steps are:</p> </li> <li> <p>Log into the <code>principal</code>'s shell, and CD to <code>/home/warp/reveal</code>. This directory contains <code>tart_reveal.sh</code> script.</p> </li> <li> <p>Run</p> <pre><code>sudo bash start_reveal.sh\n</code></pre> <p>This would rebuild the docker image, and then spin up the container at port 50988, which is reverse-proxied to <code>slides</code> subdomain.</p> </li> </ol> <p>Additional deck placed into <code>decks</code> directory with name <code>apresent.html</code> can be accessed at <code>https://slides.subjective.agency/decks/apresent.html</code></p>"},{"location":"rfc/1/","title":"RFC-1. P-Rating","text":""},{"location":"rfc/1/#abstract","title":"Abstract","text":"<p>TBD</p>"},{"location":"rfc/1/#context","title":"Context","text":"<p>With over 1,000 propagandists on file (and counting), it is natural to consider a scale along which they all could be placed in accordance with the measure of their respective influence. However, in order to have such scale, there must be a reliable way to measure said influence. This is a challenging task, to put it mildly.</p>"},{"location":"rfc/1/#sources-of-influence","title":"Sources of influence","text":"<p>For now, I distinguish 3 major sources of influence: <code>airtime</code> (appearances on TV and in streams / videos),  <code>personal influence</code> (which refers, for the most part, to the social media following) and <code>published works</code> (books &amp; articles). They are different in essence and require different approaches, but at the end of the day, the former two result in accumulation or loss of <code>p-points</code> per day. The 3rd also produces p-points, but over longer stretches of time.</p> <p>With <code>airtime</code>, the amount of daily p-points depends on the following factors:</p> <ol> <li>Place. Is it a TV channel? If so, is it federal or local? Is it a Youtube channel? What was the audience of that channel at the time of streaming? What was the audience of that particular stream?</li> <li>Role. Is it a host? Is it a guest? Are we talking self-hosted video? etc.</li> <li>Time. For how long was this person present in the shot? [what about when it's only the voice? do we measure exact time (per shot; possible with machine learning; takes a hell of a long time), or is it better to use approximations based on internal patterns of each media segment?]</li> <li>Perhaps, influence already accumulated as of the air time, can be considered a factor, as well.</li> </ol> <p>With <code>personal influence</code> p-points appear from the published posts, with following things playing a part:</p> <ol> <li>[per media] number of posts, number of views/reads for each post;</li> <li>[per media] average daily views / average post views;</li> <li>number of social media maintained;</li> <li>[cumulative] TBD</li> </ol> <p>With <code>published works</code></p> <p>TBD</p>"},{"location":"rfc/1/#gaps-issues","title":"Gaps &amp; Issues","text":"<p>The main issue with this whole thing is that quite a lot of information is unavailable and/or unobtainable, which makes any possible calculations contingent. That is unavoidable, but it doesn't mean that they can't be consistent, too: in order to achieve consistency, a baseline personality should be selected to serve as an exemplary case, against which everybody else would be measured.</p> <p>Further issues: should this be a single person who feeds from all known sources of influence (not too rare; but rarely a person would be equally successful all around), or should there be separate measures for each source (which would effectively frankenstein a perfect influencer)?</p> <p>Another issue is that <code>airtime</code> and <code>personal influence</code> are quite complex, although differently. With <code>airtime</code>, time measure (seconds) and quantitative measure (viewers) must be equated and normalized, and various coefficients must be accounted for. With <code>personal influence</code> there are multiple social media, each with its own architecture, meaning each would require a dedicated connector.</p>"},{"location":"rfc/2/","title":"RFC-2. Transcripts","text":"<p>In the context of the project, transcript is a textual representation of a media episode. Media episodes are different in terms of length and structure, which means that storing their transcripts (essentially, large amounts of text) becomes a bit of a challenge.</p> <p>There are three general usecases here:</p> <ol> <li>Segment's internal structure is simple and does not change; and/or it starts and finishes with the same cast.</li> <li>Segment's internal structure is complex and/or involves multiple cast that can unpredictably change over the course of an episode. It is impossible to fully separate specific speakers.</li> <li>Segment's internal structure is relatively complex but predictable and separable.</li> </ol> <p>In cases 1 and 2, it would make sense to store the full episode transcript in the <code>_vids</code> table.</p> <p>Case 3 is different. These episodes consist (for the most part) of sequential chunks and usually follow one of several specific patterns (they are not 100% reliable, though). Generally, there are 2 main types of chunks that can be pinned down as <code>monologs</code> and <code>dialogs</code> for lack of better terminology. The most important thing, however, is that they can be separated - this offers a benefit of more targeted text selection down the line (i.e. reduction in processing overhead). This requires, on the one hand, a more intricate and time-consuming mark-up process, and on the other, a more complex storage structure.</p> <p>Specifically, instead of having <code>transcript</code> field in the <code>_vids</code> table, it would be a part of the <code>people_on_</code> table, where each guest record would have a corresponding <code>dialog</code>, and the host record would contain all the <code>monologs</code>. ?? what if &gt; 1 host  ?? ?? Should the order be preserved ?? ?? if so - how ??</p> Type 1 Type 2 Type 3 besogon_vids 60_minutes_vids solovyovlive_vids daytv_vids kto_protiv_vids komsomolskayapravda_vids komsomolskayapravda_vids solovyovvecher_vids metametrica_vids vesti_vids mkp_vids typychny_vyshinsky_vids youtube_vids"},{"location":"rfc/3/","title":"RFC-3. About Page","text":"<p>Welcome! Thank you for your curiosity.</p>"},{"location":"rfc/3/#origins","title":"Origins","text":"<p>Wapaganda the Project was conceived in the wake of Russo-Ukrainian war around the end of February 2022. It took some time for it to acquire shape (until the end of summer 2022, give or take), and it wasn't until January 2023 that it was deemed ready enough for public scrutiny.</p> <p>There is a good reason for such a lengthy timeframe: our team is tiny (2 people currently, just 1 in the beginning), and our resources were and still are quite limited.\u00a0</p> <p>//\ufeffBy the way, if you want to support us (and we would appreciate it very much!), there are several ways how you can do that. Check out this page for more info.//</p>"},{"location":"rfc/3/#essence","title":"Essence","text":"<p>So, what is Wapaganda, exactly?</p> <p>On the surface you will find here a catalog of personalities, each of whom is guilty of actively supporting Vladimir Putin's criminal regime by\u00a0 propagating false narratives and tropes, and with goals of\u00a0easing the negative effects of the war on the Russian population and pumping up support for the regime. Why we believe that propaganda is such an important machinery for this process is explained over here.</p> <p>Underneath the flimsy list of obscure individuals,\u00a0there is an immense amount of data and hundreds of tiny relationships connecting all the separate pieces into a vast and fascinating system - complex, but not unfathomable.</p> <p>We are working to accumulate all possible data on these people, including their written works (actual books and articles, as well as blog posts, and social media footprint), their appearance in the media (including transcripts of what was said - currently, only a share of data is processed, and is available only in Russian - English translation is planned), anything else that's available out there in the open web, and even a little bit of what the dark web has to offer. Simultaneously, we're mapping all the little pieces together, sort of like a multidimensional jigsaw puzzle.</p> <p>This accumulation of data is being done with several things in mind:</p> <ol> <li>First and foremost, the data would serve as evidence for when these people are tried for their activities.</li> <li>The data would serve as a basis for working out the\u00a0influence rating (P-rating), which could be used to assign accountability with better precision.</li> <li>The data should become a valuable source for any researcher out there. We're following the CVARC model, ensuring data's Completeness, Verifiability, Accessibility, Reliability and Consistency. (TODO)</li> <li>Finally, the data may serve as a starting spark in the process of establishing international monitoring with the purpose of preventing yet another intricate propagandistic machine from developing.</li> </ol>"},{"location":"rfc/3/#criteria","title":"Criteria","text":"<p>One important question to answer is how exactly people end up in the Wapaganda database. You might wanna check out the Copium Theory of Propaganda to understand the approach, but in plain language, each and every person on this list has willingly, and oftentimes eagerly, participated in the activities aimed at spreading the narratives of the Putin's creed (TODO). There are other projects out there maintaining lists of\u00a0all\u00a0kinds of\u00a0Ukraine's enemies (Myrotvorets, OSINTBees,\u00a0..) - we, on the other hand, are specifically focused on the propagandists. You will\u00a0find here soldiers and collaborants, too, but only if, on top of their military or administrative function, they also propagated the Putin's\u00a0lies via mass media.</p> <p>Each and every person is handpicked, and there is always a reason for their inclusion. That being said, mistakes are possible, and if you encounter an incorrect piece of information, please do let us know (TODO).</p> <p>\ufeff\ufeffThe bulk of our database is day-to-day propaganda functionaries, such as the cast of the\u00a0Solovyov Live, 60 minutes, Vesti etc.\u00a0talk shows, as well as the numerous guests featured there.\u00a0</p> <p>Only appearances\u00a0after February 24, 2022 are taken into account, the assumption being that the start of the war\u00a0was a sobering experience for some, so a person is considered complicit only if they have continued (or started)\u00a0\ufeffshowing\u00a0up.\u00a0</p> <p>Other notable subgroups include high-level figures whom many people know, such as Margarita Simonyan\u00a0and Dmitry Kiselyov; authors of books and articles directed against Ukraine and/or justifying Russian decrepit ways;\u00a0semi-independent influencers; foreign friends (agents of influence\u00a0outside of Russia); and others.</p> <p>Finally, a swarm of pro-Russian influencers, inhabiting YouTube, TikTok, Telegram and other media platforms, constitutes a significant share of monitoring targets.</p>"},{"location":"rfc/3/#data","title":"Data","text":"<p>What kind of data is being absorbed, what can you find here?</p> <p>First off, there's data that's already available out there, in other sources. In Wapaganda it's organized a little better, and generally more accessible.</p> <p>Then, there is data that comes from the ongoing process of observation. Mostly, these are media appearances, quotes, and\u00a0full transcripts of what was said.</p> <p>We monitor and scrape media platforms, including YouTube, Smotrim.ru, Vk.com.</p> <p>There is\u00a0Telegram channels\u00a0statistics, as well as\u00a0a backup of their history (730+ channels, close to 10 million messages in total).</p> <p>Not all of these riches are currently fully accessible. If you want to speed up the process a bit, you can do it here (TODO).</p>"},{"location":"rfc/4/","title":"RFC-4. Media Segments, YouTube Channels &amp; Tables","text":"<p>This article describes relationships among media segments, YouTube channels and subset of database tables suffixed with <code>_vids</code> or <code>_episodes</code>.</p> <p>A media segment is a sequence of related media episodes. Media episode is a distinct piece of media content willingly published on the Internet by its owners with the purpose of public distribution. </p> <p>Note: majority of media episodes are also distributed via other means, i.e. television, radio, printed media. Note: there may be media that are distributed via those other means but aren't available in the web. </p> <p>Media episode can be of different media type (podcast, talk show, news piece, etc.) ~ this is not currently taken into account.</p> <p>Media segments can be explicit (named talk shows, in particular), authorial (a particular expert on a particular outlet, for example, \"Aleksandr Artamonov on DayTV\"), or synthetic (for example, \"Nikolay Platoshkin on YouTube\").</p> <p>Note the difference between authorial and synthetic. Both are named after the main author of the segment, but where authorial is tied to a specific media outlet (DayTV in this example) and could move to a different platform with it, the synthetic type refers to free-to-use platforms like YouTube, where an author may have multiple channels and/or can be banned by the platform but has an option of registering a new account. For example, there are 3 YouTube channels that publish Artamonov-related stuff, and there are 2 known channels for Radio Aurora, one of which is banned.</p> <p>Media segments also differ by the primary source, which could be youtube.com, smotrim.ru or vk.com. This is mixed up a little, since segments could be present on several platforms simultaneously. The solution to this ambiguity is to simply select one or another. vk.com is not used anymore because other 2 platforms give us better structured data, but it has a benefit of storing hi-resolution videos of those segments that choose to publish there.</p> <p>Some media segments have their own tables in the db, while others are clustered together and use a common table. The current set up is somewhat inconsistent, for example, it may make sense to merge <code>metametrica_vids</code> and <code>komsomolskayapravda_vids</code> into <code>youtube_vids</code>, or merge all the <code>smotrim</code>-based tables together, but doing that won't improve things by that much yet it would take quite a bit of effort. Still, an optimization plan might be in order for implementation in the future.</p>"},{"location":"rfc/5/","title":"RFC-5. Pre-Launch Database Optimization","text":""},{"location":"rfc/5/#context","title":"Context","text":"<p>The database schema has evolved quite significantly since it was conceived, and while its evolution was guided by performance considerations, certain initial decisions have led to suboptimal landscape and need to be re-evaluated now.</p>"},{"location":"rfc/5/#decision","title":"Decision","text":"<p>A number of tables in the database contain various media episodes, and the current setup is ambiguous in the sense that there is no clear reasoning for why certain media segments would have their own dedicated tables, while others would be sharing.</p> <p>To remove this ambiguity, a squashing operation should be implemented. Specifically, tables <code>60_minutes_vids</code>, <code>kto_protiv_vids</code>, <code>mkp_vids</code>, <code>solovyovlive_vids</code> (partially), <code>solovyovvecher_vids</code>, <code>typychny_vyshinsky_vids</code>, <code>vesti_vids</code>, <code>vestifm_episodes</code>, <code>radio_episodes</code> are to be merged into a single <code>smotrim_episodes</code> table; <code>besogon_vids</code>, <code>daytv_vids</code>, <code>komsomolskayapravda_vids</code>, <code>metametrica_vids</code> are to be merged into <code>youtube_vids</code>.</p> <p>This would require modifications to the schema: both [new] tables are to be based off of the current templates, except to indicate the distinction that is upheld by different tables right now.</p> from to <code>60_minutes_vids</code> <code>smotrim_episodes</code> <code>kto_protiv_vids</code> <code>smotrim_episodes</code> <code>mkp_vids</code> <code>smotrim_episodes</code> <code>solovyovlive_vids</code> <code>smotrim_episodes</code> <code>people_on_solovyovlive</code> <code>people_on_smotrim</code> <code>solovyovvecher_vids</code> <code>smotrim_episodes</code> <code>~~people_on_solovyovvecher~~</code> <code>people_on_smotrim</code> <code>typychny_vyshinsky_vids</code> <code>smotrim_episodes</code> <code>vesti_vids</code> <code>smotrim_episodes</code> <code>vestifm_episodes</code> <code>smotrim_episodes</code> <code>radio_episodes</code> <code>smotrim_episodes</code> <code>besogon_vids</code> <code>youtube_vids</code> <code>daytv_vids</code> <code>youtube_vids</code> <code>people_on_daytv</code> <code>people_on_youtube</code> <code>komsomolskayapravda_vids</code> <code>youtube_vids</code> <code>people_on_morning_mardan</code> <code>people_on_youtube</code> <code>metametrica_vids</code> <code>youtube_vids</code> <code>people_on_metametrica</code> <code>people_on_youtube</code> <p>In addition:</p> <ul> <li><code>~~youtube_authors~~</code> should be renamed <code>~~people_on_youtube~~</code>; column to indicate role_id should be added to it.</li> <li>decide what to do with <code>youtube_authors</code> table</li> <li>MongoDB objects in the <code>transcripts</code> collection should be updated with new table name / id</li> </ul>"},{"location":"rfc/5/#plan-of-action","title":"Plan of action","text":"<p>The 1st part of the operation, the copying, should be completed over the course of 1 day, and the dropping should be done on the next day - this is so that we could have a backup of the complete DB, just in case. As an additional precaution measure, 2 index files, one per platform, should be saved with mapping <code>old_table</code>+<code>old_id</code> \u2192 <code>new_id</code>.</p> <ul> <li>[x] Create table <code>smotrim_episodes</code></li> <li>[x] Copy contents of the smotrim tables to <code>smotrim_episodes</code> (except solovyolive_vids)</li> <li>[x] Create table <code>people_on_smotrim</code></li> <li>[x] Copy contents of the <code>solovyovlive_vids</code></li> <li>[x] Copy contents of the <code>people_on_solovyovlive</code> also updating indices</li> <li>[x] Create <code>people_on_youtube</code> table</li> <li>[x] Copy contents of the youtube tables to <code>youtube_vids</code> also updating indices</li> <li>[x] Update all scripts dealing with the deprecated tables to deal with the new set-up.</li> <li>[x] Drop smotrim tables</li> <li>[x] Drop youtube tables</li> </ul>"},{"location":"rfc/5/#status","title":"Status","text":"<p>In progress.</p>"},{"location":"rfc/5/#consequences","title":"Consequences","text":"<p>This operation has a high potential for breaking things, so all the suggested changes must be listed first, with as much foresight into the specifics as possible, and should be applied after an internal discussion.</p>"},{"location":"rfc/7/","title":"RFC-7. Media Episodes Attribution Problem","text":""},{"location":"rfc/7/#context","title":"Context","text":"<p>Currently, DVE-A-56, DVE-A-29 and DVE-A-30 (all unfinished) describe the approach to <code>media segment</code> - <code>media episode</code> relationship. Due to the complexity of the real-life picture, though, this description ignores certain important nuances. This document offers a detalization on the <code>media episode</code> entity, and describes the problem of attributing a <code>media episode</code> to a <code>media segment</code>.</p>"},{"location":"rfc/7/#media-episode-structure","title":"Media Episode Structure","text":"<p>Every <code>media episode</code> consits of a number of  <code>scenes</code> (at least 1).</p> <p>A <code>scene</code> has 2 boolean attributes:</p> <ol> <li><code>irl</code> indicates if the <code>scene</code> happened offline (<code>True</code>) or via some virtual means of communication;</li> <li><code>quicky</code> indicates if the <code>scene</code> is supplementary (and short; example: Golovanov's Time within Polny contact and Mardan segments).</li> </ol> <p>Furthermore, a <code>scene</code> can be 1 of the following types/subtypes (<code>host2guest</code>):</p> Type Subtype Example <code>many2many</code> <code>circle wank</code> \u0412\u0435\u0447\u0435\u0440 \u0441 \u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440\u043e\u043c \u0421\u043e\u043b\u043e\u0432\u044c\u0435\u0432\u044b\u043c, \u041a\u0442\u043e \u041f\u0440\u043e\u0442\u0438\u0432 <code>many2many</code> <code>group</code> <code>one2zero</code> <code>wank-off</code> Any lecture, any scene with just the host <code>one2one</code> <code>boning</code> <code>one2many</code> <code>two2zero</code> <code>mutual wank</code> \u041c\u0438\u0445\u0435\u0435\u0432 \u0413\u043e\u0432\u043e\u0440\u0438\u0442 <code>two2one</code> <code>threeway</code>"},{"location":"rfc/7/#attribution","title":"Attribution","text":"<p>The attribution problem arises from the recently introduced <code>synthetic</code> segment type, which is tied to a special organization type <code>personal brand</code> (id 35). <code>Synthetic</code> segment is meant to serve as a container that holds media episodes produced by sources with low consistency &amp; recognition rate (mostly, YouTube channels with random selection of content) but that can be tied to an influencer patient, such as Andrey Fursov or Aleksandr Artamonov. </p> <p>In effect, this means, for example, that the numerous YouTube videos in which Andrey Fursov took part, (almost) regardless of which YouTube channel they were published on, should be identified as episodes of the <code>Andrey Fursov (WYT)</code> segment, which belongs to <code>Andrey Fursov</code> personal brand organization. </p> <p>The <code>Andrey Fursov</code> personal brand organization is also tied to <code>Andrey Fursov (DayTV)</code> segment, which is a collection of Fursov's appearances on specifically DayTV. DayTV is a media outlet whose content can be obtained from several sources, including YouTube, which makes this whole thing a bit more confusing.</p> <p>The other major type of <code>media segment</code> is <code>named</code> segment - this is BesogonTV, Metametrica, Empatiya Manuchi, Polny Kontact, etc. Confusion intensifies when you consider Andrey Fursov's appearances on such <code>named</code> segments. For example, he gave several interviews to Metametrica - do these episodes belong to <code>Metametrica</code> segment, or <code>Andrey Fursov</code> segment?</p> <p>Furthermore, Metametrica is organized and hosted by 2 people (Danyuk and Egorchenkov), both of whom are influencers in their own right - so, maybe these episodes should be attributed to their respective <code>personal brand</code>-related segments?</p> <p>One solution could be to allow an episode to have multiple segment-relationships, i.e. Fursov's Metametrica interviews would be tied to Metametrica segment, as well as <code>personal brand</code>-related segments of all the participants. This could get out of hand, if there many.</p>"},{"location":"working/media/social_blade/","title":"Social Blade Addition","text":"<p>SocialBlade is a platform collecting statistics on various media platforms, including Youtube. SocialBlade data would compliment the youtube stats data we have currently. SocialBlade data is NOT free, so related flow should be designed to draw maximum value from what we get.</p> <p>Attached is a standard result you'd get if you spend 1 credit on a channel. Historical data is available for additional creds.</p>"},{"location":"working/media/social_blade/#relevant-sections","title":"Relevant sections","text":"Section Datatype Notes Where info -&gt; access -&gt; expires_at Datetime Service item that should be used to manage how a channel data would be updated further data -&gt; id JSON[TEXT] Data item that should be parsed and saved once youtube_channels data -&gt; general -&gt; created_at Datetime Data item that should be parsed and saved once. Only needed for unresolved channels youtube_channels data -&gt; general -&gt; geo JSON[TEXT] Data item that should be parsed and saved once youtube_channels data -&gt; general -&gt; branding -&gt; social JSON[TEXT] Should be re-checked on every request. youtube_channels statistics -&gt; total JSON[TEXT/INT] Contains 3 keys - uploads, subscribers and views with values current as of the moment of request.Values for subs and views are the same as the most recent <code>daily</code> values.Value for current number of uploaded vids can be procured from Youtube API for free. nowhere statistics -&gt; growth JSON[TEXT/INT] It's not exactly clear what these numbers signify. Growth over the first year of existence? What's the deal with the negative values?TBD ranks JSON[TEXT/INT] Data is relevant as of the moment of request. Not included in daily daily JSON[TEXT/INT] With 1 credit we get number of subs and views over previous 30 days youtube_stats <ul> <li>Reconciliation required for stats data currently stored in <code>youtube_vids</code>. It gets fetched via Youtube API, and contains numbers for views, comments and likes on specific videos (as of the moment of request).</li> <li>Stats data stored in <code>youtube_channels</code> should be the most current, and should be updated from <code>youtube_stats</code> table data.</li> </ul>"},{"location":"working/media/social_blade/#approximate-flow","title":"Approximate Flow","text":"<ol> <li>Fetch a channel from the database.</li> <li>Get max amount of historical data on this channel.</li> <li>Parse and save the data to the database. We would get:</li> <li><code>expires_at</code> date;</li> <li><code>general</code> information that we only need once;</li> <li>related <code>social</code> network accounts;</li> <li><code>growth</code> data;</li> <li><code>ranks</code> data relevant as of the moment of request;</li> <li>complete set of <code>daily</code> data up to the date of request.</li> <li>If a channel is defunct, this is the end of it.</li> <li>If a channel is not defunct:</li> <li>Set new value for <code>expires_at</code> date (TBD)</li> <li>Fetch updates on this channel until <code>expires_at</code> date comes. With every response we would get:<ul> <li>related <code>social</code> network accounts (possible update);</li> <li><code>growth</code> data; (possible update?) (to be saved?)</li> <li><code>ranks</code> data relevant as of the moment of request; (to be saved)</li> <li><code>daily</code> data for previous 30 days (items not in db to be saved)</li> </ul> </li> <li>Put channel on pause for 30 days. At this point we have:<ul> <li>most current list of related <code>social</code> accounts;</li> <li><code>ranks</code> data for the recent 30 days;</li> <li><code>growth</code> data for the recent 30 days;</li> <li>entire set of <code>daily</code> data up until current date.</li> </ul> </li> <li>After 30 days, spend 1 credit on this channel to fetch the standart amount of data. We would get:<ul> <li>related <code>social</code> network accounts (possible update);</li> <li><code>growth</code> data; (possible update?) (to be saved?)</li> <li><code>ranks</code> data relevant as of the moment of request; (to be saved)</li> <li><code>daily</code> data for previous 30 days (to be saved).</li> </ul> </li> <li>Put channel on pause for 30 days. At this point we have:<ul> <li>most current list of related <code>social</code> accounts;</li> <li><code>ranks</code> data for 60 days in total: 30 days of the 1st iteration, and 30 more of the 2nd iteration;</li> <li><code>growth</code> data for 60 days; (...)</li> <li>entire set of <code>daily</code> data up until current date.</li> </ul> </li> <li>Rinse and repeat.</li> </ol>"},{"location":"working/media/social_blade/#observations","title":"Observations","text":"<ul> <li>We would have consistent and steady set of <code>daily</code> data with no gaps.</li> <li>We would have inconsistent set of <code>ranks</code> data - 30 days stretches with 30 days gaps in between.</li> <li>Value of <code>growth</code> data is dubious at the moment.</li> </ul>"},{"location":"working/media/social_blade/#to-do","title":"To do","text":"<ul> <li>Find a place for <code>expires_at</code> timestamp;</li> <li>Create a table for storing <code>daily</code> data. Possibly, store <code>ranks</code> (and <code>growth</code>?) data in the same table;</li> <li>Decide what to do with videos-level stats;</li> <li>Set up a trigger to update <code>youtube_channels</code> table with most current views, subs etc.</li> </ul>"},{"location":"working/media/social_blade/#upd1","title":"UPD1","text":"<p>After reviewing the video https://www.youtube.com/watch?v=LihUTkSMRFw found by @saszko, it became clear to me that the actual flow should be as follows:</p> <pre><code>Get a channel from database;\nConsider channel's creation date.\n    If creation_date is less than 700 days ago, do nothing.\n    If creation_date is more than 700 days ago, spend 3 credits to retrieve max amount of historical data from SocialBlade.\nPut the channel up for update via the compare flow (TBD)\n\nif channel is defunct - one time;\nif channel is not defunct - continuously on schedule;\n</code></pre> <p>The compare flow would be based on the info from the aforementioned video, and would involve using playwright for access to https://socialblade.com/youtube/compare page (it returns 503 to requests ). Example compare URL is https://socialblade.com/youtube/compare/UCBMXxadAHZl9FOGhMeR_ptg/UCKgdkhHgBQrwWY_PVGxMH9Q, where pieces after compare/ are identifiers of Youtube channels. With this tool, you can retrieve up to 3 channels at a time.</p> <p>Retrieval algorythm is approximately as follows:</p> <pre><code>Get 3 target channels from the database accounting for 1) if the channel is defunct; 2) when was it last updated.\nShape compare URL.\nUse playwright to visit the URL and retrieve page's source code\nParse the source code to retrieve 5 relevant raw strings corresponding to 5 sections returned by the compare module.\nParse raw strings into proper data structures\nPersist only new items to the database.\n</code></pre>"},{"location":"working/media/social_blade/#upd2","title":"UPD2","text":"<p>With the compare flow, only 2 sections (raw strings) are relevant:</p> <pre><code>document.getElementById('subscribersYTDYGraph')\ndocument.getElementById('videoviewsYTDYGraph')\n</code></pre> <p>Of the other 4, dailysubscribersYTDYGraph and dailyvideoviewsYTDYGraph represent a diff between days, which we can calculate ourselves; and futuresubsYTDYGraph and futureviewsYTDYGraph are projections based on unknown algorythm, which makes them rather useless.</p>"},{"location":"working/onto/classes/","title":"Ontologies, Classes & Semantic Web","text":"<p>This document is supposed to serve as a basic informational resource on things related to the ontology-building effort.</p>"},{"location":"working/onto/classes/#what-is-an-ontology-and-why-do-we-need-one","title":"What is an ontology and why do we need one?","text":"<p>In the context of computer science and information science, an ontology is a formal representation of knowledge that defines the concepts, entities, relationships, and properties within a particular domain. It provides a structured and organized way to describe and model a specific domain by defining the classes, attributes, and interrelationships of the entities or concepts within that domain.</p> <p>In the case of Wapaganda, the domain in question is the propagandistic activity of the current Russian state, and top-level classes are:</p> <ul> <li>Organization</li> <li>Person</li> <li>Group</li> <li>MediaSegment</li> </ul> <p>Attributes and relationships for <code>organization</code>, <code>person</code> and <code>group</code> should be adopted from FOAF, where all of these entities have the same parent - <code>agent</code>. Basic terms and attributes should be adopted from DC and, possibly, other existing vocabularies </p> <p>On the other hand, <code>MediaSegments</code> do not seem to have any kind of vocabulary developed, so this would be the primary direction of the ontology effort.</p> <p>The resulting ontology must serve as the basis for classes and attributes created in the Python code. With library <code>owlready2</code> it is possible to link an ontology and a python module so that they compliment each other.</p>"},{"location":"working/onto/classes/#resourses","title":"Resourses","text":"<ul> <li>FOAF (Friend-of-a-Friend) http://xmlns.com/foaf/0.1/</li> <li>DC (Dublin Core) https://www.dublincore.org/specifications/dublin-core/dcmi-terms/</li> <li>SIOC (Semantically Interlinked Online Communities) http://rdfs.org/sioc/spec/</li> <li>https://schema.org/docs/schemas.html</li> <li>https://lov.linkeddata.es/dataset/lov/vocabs</li> <li>https://www.w3.org/TR/powder-primer/</li> <li>https://www.ietf.org/rfc/rfc3987.txt</li> <li>https://www.w3.org/TR/rdf11-primer/</li> <li>https://www.w3.org/TR/2004/REC-owl-guide-20040210/</li> <li>https://www.w3.org/TR/2004/REC-owl-semantics-20040210/</li> <li>https://www.w3.org/2002/03/tutorials.html</li> <li>https://protegeproject.github.io/protege/</li> <li>https://www.w3.org/2001/sw/SW-FAQ</li> <li></li> </ul>"},{"location":"working/onto/model_requirements/","title":"Ontologies. Model Requirements","text":"<p>The entities and relationships for the Wapaganda Semantic Model (WSM) must be extracted from the data produced by the persons and organizations engaged in the informational support of the Russo-Ukrainian war on the Russian side (aka Russian propaganda).</p> <p>To make sure these entities are generic enough to be applicable to other similar situations, they must be fitted against at least 1 well-documented historical case of comparable nature, for example the history of propaganda in the Nazi Germany.</p> <p>Individuals of the model are people, organizations and various media resources engaged in the propagandistic activity.</p> <p>Correspondingly, the final arrangement must consist of the following parts:</p> <ul> <li>light-weight OWL model describing general entities and relationships, such as people and organizations, which should be applicable to any similar situation regardless of geography;</li> <li>light-weight SKOS model describing the system of axioms and ideas that constitute the current state-driven public consensus in Russia;</li> <li>application that takes both these models and adds a data layer to describe the state and history of the Russian informational space in the context of the military conflict in Ukraine.</li> </ul> <p>The model should be able to provide a way for answering questions that involve</p> <ul> <li>Level of engagement of any particular entity in the propagandistic activities;</li> <li>Amount of value any particular entity brings into the propaganda system;</li> <li>Relative and absolute impact any particular entity has on the public opinions;</li> <li>The specific subset of axioms/ideas any particular entity supports / spreads;</li> <li>Relative amount of hate speech in any particular entity's public statements; etc.</li> </ul>"},{"location":"working/onto/ontology_flow_overview/","title":"Ontology Flow Overview","text":"<p>On the high level, the flow is approximately as this:</p> <ul> <li>Over the day, some new data gets added to the DB.</li> <li>Once a day, on schedule, the <code>reasoner</code> is run on the updated DB. This step produces additional data that should be saved to the DB. </li> <li>Data such generated is displayed in the interface in some distinct way.</li> <li>Final HTML code contains additional elements/tags/attributes reflecting ontology-related stuff. Here's a non-specific example provided by ChatGPT:</li> </ul> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;title&gt;Ontology HTML Page&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;About Me&lt;/h1&gt;\n    &lt;div xmlns:foaf=\"http://xmlns.com/foaf/0.1/\"&gt;\n        &lt;h2 property=\"foaf:name\"&gt;John Doe&lt;/h2&gt;\n        &lt;p&gt;\n            &lt;strong&gt;Occupation:&lt;/strong&gt;\n            &lt;span property=\"foaf:title\"&gt;Software Engineer&lt;/span&gt;\n        &lt;/p&gt;\n        &lt;p&gt;\n            &lt;strong&gt;Birthdate:&lt;/strong&gt;\n            &lt;time property=\"foaf:birthday\" datetime=\"1985-10-15\"&gt;October 15, 1985&lt;/time&gt;\n        &lt;/p&gt;\n        &lt;p&gt;\n            &lt;strong&gt;Address:&lt;/strong&gt;\n            &lt;span typeof=\"foaf:Address\"&gt;\n                &lt;span property=\"foaf:streetAddress\"&gt;123 Main Street&lt;/span&gt;,\n                &lt;span property=\"foaf:locality\"&gt;Cityville&lt;/span&gt;,\n                &lt;span property=\"foaf:region\"&gt;State&lt;/span&gt;,\n                &lt;span property=\"foaf:postalCode\"&gt;12345&lt;/span&gt;,\n                &lt;span property=\"foaf:country\"&gt;United States&lt;/span&gt;\n            &lt;/span&gt;\n        &lt;/p&gt;\n        &lt;p&gt;\n            &lt;strong&gt;Email:&lt;/strong&gt;\n            &lt;a href=\"mailto:john.doe@example.com\" property=\"foaf:mbox\"&gt;john.doe@example.com&lt;/a&gt;\n        &lt;/p&gt;\n    &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"working/onto/ontology_flow_overview/#reasoners","title":"Reasoners","text":"<p>Some possible options for reasoner:</p> <ul> <li>HermiT: HermiT is a highly optimized OWL reasoner developed by the University of Oxford. It provides efficient reasoning services for OWL ontologies and supports various OWL profiles.</li> <li>Pellet: Pellet is an OWL reasoner developed by Clark &amp; Parsia. It is known for its scalability and performance, making it suitable for large-scale ontologies. Pellet supports OWL 2 and provides reasoning support for various OWL profiles.</li> <li>FaCT++: FaCT++ is a highly optimized OWL reasoner developed by the University of Manchester. It is based on the FaCT system and provides reasoning services for OWL 2 ontologies. FaCT++ is known for its efficiency and scalability.</li> <li>RDFox: RDFox is a highly scalable semantic reasoning engine developed by Oxford Semantic Technologies. It supports OWL 2 RL and OWL 2 QL profiles and is designed for efficient reasoning over large-scale RDF datasets.</li> </ul>"},{"location":"working/onto/rucr_intro/","title":"RUCR Intro Description","text":""},{"location":"working/onto/rucr_intro/#ruskymir-creed-rucr","title":"Ruskymir Creed (RUCR)","text":"<p>What you can see here is a model of concepts and ideas commonly re-used by the Russian propaganda on the daily basis. It is currently comprised of {number_of_concepts} concepts (or tropes, as we call them) touching on all kinds of topics, from abortions to moon landing, but mainly concerned with diminishing and lying about Ukraine and its people.</p> <p>These are the ideas you'd hear if you were to turn on any today's Russian media outlet, apart from the exiled political opposition and those that focus on entertainment exclusively - although, this last category is far from innocent, as more and more singers, poets and producers turn to suck from the perennial tit of the state.</p> <p>You will notice that some of them would explicitly contradict others - this is normal, because it reflects the actual state of affairs, where people with, sometimes, diametrically opposing views unite in the blind adoration of Vladimir Putin's activities, jokes and speeches.</p> <p>If you're wondering, how exactly is this even possible, - this entire project is dedicated to finding the answer to this question. We believe that RUCR is a step in the direction of it.</p>"},{"location":"working/onto/rucr_intro/#how-it-works","title":"How it works","text":"<p>RUCR is based on the Simple Knowledge Organization System (SKOS), which is a standard developed by World Wide Web Consortium (also known as W3C) in &gt;&gt;&gt;&gt; mainly for representing existing systems of knowledge (such as library catalogs) in digital format.</p> <p>What's important to know, is that it defines 2 kinds of relations: the <code>broader</code> - <code>narrower</code> kind is hierarchical and says that a certain idea is broader or narrower in relation to another certain idea; and <code>related</code> kind says that two ideas are connected in a non-hierarchical way. For example, the idea that <code>homosexuality is a sin</code> is narrower to the idea that <code>homosexuality is wrong</code>; and that latter one is <code>related</code> to the idea that <code>liberal values were made up as a tool for disrupting nations from within</code>.</p> <p>Arrows on the diagram represent <code>broader</code> / <code>narrower</code> relationships, with the arrowhead pointing to the broader term. Representing <code>related</code> relations is under development.</p> <p>Two large circles contain the 15 core ideas: they are considered such because themselves they cannot be broken down any further (well, kind of - see the WIP note below). One of the circles contains anti-Ukrainian collection, the other - everything else. This layout is quite arbitrary, that is - dictated by aesthetics mostly; however the defined relations are solid.</p>"},{"location":"working/onto/rucr_intro/#wip","title":"WIP","text":"<p>RUCR is not a finished work: some concepts may not have been identified yet, others are yet to arise - this terrain is complex and fluid, and capturing and analyzing it takes time and work. If you feel like you can help us move things forward, send us a letter.</p>"},{"location":"working/onto/rucr_intro/#_1","title":"\u041a\u0440\u0435\u0434\u043e \u0440\u0443\u0441\u0441\u043a\u043e\u0433\u043e \u043c\u0438\u0440\u0430 (\u0420\u0423\u041a\u0420)","text":"<p>\u0422\u043e, \u0447\u0442\u043e \u0432\u044b \u043d\u0430\u0439\u0434\u0435\u0442\u0435 \u0437\u0434\u0435\u0441\u044c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0441\u043e\u0431\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u044c \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u0439 \u0438 \u0438\u0434\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u043e\u0439 \u043f\u0440\u043e\u043f\u0430\u0433\u0430\u043d\u0434\u043e\u0439. \u041d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u043e\u043d\u0430 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 {number_of_concepts} \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u0439 (\u0438\u043b\u0438, \u043a\u0430\u043a \u043c\u044b \u0438\u0445 \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u043c, \u0442\u0440\u043e\u043f\u043e\u0432), \u0437\u0430\u0442\u0440\u0430\u0433\u0438\u0432\u0430\u044e\u0449\u0438\u0445 \u0441\u0430\u043c\u044b\u0435 \u0440\u0430\u0437\u043d\u044b\u0435 \u0442\u0435\u043c\u044b, \u043e\u0442 \u0430\u0431\u043e\u0440\u0442\u043e\u0432 \u0434\u043e \u043f\u043e\u0441\u0430\u0434\u043a\u0438 \u043d\u0430 \u041b\u0443\u043d\u0443, \u043d\u043e \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u043d\u0430 \u043f\u0440\u0438\u043d\u0438\u0436\u0435\u043d\u0438\u0435 \u0423\u043a\u0440\u0430\u0438\u043d\u044b, \u0438 \u043b\u043e\u0436\u044c \u043e \u0435\u0435 \u043d\u0430\u0440\u043e\u0434\u0435.</p> <p>\u042d\u0442\u043e \u0442\u0435 \u0438\u0434\u0435\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u044b \u0443\u0441\u043b\u044b\u0448\u0438\u0442\u0435, \u0435\u0441\u043b\u0438 \u0432\u043a\u043b\u044e\u0447\u0438\u0442\u0435 \u043b\u044e\u0431\u043e\u0435 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0435 \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u043e\u0435 \u0421\u041c\u0418, \u0437\u0430 \u0438\u0441\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435\u043c \u0440\u0430\u0437\u0432\u0435 \u0447\u0442\u043e \u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043e\u043f\u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u0432 \u0438\u0437\u0433\u043d\u0430\u043d\u0438\u0438, \u0438 \u0442\u0435\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u0438\u0441\u043a\u043b\u044e\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043d\u0430 \u0440\u0430\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f - \u0432\u043f\u0440\u043e\u0447\u0435\u043c, \u044d\u0442\u0443 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u044e\u044e \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044e \u0442\u043e\u0436\u0435 \u0441\u043b\u043e\u0436\u043d\u043e \u043d\u0430\u0437\u0432\u0430\u0442\u044c \u0447\u0438\u0441\u0442\u043e\u0439, \u0432\u0435\u0434\u044c \u0432\u0441\u0435 \u0431\u043e\u043b\u044c\u0448\u0435 \u0438 \u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u0435\u0432\u0446\u043e\u0432, \u043f\u043e\u044d\u0442\u043e\u0432, \u043f\u0440\u043e\u0434\u044e\u0441\u0435\u0440\u043e\u0432 \u0438 \u0442.\u043f. \u0441\u0442\u0430\u0440\u0430\u044e\u0442\u0441\u044f \u043f\u0440\u0438\u043d\u0438\u043a\u043d\u0443\u0442\u044c \u043a \u043d\u0435\u043f\u0435\u0440\u0435\u0441\u044b\u0445\u0430\u044e\u0449\u0435\u0439 \u0433\u043e\u0441\u0443\u0434\u0430\u0440\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439 \u0433\u0440\u0443\u0434\u0438.</p> <p>\u0412\u044b \u0437\u0430\u043c\u0435\u0442\u0438\u0442\u0435, \u0447\u0442\u043e \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u0437 \u043d\u0438\u0445 \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0440\u0435\u0447\u0430\u0442 \u0434\u0440\u0443\u0433\u0438\u043c - \u044d\u0442\u043e \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e, \u043f\u043e\u0441\u043e\u043b\u044c\u043a\u0443 \u043e\u0442\u0440\u0430\u0436\u0430\u0435\u0442 \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u0435 \u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0432\u0435\u0449\u0435\u0439, \u0433\u0434\u0435 \u043b\u044e\u0434\u0438 \u0441 \u043f\u043e\u0440\u043e\u0439 \u0434\u0438\u0430\u043c\u0435\u0442\u0440\u0430\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u043f\u043e\u043b\u043e\u0436\u043d\u044b\u043c\u0438 \u0432\u0437\u0433\u043b\u044f\u0434\u0430\u043c\u0438 \u0441\u043b\u0438\u0432\u0430\u044e\u0442\u0441\u044f \u0432 \u0441\u043b\u0435\u043f\u043e\u043c \u043e\u0431\u043e\u0436\u0430\u043d\u0438\u0438 \u0440\u0435\u0448\u0435\u043d\u0438\u0439, \u0448\u0443\u0442\u043e\u043a \u0438 \u0446\u0438\u0442\u0430\u0442 \u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440\u0430 \u041f\u0443\u0442\u0438\u043d\u0430.</p> <p>\u0415\u0441\u043b\u0438 \u0432\u044b \u0437\u0430\u0434\u0430\u0435\u0442\u0435\u0441\u044c \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u043c, \u043a\u0430\u043a \u044d\u0442\u043e \u0432\u043e\u043e\u0431\u0449\u0435 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e, - \u0447\u0442\u043e \u0436, \u043d\u0430\u0448 \u043f\u0440\u043e\u0435\u043a\u0442 \u043f\u043e\u0441\u0432\u044f\u0449\u0435\u043d \u043f\u043e\u0438\u0441\u043a\u0443 \u043e\u0442\u0432\u0435\u0442\u0430 \u043d\u0430 \u044d\u0442\u043e\u0442 \u0432\u043e\u043f\u0440\u043e\u0441 \u0446\u0435\u043b\u0438\u043a\u043e\u043c \u0438 \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e. \u041c\u044b \u0441\u0447\u0438\u0442\u0430\u0435\u043c, \u0447\u0442\u043e \u0420\u0423\u041a\u0420 - \u044d\u0442\u043e \u0448\u0430\u0433 \u0432 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u043c \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0438.</p>"},{"location":"working/onto/rucr_intro/#_2","title":"\u041a\u0430\u043a \u044d\u0442\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442","text":"<p>\u0420\u0423\u041a\u0420 \u043e\u0441\u043d\u043e\u0432\u0430\u043d \u043d\u0430 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0435 \u043f\u043e\u0434 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435\u043c Simple Knowledge Organization System (SKOS), \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u044b\u043b \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u043d World Wide Web Consortium (\u0442\u0430\u043a\u0436\u0435 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u043c \u043a\u0430\u043a W3C) \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u0434\u043b\u044f \u0440\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0443\u0436\u0435 \u0441\u043b\u043e\u0436\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u0441\u0438\u0441\u0442\u0435\u043c \u0437\u043d\u0430\u043d\u0438\u0439 (\u0442\u0430\u043a\u0438\u0445 \u043a\u0430\u043a \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u0447\u043d\u044b\u0435 \u043a\u0430\u0442\u0430\u043b\u043e\u0433\u0438) \u0432 \u0446\u0438\u0444\u0440\u043e\u0432\u043e\u043c \u0444\u043e\u0440\u043c\u0430\u0442\u0435.</p> <p>\u0417\u0434\u0435\u0441\u044c \u0432\u0430\u0436\u043d\u043e \u0437\u043d\u0430\u0442\u044c, \u0447\u0442\u043e SKOS \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442 2 \u0442\u0438\u043f\u0430 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439: <code>broader</code> (\u0431\u043e\u043b\u0435\u0435 \u0448\u0438\u0440\u043e\u043a\u0438\u0439) - <code>narrower</code> (\u0431\u043e\u043b\u0435\u0435 \u0443\u0437\u043a\u0438\u0439) - \u044d\u0442\u043e \u0438\u0435\u0440\u0430\u0440\u0445\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0442\u0438\u043f, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442, \u0447\u0442\u043e \u043d\u0435\u043a\u0430\u044f \u0438\u0434\u0435\u044f \u0431\u043e\u043b\u0435\u0435 \u043e\u0431\u0449\u0430\u044f \u0438\u043b\u0438 \u0431\u043e\u043b\u0435\u0435 \u0441\u043f\u0435\u0446\u0438\u0444\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u043e \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044e \u043a \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0434\u0440\u0443\u0433\u043e\u0439 \u0438\u0434\u0435\u0435; \u0438 <code>related</code> \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442 \u043d\u0435-\u0438\u0435\u0440\u0430\u0440\u0445\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u0441\u0432\u044f\u0437\u044c \u043c\u0435\u0436\u0434\u0443 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0430\u043c\u0438. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0438\u0434\u0435\u044f, \u0447\u0442\u043e <code>\u0433\u043e\u043c\u043e\u0441\u0435\u043a\u0441\u0443\u0430\u043b\u0438\u0437\u043c - \u044d\u0442\u043e \u0433\u0440\u0435\u0445</code>, \u0443\u0436\u0435 \u043f\u043e \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044e \u043a \u0438\u0434\u0435\u0435, \u0447\u0442\u043e <code>\u0433\u043e\u043c\u043e\u0441\u0435\u043a\u0441\u0443\u0430\u043b\u0438\u0437\u043c - \u044d\u0442\u043e \u043f\u043b\u043e\u0445\u043e</code>, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0432 \u0441\u0432\u043e\u044e \u043e\u0447\u0435\u0440\u0435\u0434\u044c <code>\u0441\u0432\u044f\u0437\u0430\u043d\u0430</code> \u0441 \u0438\u0434\u0435\u0435\u0439, \u0447\u0442\u043e <code>\u043b\u0438\u0431\u0435\u0440\u0430\u043b\u044c\u043d\u044b\u0435 \u0446\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0431\u044b\u043b\u0438 \u0438\u0437\u043e\u0431\u0440\u0435\u0442\u0435\u043d\u044b \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043b\u044f \u0440\u0430\u0437\u0440\u0443\u0448\u0435\u043d\u0438\u044f \u043d\u0430\u0446\u0438\u0439 \u0438\u0437\u043d\u0443\u0442\u0440\u0438</code>.</p> <p>\u0421\u0432\u044f\u0437\u0438 \u043d\u0430 \u0434\u0438\u0430\u0433\u0440\u0430\u043c\u043c\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f <code>\u0448\u0438\u0440\u0435</code> / <code>\u0443\u0436\u0435</code>, \u043f\u0440\u0438\u0447\u0435\u043c \u0441\u0442\u0440\u0435\u043b\u043a\u0430 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043d\u0430 \u0431\u043e\u043b\u0435\u0435 \u0448\u0438\u0440\u043e\u043a\u0438\u0439 \u0442\u0435\u0440\u043c\u0438\u043d. \u0420\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0446\u0438\u044f <code>related</code> \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439 \u043f\u043e\u043a\u0430 \u0447\u0442\u043e \u0432 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0435.</p> <p>\u0414\u0432\u0430 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u043a\u0440\u0443\u0433\u0430 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 15 \u043a\u043e\u0440\u043d\u0435\u0432\u044b\u0445 \u0438\u0434\u0435\u0439: \u043c\u044b \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0438\u0445 \u0442\u0430\u043a\u043e\u0432\u044b\u043c\u0438, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0432 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 \u043d\u0438\u0445 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0430\u044f \u0434\u0435\u0442\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0437\u0430\u0442\u0440\u0443\u0434\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u0430 (\u043e\u0434\u043d\u0430\u043a\u043e \u0441\u043c. \u043f\u0440\u0438\u043c\u0435\u0447\u0430\u043d\u0438\u0435 \u043d\u0438\u0436\u0435). \u041e\u0434\u0438\u043d \u0438\u0437 \u043d\u0438\u0445 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u043a\u043e\u043b\u043b\u0435\u043a\u0446\u0438\u044e \u0430\u043d\u0442\u0438\u0443\u043a\u0440\u0430\u0438\u043d\u0441\u043a\u0438\u0445 \u0438\u0434\u0435\u0439, \u0434\u0440\u0443\u0433\u043e\u0439 - \u0432\u0441\u0435 \u043f\u0440\u043e\u0447\u0435\u0435. \u0422\u0430\u043a\u0430\u044f \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u0432\u043f\u043e\u043b\u043d\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u043b\u044c\u043d\u0430, \u0442\u043e \u0435\u0441\u0442\u044c, \u043f\u0440\u043e\u0434\u0438\u043a\u0442\u043e\u0432\u0430\u043d\u043e \u043f\u0440\u0435\u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u044d\u0441\u0442\u0435\u0442\u0438\u043a\u043e\u0439; \u043e\u0434\u043d\u0430\u043a\u043e \u0442\u0435 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u044b \u043c\u0435\u0436\u0434\u0443 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0430\u043c\u0438, - \u0432 \u043f\u043e\u0440\u044f\u0434\u043a\u0435.</p>"},{"location":"working/onto/rucr_intro/#_3","title":"\u0420\u0430\u0431\u043e\u0442\u0430 \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u0435\u0442\u0441\u044f","text":"<p>\u0420\u0423\u041a\u0420 \u043d\u0435 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u044b\u043c \u043f\u0440\u043e\u0435\u043a\u0442\u043e\u043c: \u043a\u0430\u043a\u0438\u0435-\u0442\u043e \u0438\u0434\u0435\u0438 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u0435\u0449\u0435 \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u044b, \u0434\u0440\u0443\u0433\u0438\u043c \u0435\u0449\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u043e\u0438\u0442 \u043f\u0440\u043e\u044f\u0432\u0438\u0442\u044c\u0441\u044f - \u044d\u0442\u043e \u0442\u0435\u0440\u0440\u0438\u0442\u043e\u0440\u0438\u044f \u0441\u043b\u043e\u0436\u043d\u0430\u044f \u0438 \u0438\u0437\u043c\u0435\u043d\u0447\u0438\u0432\u0430\u044f, \u0441\u0431\u043e\u0440 \u0438 \u0430\u043d\u0430\u043b\u0438\u0437 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u0442\u0440\u0435\u0431\u0443\u044e\u0442 \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0438 \u0442\u0440\u0443\u0434\u0430. \u0415\u0441\u043b\u0438 \u0432\u044b \u0434\u0443\u043c\u0430\u0435\u0442\u0435, \u0447\u0442\u043e \u043c\u043e\u0436\u0435\u0442\u0435 \u043f\u043e\u043c\u043e\u0447\u044c \u043d\u0430\u043c \u043f\u0440\u043e\u0434\u0432\u0438\u043d\u0443\u0442\u044c\u0441\u044f \u0434\u0430\u043b\u044c\u0448\u0435, \u043d\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u043d\u0430\u043c \u043f\u0438\u0441\u044c\u043c\u043e.</p>"},{"location":"working/onto/rucr_intro/#_4","title":"\u041a\u0440\u0435\u0434\u043e \u0440\u0443\u0441\u044c\u043a\u043e\u0433\u043e \u043c\u0438\u0440\u0443 (\u0420\u0423\u041a\u0420)","text":"<p>\u0422\u0435, \u0449\u043e \u0432\u0438 \u0442\u0443\u0442 \u0431\u0430\u0447\u0438\u0442\u0435, \u0454 \u043c\u043e\u0434\u0435\u043b\u043b\u044e \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0456\u0439 \u0442\u0430 \u0456\u0434\u0435\u0439, \u044f\u043a\u0456 \u0440\u043e\u0441\u0456\u0439\u0441\u044c\u043a\u0430 \u043f\u0440\u043e\u043f\u0430\u0433\u0430\u043d\u0434\u0430 \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u043e\u0432\u0443\u0454 \u043f\u043e\u0441\u0442\u0456\u0439\u043d\u043e. \u041d\u0430\u0440\u0430\u0437\u0456 \u0432\u043e\u043d\u0430 \u0441\u043a\u043b\u0430\u0434\u0430\u0454\u0442\u044c\u0441\u044f \u0437 {number_of_concepts} \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0456\u0439 (\u0430\u0431\u043e \u0442\u0440\u043e\u043f\u0456\u0432, \u044f\u043a \u043c\u0438 \u0457\u0445 \u043d\u0430\u0437\u0438\u0432\u0430\u0454\u043c\u043e), \u044f\u043a\u0456 \u0441\u0442\u043e\u0441\u0443\u044e\u0442\u044c\u0441\u044f \u0432\u0441\u0456\u043b\u044f\u043a\u0438\u0445 \u0442\u0435\u043c, \u0432\u0456\u0434 \u0430\u0431\u043e\u0440\u0442\u0456\u0432 \u0434\u043e \u043f\u0440\u0438\u0437\u0435\u043c\u043b\u0435\u043d\u043d\u044f \u043d\u0430 \u043c\u0456\u0441\u044f\u0446\u044c, \u0430\u043b\u0435 \u0433\u043e\u043b\u043e\u0432\u043d\u0438\u043c \u0447\u0438\u043d\u043e\u043c \u0441\u043f\u0440\u044f\u043c\u043e\u0432\u0430\u043d\u0456 \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u043d\u0448\u0435\u043d\u043d\u044f \u0442\u0430 \u0431\u0440\u0435\u0445\u043d\u044e \u043f\u0440\u043e \u0423\u043a\u0440\u0430\u0457\u043d\u0443 \u0442\u0430 \u0457\u0457 \u043b\u044e\u0434\u0435\u0439.</p> <p>\u0426\u0435 \u0442\u0456 \u0456\u0434\u0435\u0457, \u044f\u043a\u0456 \u0432\u0438 \u043f\u043e\u0447\u0443\u0454\u0442\u0435, \u044f\u043a\u0449\u043e \u0432\u0432\u0456\u043c\u043a\u043d\u0435\u0442\u0435 \u0431\u0443\u0434\u044c-\u044f\u043a\u0438\u0439 \u0441\u0443\u0447\u0430\u0441\u043d\u0438\u0439 \u0440\u043e\u0441\u0456\u0439\u0441\u044c\u043a\u0438\u0439 \u043c\u0435\u0434\u0456\u0430-\u0440\u0435\u0441\u0443\u0440\u0441, \u0437\u0430 \u0432\u0438\u043d\u044f\u0442\u043a\u043e\u043c \u043f\u043e\u043b\u0456\u0442\u0438\u0447\u043d\u043e\u0457 \u043e\u043f\u043e\u0437\u0438\u0446\u0456\u0457 \u0443 \u0432\u0438\u0433\u043d\u0430\u043d\u043d\u0456, \u0442\u0430 \u0442\u0438\u0445, \u0449\u043e \u0437\u043e\u0441\u0435\u0440\u0435\u0434\u0436\u0443\u044e\u0442\u044c\u0441\u044f \u0432\u0438\u043d\u044f\u0442\u043a\u043e\u0432\u043e \u043d\u0430 \u0440\u043e\u0437\u0432\u0430\u0433\u0430\u0445 - \u0445\u043e\u0447\u0430 \u0446\u044f \u043e\u0441\u0442\u0430\u043d\u043d\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0456\u044f \u0442\u0435\u0436 \u0434\u0430\u043b\u0435\u043a\u0430 \u043d\u0435 \u0431\u0435\u0437\u0432\u0438\u043d\u043d\u0430, \u043e\u0441\u043a\u0456\u043b\u044c\u043a\u0438 \u0432\u0441\u0435 \u0431\u0456\u043b\u044c\u0448\u0435 \u0441\u043f\u0456\u0432\u0430\u043a\u0456\u0432, \u043f\u043e\u0435\u0442\u0456\u0432 \u0442\u0430 \u043f\u0440\u043e\u0434\u044e\u0441\u0435\u0440\u0456\u0432 \u043e\u0431\u0438\u0440\u0430\u044e\u0442\u044c \u043d\u0430\u043c\u0430\u0433\u0430\u044e\u0442\u044c\u0441\u044f \u043f\u0440\u0438\u043f\u0430\u0441\u0442\u0438 \u0434\u043e \u043d\u0435\u043f\u0435\u0440\u0435\u0441\u0438\u0445\u0430\u044e\u0447\u0438\u0445 \u0434\u0435\u0440\u0436\u0430\u0432\u043d\u0438\u0445 \u0433\u0440\u0443\u0434\u0435\u0439.</p> <p>\u0412\u0438 \u043f\u043e\u043c\u0456\u0442\u0438\u0442\u0435, \u0449\u043e \u0434\u0435\u044f\u043a\u0456 \u0437 \u043d\u0438\u0445 \u0432\u043e\u0447\u0435\u0432\u0438\u0434\u044c \u0441\u0443\u043f\u0435\u0440\u0435\u0447\u0430\u0442\u044c \u0456\u043d\u0448\u0438\u043c - \u0446\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e, \u043e\u0441\u043a\u0456\u043b\u044c\u043a\u0438 \u0432\u0456\u0434\u043e\u0431\u0440\u0430\u0436\u0430\u0454 \u0440\u0435\u0430\u043b\u044c\u043d\u0438\u0439 \u0441\u0442\u0430\u043d \u0440\u0435\u0447\u0435\u0439, \u043a\u043e\u043b\u0438 \u043b\u044e\u0434\u0438, \u0437 \u0456\u043d\u043e\u0434\u0456 \u0434\u0456\u0430\u043c\u0435\u0442\u0440\u0430\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0442\u0438\u043b\u0435\u0436\u043d\u0438\u043c\u0438 \u043f\u043e\u0433\u043b\u044f\u0434\u0430\u043c\u0438, \u043e\u0431'\u0454\u0434\u043d\u0443\u044e\u0442\u044c\u0441\u044f \u0432 \u0441\u043b\u0456\u043f\u043e\u043c\u0443 \u0437\u0430\u0445\u043e\u043f\u043b\u0435\u043d\u043d\u0456 \u0440\u0456\u0448\u0435\u043d\u043d\u044f\u043c\u0438, \u0434\u0456\u044f\u043b\u044c\u043d\u0456\u0441\u0442\u044c\u044e, \u0436\u0430\u0440\u0442\u0430\u043c\u0438 \u0442\u0430 \u0446\u0438\u0442\u0430\u0442\u0430\u043c\u0438 \u0412\u043e\u043b\u043e\u0434\u0438\u043c\u0438\u0440\u0430 \u041f\u0443\u0442\u0456\u043d\u0430.</p> <p>\u042f\u043a\u0449\u043e \u0432\u0438 \u0446\u0456\u043a\u0430\u0432\u0438\u0442\u0435\u0441\u044c, \u044f\u043a \u0446\u0435 \u0432\u0437\u0430\u0433\u0430\u043b\u0456 \u043c\u043e\u0436\u043b\u0438\u0432\u043e, - \u0432\u0435\u0441\u044c \u043d\u0430\u0448 \u043f\u0440\u043e\u0435\u043a\u0442 \u043f\u0440\u0438\u0441\u0432\u044f\u0447\u0435\u043d\u043e \u043f\u043e\u0448\u0443\u043a\u0443 \u0432\u0456\u0434\u043f\u043e\u0432\u0456\u0434\u0456 \u043d\u0430 \u0446\u0435 \u043f\u0438\u0442\u0430\u043d\u043d\u044f. \u041c\u0438 \u0432\u0432\u0430\u0436\u0430\u0454\u043c\u043e, \u0449\u043e \u0420\u0423\u041a\u0420 - \u0446\u0435 \u043a\u0440\u043e\u043a \u0443 \u0446\u044c\u043e\u043c\u0443 \u043d\u0430\u043f\u0440\u044f\u043c\u043a\u0443.</p>"},{"location":"working/onto/rucr_intro/#_5","title":"\u042f\u043a \u0446\u0435 \u043f\u0440\u0430\u0446\u044e\u0454","text":"<p>\u0420\u0423\u041a\u0420 \u0431\u0430\u0437\u0443\u0454\u0442\u044c\u0441\u044f \u043d\u0430 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0456 \u043f\u0456\u0434 \u043d\u0430\u0437\u0432\u043e\u044e Simple Knowledge Organization System (SKOS), \u044f\u043a\u0438\u0439 \u0431\u0443\u0432 \u0440\u043e\u0437\u0440\u043e\u0431\u043b\u0435\u043d World Wide Web Consortium (\u0432\u0456\u0434\u043e\u043c\u0438\u043c \u044f\u043a W3C) \u0432 &gt;&gt;&gt;&gt; \u0433\u043e\u043b\u043e\u0432\u043d\u0438\u043c \u0447\u0438\u043d\u043e\u043c \u0434\u043b\u044f \u0440\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0446\u0456\u0457 \u0456\u0441\u043d\u0443\u044e\u0447\u0438\u0445 \u0441\u0438\u0441\u0442\u0435\u043c \u0437\u043d\u0430\u043d\u044c (\u0442\u0430\u043a\u0438\u0445 \u044f\u043a \u0431\u0456\u0431\u043b\u0456\u043e\u0442\u0435\u0447\u043d\u0456 \u043a\u0430\u0442\u0430\u043b\u043e\u0433\u0438) \u0432 \u0446\u0438\u0444\u0440\u043e\u0432\u043e\u043c\u0443 \u0444\u043e\u0440\u043c\u0430\u0442\u0456.</p> <p>\u0412\u0430\u0436\u043b\u0438\u0432\u043e \u0437\u043d\u0430\u0442\u0438, \u0449\u043e \u0432\u043e\u043d\u0430 \u0432\u0438\u0437\u043d\u0430\u0447\u0430\u0454 2 \u0442\u0438\u043f\u0438 \u0432\u0456\u0434\u043d\u043e\u0441\u0438\u043d: <code>broader</code> (\u0448\u0438\u0440\u0448\u0435) - <code>narrower</code> (\u0432\u0443\u0436\u0447\u0435) - \u0446\u0435 \u0456\u0454\u0440\u0430\u0440\u0445\u0456\u0447\u043d\u0438\u0439 \u0442\u0438\u043f, \u044f\u043a\u0438\u0439 \u0432\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u044e\u0454, \u0449\u043e \u043f\u0435\u0432\u043d\u0430 \u0456\u0434\u0435\u044f \u0454 \u0431\u0456\u043b\u044c\u0448 \u0437\u0430\u0433\u0430\u043b\u044c\u043d\u043e\u044e \u0430\u0431\u043e \u0431\u0456\u043b\u044c\u0448 \u0441\u043f\u0435\u0446\u0438\u0444\u0456\u0447\u043d\u043e\u044e \u0432\u0456\u0434\u043d\u043e\u0441\u043d\u043e \u0456\u043d\u0448\u043e\u0457 \u043f\u0435\u0432\u043d\u043e\u0457 \u0456\u0434\u0435\u0457; \u0430 <code>related</code> \u0432\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u044e \u043d\u0435-\u0456\u0454\u0440\u0430\u0440\u0445\u0456\u0447\u043d\u0438\u0439 \u0437\u0432'\u044f\u0437\u043e\u043a \u043c\u0456\u0436 \u0434\u0432\u043e\u043c\u0430 \u0456\u0434\u0435\u044f\u043c\u0438. \u041d\u0430\u043f\u0440\u0438\u043a\u043b\u0430\u0434, \u0456\u0434\u0435\u044f, \u0449\u043e <code>\u0433\u043e\u043c\u043e\u0441\u0435\u043a\u0441\u0443\u0430\u043b\u0456\u0437\u043c \u0454 \u0433\u0440\u0456\u0445\u043e\u043c</code>, \u0454 \u0432\u0443\u0436\u0447\u043e\u044e \u0437\u0430 \u0456\u0434\u0435\u044e, \u0449\u043e <code>\u0433\u043e\u043c\u043e\u0441\u0435\u043a\u0441\u0443\u0430\u043b\u0456\u0437\u043c \u0446\u0435 \u043f\u043e\u0433\u0430\u043d\u043e</code>, \u0430 \u0446\u044f \u043e\u0441\u0442\u0430\u043d\u043d\u044f, \u0432 \u0441\u0432\u043e\u044e \u0447\u0435\u0440\u0433\u0443,<code>\u043f\u043e\u0432'\u044f\u0437\u0430\u043d\u0430</code> \u0437 \u0456\u0434\u0435\u0454\u044e, \u0449\u043e <code>\u043b\u0456\u0431\u0435\u0440\u0430\u043b\u044c\u043d\u0456 \u0446\u0456\u043d\u043d\u043e\u0441\u0442\u0456 \u0431\u0443\u043b\u0438 \u0432\u0438\u0433\u0430\u0434\u0430\u043d\u0456 \u044f\u043a \u0456\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043b\u044f \u0440\u0443\u0439\u043d\u0443\u0432\u0430\u043d\u043d\u044f \u043d\u0430\u0446\u0456\u0439 \u0437\u0441\u0435\u0440\u0435\u0434\u0438\u043d\u0438</code>.</p> <p>\u0421\u0442\u0440\u0456\u043b\u043a\u0438 \u043d\u0430 \u0434\u0456\u0430\u0433\u0440\u0430\u043c\u0456 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442\u044c \u0432\u0456\u0434\u043d\u043e\u0441\u0438\u043d\u0438 <code>\u0448\u0438\u0440\u0448\u0435</code> / <code>\u0432\u0443\u0436\u0447\u0435</code>, \u0442\u0430 \u0432\u043a\u0430\u0437\u0443\u044e\u0442\u044c \u043d\u0430 \u0448\u0438\u0440\u0448\u0438\u0439 \u0442\u0435\u0440\u043c\u0456\u043d. \u0420\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0446\u0456\u044f <code>\u043f\u043e\u0432'\u044f\u0437\u0430\u043d\u0438\u0445</code> \u0432\u0456\u0434\u043d\u043e\u0441\u0438\u043d \u043d\u0430\u0440\u0430\u0437\u0456 \u0443 \u0440\u043e\u0437\u0440\u043e\u0431\u0446\u0456.</p> <p>\u0414\u0432\u0430 \u0432\u0435\u043b\u0438\u043a\u0438\u0445 \u043a\u043e\u043b\u0430 \u043c\u0456\u0441\u0442\u044f\u0442\u044c 15 \u043a\u043e\u0440\u0456\u043d\u043d\u0438\u0445 \u0456\u0434\u0435\u0439: \u043c\u0438 \u0432\u0432\u0430\u0436\u0430\u0454\u043c\u043e \u0457\u0445 \u0442\u0430\u043a\u0438\u043c\u0438, \u0442\u043e\u043c\u0443 \u0449\u043e \u0437 \u043d\u0438\u0445 \u0441\u043a\u043b\u0430\u0434\u043d\u043e \u0432\u0438\u0432\u0435\u0441\u0442\u0438 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0438 \u0449\u0435 \u0431\u0456\u043b\u044c\u0448 \u0437\u0430\u0433\u0430\u043b\u044c\u043d\u0456 (\u0430\u043b\u0435 \u0434\u0438\u0432. \u043d\u0438\u0436\u0447\u0435). \u041e\u0434\u043d\u0435 \u0437 \u043a\u0456\u043b \u043c\u0456\u0441\u0442\u0438\u0442\u044c \u043a\u043e\u043b\u0435\u043a\u0446\u0456\u044e \u0430\u043d\u0442\u0438\u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0438\u0445 \u0456\u0434\u0435\u0439, \u0434\u0440\u0443\u0433\u0435 - \u0432\u0441\u0435 \u0456\u043d\u0448\u0435. \u0426\u044f \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u0434\u043e\u0441\u0438\u0442\u044c \u0434\u043e\u0432\u0456\u043b\u044c\u043d\u0430, \u0442\u043e\u0431\u0442\u043e  \u043f\u0440\u043e\u0434\u0438\u043a\u0442\u043e\u0432\u0430\u043d\u0430 \u043f\u0435\u0440\u0435\u0432\u0430\u0436\u043d\u043e \u0435\u0441\u0442\u0435\u0442\u0438\u043a\u043e\u044e; \u043e\u0434\u043d\u0430\u043a \u0432\u0438\u0437\u043d\u0430\u0447\u0435\u043d\u0456 \u043d\u0430\u043c\u0438 \u0432\u0456\u0434\u043d\u043e\u0441\u0438\u043d\u0438 \u0454 \u043e\u0431\u0491\u0440\u0443\u043d\u0442\u043e\u0432\u0430\u043d\u0438\u043c\u0438.</p>"},{"location":"working/onto/rucr_intro/#_6","title":"\u0420\u043e\u0431\u043e\u0442\u0430 \u043f\u0440\u043e\u0434\u043e\u0432\u0436\u0443\u0454\u0442\u044c\u0441\u044f","text":"<p>\u0420\u0423\u041a\u0420 \u043d\u0435 \u0454 \u0434\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043e\u044e \u0440\u043e\u0431\u043e\u0442\u043e\u044e: \u0434\u0435\u044f\u043a\u0456 \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0456\u0457 \u0449\u0435, \u043c\u043e\u0436\u043b\u0438\u0432\u043e, \u043d\u0435 \u0432\u0438\u044f\u0432\u043b\u0435\u043d\u0456, \u0456\u043d\u0448\u0456 \u0442\u0456\u043b\u044c\u043a\u0438 \u043c\u0430\u044e\u0442\u044c \u0437'\u044f\u0432\u0438\u0442\u0438\u0441\u044f - \u0446\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c \u0441\u043a\u043b\u0430\u0434\u043d\u0430 \u0456 \u0434\u0443\u0436\u0435 \u0448\u0432\u0438\u0434\u043a\u043e\u043f\u043b\u0438\u043d\u043d\u0430; \u0457\u0457 \u0432\u0438\u0432\u0447\u0435\u043d\u043d\u044f \u0442\u0430 \u0430\u043d\u0430\u043b\u0456\u0437 \u0437\u0430\u0439\u043c\u0430\u044e\u0442\u044c \u0447\u0430\u0441 \u0456 \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u044e\u0442\u044c \u043f\u0440\u0430\u0446\u0456. \u042f\u043a\u0449\u043e \u0432\u0438 \u0432\u0456\u0434\u0447\u0443\u0432\u0430\u0454\u0442\u0435, \u0449\u043e \u043c\u043e\u0436\u0435\u0442\u0435 \u0434\u043e\u043f\u043e\u043c\u043e\u0433\u0442\u0438 \u043d\u0430\u043c \u043f\u0440\u043e\u0441\u0443\u0432\u0430\u0442\u0438 \u0446\u0435 \u0432\u0441\u0435 \u0432\u043f\u0435\u0440\u0435\u0434, \u043d\u0430\u0434\u0456\u0448\u043b\u0456\u0442\u044c \u043d\u0430\u043c \u043b\u0438\u0441\u0442\u0430.</p>"},{"location":"working/onto/setting_the_stage/","title":"Ontologies. Setting the stage","text":""},{"location":"working/onto/setting_the_stage/#what-are-we-building","title":"What are we building?","text":"<ul> <li>A set of interrelated ontologies and taxonomies that, taken together, describe the domain of propaganda; and</li> <li>A number of web applications, whose structure is tied to these ontologies, and whose content is focused on the Russian propaganda phenomena that stems from the Vladimir Putin's delusional view of the world, and the Russo-Ukrainian war he started on Feb 24, 2022.</li> </ul>"},{"location":"working/onto/setting_the_stage/#why-are-we-building-it","title":"Why are we building it?","text":"<p>The domain in question is highly versatile, with natural language playing a major role in it. There are hundreds of entities involved in the process, the relationships between which have to be mined and parsed. Moreover, the amount of data that needs to be processed is insurmountable without extra tricks in our sleeve. One such trick is machine learning (we already have 1 application and there will be more).</p> <p>But it's the semantic web technologies that would help us simplify the task of tieing all the different pieces together into a coherent web of knowledge that can be used to answer some interesting questions.</p>"},{"location":"working/onto/setting_the_stage/#how-are-we-building-it","title":"How are we building it?","text":"<p>The scope, structure, and content of the ontologies would be driven by the actual data that our software is expected to interact with: that is, entities and relationships between them would be defined against the mass of available Russian data. However, in order to introduce a sort of validation element into the process, these entities must be applied to at least 1 of the following:</p> <ul> <li>The propaganda phenomena of the Nazi Germany, 1923-1945.</li> <li>The Rwandan genocide of 1994 and its roots.</li> </ul> <p>The individual instances and other entities would be generated (and regularly updated) by mining a variety of structured and unstructured data sources.</p> <p>The mining would be made automatic to the furthest extent possible, but with a required human-based verification.</p> <p>Natural language processing and entity disambiguation would be an integral part of the flow.</p>"},{"location":"working/onto/setting_the_stage/#who-is-building-it","title":"Who is building it?","text":"<p>As of the moment of writing this, exactly 1 person is involved in the ontology development. The rest of the Wapaganda team is working mainly with the web application. People skillful in machine learning, natural language processing, etc. may still join the team at some point, but for now we are learning all these things ourselves in order to apply them.</p>"},{"location":"working/onto/setting_the_stage/#bias-awareness-notice","title":"Bias awareness notice","text":"<p>The first author of the project hasn't left Ukraine since the war started, and all the other team members have some ties to the situation in one way or another - that is to say, we all have skin in the game. This provides energy and vigor, which are crucial in achieving the goal, but also inevitably results in a certain bias. The nature of this bias is known to us, and we are taking extra measures to prevent it from effecting the quality of the models.</p>"},{"location":"working/onto/setting_the_stage/#who-cares","title":"Who cares?","text":"<p>Since one of the underlying drivers behind the entire project is the desire to understand, to make sense of what we are observing, everybody who experiences the same lack of clarity on the subject, may care. It is a firm conviction of the authors, that achieving the goals of the project would bring us slightly closer to understanding the process of human thinking in general, and of thinking aberrations, in particular.</p>"},{"location":"working/onto/web_resources/","title":"W3C","text":"<p>https://www.w3.org/2001/sw/</p>"},{"location":"working/onto/web_resources/#upper-ontologies","title":"Upper Ontologies","text":"<p>Terms mentioned in <code>Relevance</code> fields provide basis to be extended by the W ontology.</p>"},{"location":"working/onto/web_resources/#foaf","title":"FOAF","text":"Name FOAF :: Friend-of-a-Friend URL http://xmlns.com/foaf/0.1/# Description FOAF provides a standardized way to describe people and their relationships, such as friends, colleagues, and acquaintances Relevance FOAF uses foundational class <code>Agent</code> that has a number of relevant subclasses, namely <code>Person</code>, <code>Group</code> and <code>Organization</code>."},{"location":"working/onto/web_resources/#dublin-core","title":"Dublin Core","text":"Name URL https://www.dublincore.org/specifications/dublin-core/dcmi-terms/ Description Dublin Core is a metadata standard for describing digital resources, providing a set of basic elements to enhance resource discovery and interoperability on the web.It offers 15 core metadata elements, such as <code>title</code>, <code>creator</code>, <code>date</code>, and <code>description</code>, which can be used to describe various types of resources, including web pages, images, videos, and documents. Relevance DC should be used for all the media-entities."},{"location":"working/onto/web_resources/#bio","title":"BIO","text":"Name BIO URL https://vocab.org/bio/ Description Vocabulary for describing biographical information about people, both living and dead Relevance BIO should be used as a FOAF extension to describe relationships between people, as well events related to them"},{"location":"working/onto/web_resources/#sioc","title":"SIOC","text":"Name SIOC :: Semantically-Interlinked Online Communities URL http://rdfs.org/sioc/spec/# Description SIOC is an attempt to link online community sites (weblogs, message boards, wikis, etc.) and to describe the information that communities have about their structure and contents, and to find related information and new connections between content items and other community objects Relevance SIOC could be useful as far as describing the online communities and resources"},{"location":"working/onto/web_resources/#skos","title":"SKOS","text":"Name SKOS :: Simple Knowledge Organization System URL https://www.w3.org/2009/08/skos-reference/skos.html Description SKOS is a vocabulary and data model designed for representing and organizing knowledge organization systems, such as thesauri, taxonomies, and classification schemes. SKOS enables the creation of multilingual and interoperable knowledge models. Relevance SKOS would be useful for the task of modelling the knowledge domain - the system of axioms, assertions etc. that make up the propagandistic address."},{"location":"working/onto/web_resources/#qudt","title":"QUDT","text":"Name QUDT :: Quantities, Units, Dimensions, and Data Types URL https://qudt.org/ Description QUDT is a comprehensive ontology and data model that provides a standardized representation of physical quantities, units of measurement, dimensions, and data types Relevance Might not be relevant"},{"location":"working/onto/web_resources/#cito","title":"CiTO","text":"Name CiTO :: Citation Typing Ontology URL https://sparontologies.github.io/cito/current/cito.html# Description CiTO is an ontology that enables characterization of the nature or type of citations, both factually and rhetorically. Relevance CiTO would be useful for the task of modelling the knowledge domain - the system of axioms, assertions etc. that make up the propagandistic address, in support to SKOS."},{"location":"working/onto/web_resources/#powder","title":"POWDER","text":"Name POWDER :: Protocol for Web Description Resources URL https://www.w3.org/TR/powder-primer/ Description POWDER provides a mechanism to describe and discover Web resources, in which respect its functionality overlaps with SIOC and DC. Relevance POWDER, although a W3C recommendation, failed to gain momentum and probably ain't relevant."},{"location":"working/onto/web_resources/#other-ontologies","title":"Other ontologies","text":""},{"location":"working/onto/web_resources/#yago","title":"YAGO","text":"Name YAGO URL https://yago-knowledge.org/ Description YAGO is a large knowledge base with general knowledge about people, cities, countries, movies, and organizations. Relevance YAGO provides 2 useful things: a way to properly link entities to Wikidata pages, and the ontology to support some extra kinds of relations. It would be useful for resolving specific entities (people, organizations, etc.)"},{"location":"working/onto/web_resources/#geonames","title":"GeoNames","text":"Name GeoNames URL https://www.geonames.org/ Description Relevance"},{"location":"working/onto/web_resources/#platforms","title":"Platforms","text":""},{"location":"working/onto/web_resources/#allegrograph","title":"AllegroGraph","text":"Name URL https://allegrograph.com Description Relevance"},{"location":"working/onto/web_resources/#ontotext-graphdb","title":"Ontotext GraphDB","text":"Name URL https://www.ontotext.com/products/graphdb/ Description Relevance"},{"location":"working/onto/web_resources/#stardog","title":"Stardog","text":"Name URL https://www.stardog.com Description Relevance"},{"location":"working/onto/web_resources/#marklogic","title":"MarkLogic","text":"Name URL https://www.marklogic.com/ Description Relevance"},{"location":"working/onto/web_resources/#rdfox","title":"RDFox","text":"Name URL https://www.oxfordsemantic.tech Description Relevance"},{"location":"working/onto/web_resources/#neon4j","title":"Neon4J","text":"Name URL Description Relevance"},{"location":"working/onto/web_resources/#rdf4j","title":"RDF4J","text":"Name URL https://rdf4j.org/ Description Relevance"},{"location":"working/onto/web_resources/#apache-jena","title":"Apache Jena","text":"Name URL https://jena.apache.org/ Description Relevance"},{"location":"working/onto/web_resources/#cozodb","title":"CozoDB","text":"Name URL https://github.com/cozodb Description Relevance"},{"location":"working/onto/web_resources/#standards","title":"Standards","text":""},{"location":"working/onto/web_resources/#owl","title":"OWL","text":"Name OWL :: Web Ontology Language URL https://www.w3.org/TR/2004/REC-owl-semantics-20040210/ Description Relevance"},{"location":"working/onto/web_resources/#rdf","title":"RDF","text":"Name RDF :: Resource Description Framework URL https://www.w3.org/TR/rdf11-primer Description Relevance"},{"location":"working/onto/web_resources/#tools","title":"Tools","text":""},{"location":"working/onto/web_resources/#aida","title":"AIDA","text":"Name URL https://github.com/codepie/aida Description Relevance"},{"location":"working/onto/web_resources/#protege","title":"Protege","text":"Name URL https://protegeproject.github.io/protege/ Description Relevance"},{"location":"working/onto/web_resources/#linked-pipes","title":"Linked Pipes","text":"Name URL https://github.com/linkedpipes Description Relevance"},{"location":"working/telegram/tg_data/","title":"Telegram Data","text":"<p><code>Telegram Data</code> refers to the 2 initiatives involving the telegram-related data: <code>telegram stats</code> (using <code>telemetr.io</code>) and <code>telegram messages</code> (using Telegram API).</p>"},{"location":"working/telegram/tg_data/#progress-log","title":"Progress Log","text":""},{"location":"working/telegram/tg_data/#july-2-2023","title":"July 2, 2023","text":"<ul> <li>This flow remained untouched until about 3 weeks ago, when the refactoring work started. The refactoring is carried on by @dx</li> <li>The refactoring implies fusing all the components of the flow into a single OOP-based app, where resolving of channels, collection of stats, and downloading the messages, are all implemented as methods of primary classes. FastAPI was selected as the framework for this app, with SQLAlchemy 2 as ORM. Whenever possible, methods are made <code>async</code>.</li> <li>See ARCH-A-10</li> </ul>"},{"location":"working/telegram/tg_data/#november-25-2023","title":"November 25, 2023","text":"<ul> <li>The development of the new telegram app is approaching final stages. The app is intended to run in a Docker container on our stand-alone server. </li> <li>During development, datetime fields were updated to not contain TZ information, and coerce dates to UTC as much as possible. Telegram channel's <code>status</code> field was changed to be of type <code>text</code> instead of <code>boolean</code> so that the edge-cases (such as Telemetr using an already registered internal ID) can be handled better.</li> <li>There might be issues with integrating the new app into the <code>wapatools</code> repository accounting for its structure.</li> </ul>"},{"location":"working/telegram/tg_data/#telegram-stats","title":"Telegram Stats","text":"<p>Telegram statistical data is obtained from the <code>telemetr.io</code> web service. Results are stored in the <code>telegram_channels_stats</code> table.</p>"},{"location":"working/telegram/tg_data/#available-data","title":"Available data","text":"<ul> <li>...</li> </ul>"},{"location":"working/telegram/tg_data/#primary-goal","title":"Primary goal","text":"<ul> <li>Build visualization(s) with available data.</li> <li>Make telegram collection process async</li> </ul>"},{"location":"working/telegram/tg_data/#telegram-messages","title":"Telegram Messages","text":"<p>Telegram messages are obtained by direct download via the official Telegram API, and then stored in the <code>telegram_messages</code> table in the <code>data</code> schema of the DB.</p>"},{"location":"working/telegram/tg_data/#primary-goal_1","title":"Primary goal","text":"<p>Perform various kinds of analyses, such as</p> <ul> <li>Deleted messages rate;</li> <li>Commonly used words;</li> <li>Share of reposts/forwards;</li> <li>NLP stuff;</li> <li>TBD</li> </ul>"},{"location":"working/telegram/tg_data/#secondary-goal","title":"Secondary goal","text":"<ul> <li>Identification of important messages;</li> <li>(automatic) translation into English / Ukrainian</li> </ul>"},{"location":"working/theory/v1/","title":"Theory v1","text":"<p>The core of the <code>Theory</code> section should be composed of a small body of original texts:</p> <ul> <li>The Copium Theory of Propaganda  https://shoomow.info/?p=2069&amp;preview=true (unfinished)</li> <li>Russian Corral In the Universal Informational Golfstream https://shoomow.info/?p=1555&amp;lang=en</li> <li>Geopolitical Disease &amp; The Ruskymir Variation https://shoomow.info/?p=748&amp;lang=en</li> <li>Why the Russian Evidence For Biolabs in Ukraine is Hot Garbage https://shoomow.info/?p=411&amp;lang=en</li> <li>Why Russian Referendums Cannot Be Taken Seriously https://shoomow.info/?p=2046&amp;lang=en</li> <li>TODO: Verify Assumption of Continuity https://shoomow.info/?p=1773&amp;lang=en</li> </ul> <p>All primary original texts should be located on the root level of the section (i.e. be children of <code>Theory</code>).</p> <p>Additionally, <code>Companion texts</code> subsection is required, to include various texts by other authors that illustrate or illuminate various points made in the originals. Possible companion texts are (list is not exhaustive):</p> <ul> <li>Childhood Songs (Leonid Kaganov on March 31, 2022) https://shoomow.info/?p=97&amp;lang=en</li> <li>Putin\u2019s Reich and Its Prospects (Dima Gubin, April 4, 2022) https://shoomow.info/?p=170&amp;lang=en</li> <li>I Don\u2019t Want to Believe This (Dmitry Glukhovsky, April 9, 2022) https://shoomow.info/?p=274&amp;lang=en</li> <li>Russian Guilt (Vladimir Pastukhov, April 14, 2022) https://shoomow.info/?p=366&amp;lang=en</li> <li>From Bandera to \u201cAzov\u201d: Answers to the Questions About Ukrainian Nationalism (Konstantin Skorkin, April 17, 2022) https://shoomow.info/?p=564&amp;lang=en</li> <li>Geopolitics Is the Dehumanization Of the World (Maksim Trudolyubov, April 19, 2022) https://shoomow.info/?p=664&amp;lang=en</li> <li>Putin vs. Hitler: The Difference (Dmitry Chernyshev, April 21, 2022) https://shoomow.info/?p=810&amp;lang=en</li> <li>Motherland or Truth? (Dima Gubin, April 18, 2022) https://shoomow.info/?p=682&amp;lang=en</li> <li>What Made the Propaganda Possible (Andrey Pertsev, April 2022) https://shoomow.info/?p=375&amp;lang=en</li> <li>Enter the Gloom, Find People In There: Why Do Russians Support the War? (Shura Burtin, April 24, 2022) https://shoomow.info/?p=924&amp;lang=en</li> <li>Zigzags of Journalism: How a Small Smolensk Periodical Became the Bullhorne of the War Party With a Million+ Audience (\u201cMozhem obyasnit\u201d, April 11, 2022) https://shoomow.info/?p=670&amp;lang=en</li> <li>The Cult of Victory (Alexey Roschin, April 24, 2022) https://shoomow.info/?p=1102&amp;lang=en</li> <li>Solovyov At The Crossroads (Alexey Roschin, April 28, 2022) https://shoomow.info/?p=1201&amp;lang=en</li> <li>The Fate of Germans: No Fame, No Heroes (Alexey Roschin, May 4, 2022) https://shoomow.info/?p=1373&amp;lang=en</li> <li>Russian Hysteria (Vladimir Pastukhov, May 4, 2022) https://shoomow.info/?p=1342&amp;lang=en</li> <li>What is Propaganda and How it Works (Dmitry Sidorov, May 3, 2022) https://shoomow.info/?p=1336&amp;lang=en</li> <li>A Little About Sergey Mardan (Alexey Roschin) https://shoomow.info/?p=1798&amp;lang=en</li> <li>The Kissinger Suggestion (Vladimir Pastukhov, May 26, 2022) https://shoomow.info/?p=1815&amp;lang=en</li> <li>10 \u043f\u0440\u0438\u0447\u0438\u043d, \u043f\u043e\u0447\u0435\u043c\u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u0432 \u043e\u0431\u0449\u0435\u0441\u0442\u0432\u0435 \u2013 \u043d\u0435 \u043a\u043b\u0435\u0442\u043a\u0430 \u0432 \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u043c\u0435 https://scinquisitor.livejournal.com/208371.html</li> </ul> <p>Finally, another subsection (name TBD) is required to hold texts written by patients that are also relevant to the <code>Theory</code>. Possible such texts are:</p> <ul> <li>What Russia Must Do With Ukraine (T. Sergeytsev, April 3, 2022) https://shoomow.info/?p=144&amp;lang=en</li> <li>The Coming of Russia and the New World (P. Akopov, February 26, 2022) https://shoomow.info/?p=172&amp;lang=en</li> <li>What Russia is Fighting For in Ukraine (V. Nikiforova, April 6, 2022) https://shoomow.info/?p=281&amp;lang=en</li> <li>Putin About \u201cSpecial Operation\u201d (April 12, 2022) https://shoomow.info/?p=329&amp;lang=en</li> <li>The Kind of Ukraine We Don\u2019t Need (T. Sergeytsev, April 10, 2021) https://shoomow.info/?p=251&amp;lang=en</li> <li>Joint Press Conference of A. Lukashenko &amp; V. Putin on April 12, 2022 [Transcript] https://shoomow.info/?p=788&amp;lang=en</li> <li>Putin\u2019s Direct Speech With Commentary https://shoomow.info/?p=1558&amp;preview=true</li> <li>Russian Propaganda Snapshot: March 1, 2022 https://shoomow.info/?p=2031&amp;lang=en</li> <li>Russian Propaganda Snapshot: April 2, 2022 https://shoomow.info/?p=2038&amp;lang=en</li> <li>Vladimir Ovchinsky On Death Penalty In USA https://shoomow.info/?p=2095&amp;lang=en</li> </ul> <p>All texts must have versions in English and Russian, or English and Ukrainian, or all three languages.</p>"},{"location":"working/transfactory/anomaly_cleanup_activity/","title":"Transfactory Anomaly Cleanup Activity","text":""},{"location":"working/transfactory/anomaly_cleanup_activity/#september-29-30-2023","title":"September 29-30, 2023","text":"<p>Over the course of 29 and 30 of September, 2023, the 1st run at anomalies cleanup was undertaken, working over the <code>anomalies_transcription</code> field, with threshold of 300 seconds (i.e. anomalies with lesser duration were ignored).</p> <p>The process overall was successful, although a few bits of data got lost due to errors in the alogythm.</p> <p>In total, 1024 transcripts known to contain anomalies were analyzed. Of these:</p> <ul> <li>529  were left intact, i.e. no lines were dropped due to threshold mismatch;</li> <li>495  contained anomalies longer than the established threshold</li> <li>~  207 868  junk lines were dropped from the <code>data.transcribed_content</code> table</li> <li>This corresponds to ~ 383 hours  worth of transcribed data</li> <li>For context, total number of transcribed hours is at 8472  hours, and number of records in the <code>transcribed_content</code> table after cleanup is slightly over 9 million, as of Sep 30, 2023.</li> </ul>"},{"location":"working/transfactory/anomaly_detection_summary/","title":"Transfactory Anomaly Detection Summary","text":"<p>Driven by the DVE-A-95, an update to the transfactory was introduced to address the issue on at least some of the relevant levels.</p>"},{"location":"working/transfactory/anomaly_detection_summary/#silence-trimming","title":"Silence Trimming","text":"<p>First measure taken was addition of silence trimming process to the <code>transsuply</code> script, i.e. removal of leading and trailing silence. Trimming is done with <code>ffmpeg</code>, and extends script's runtime significantly (at least, 2x); at that silence trimming is actually required in minority of cases - but as far as I can tell, <code>ffmpeg</code> cannot simply detect if trimming is needed, but can only process the file using an internal algorythm, so the trimming happens in any case (producing a new copy of the file), but if the difference in duration between the original and trimmed versions is less than 1 second, original version is uploaded instead of the trimmed one. Trimmed version is then removed in any case, while the original version is moved to cold storage as per the usual flow.</p>"},{"location":"working/transfactory/anomaly_detection_summary/#anomaly-detection-algorythm","title":"Anomaly Detection Algorythm","text":"<p>Anomaly detection is done over the array produced by the <code>shape_transcribed_lines_object</code> function, which is a part of the <code>transprocessor</code> flow - this flow is updated with anomaly detection functionality, so all the newly processed transcripts are analyzed right away.</p> <p>The algorythm consists of 2 parts, the 1st of which is covered by <code>detect_faulty_patches</code> function, and the 2nd - by <code>group_patches</code> function; umbrella function <code>detect_anomalies</code> joins them to produce the final anomaly status. All these can be found in the newly added <code>transfactory_common</code> base module, where some other functions were also moved, with corresponding code optimizations.</p> <p>The algorythm is applied to the standard <code>transprocessor</code> flow, as well as to the <code>recover_lines</code> flow.</p>"},{"location":"working/transfactory/anomaly_detection_summary/#anomalies-data-object","title":"Anomalies data object","text":"<p>The algo produces a JSON object that must have at least 1 key, <code>anomalies</code>, which is a boolean - this would be the contents of either <code>anomalies_transcription</code> or <code>anomalies_translation</code> columns.</p> <p>If it's <code>False</code>, no other keys are expected, so the object is <code>{\"anomalies\": False}</code> - this is saved to indicate that the corresponding text array was, in fact, analyzed, and not forgotten.</p> <p>If it's <code>True</code>, 2 other keys are expected - <code>raw_faulty_patches</code>, which is a list of dicts produced by <code>detect_faulty_patches</code>, and <code>operational_timeframes</code>, which is a list of tuples, each consisting of 2 floats that indicate start and end time relative to the beginning of the corresponding audiofile, produced by <code>group_patches</code>.</p>"},{"location":"working/transfactory/anomaly_detection_summary/#anomaly-stats","title":"Anomaly Stats","text":"<p>as of Sep 29, 2023</p> Records in <code>prabyss</code> table # Total 7035 Analyzed transcriptions 4839 Analyzed translations 2013 With anomalies in transcription 1024 With anomalies in translation 887 With anomalies in both 542 Transcription: total anomaly duration \\&lt;= 1 min 204/1024 Transcription: total anomaly duration btw 1 min &amp; 5 min 262/1024 Transcription: total anomaly duration btw 5 min &amp; 10 min 143/1024 Transcription: total anomaly duration btw 10 min &amp; 30 min 154/1024 Transcription: total anomaly duration btw 30 min &amp; 60 min 120/1024 Transcription: total anomaly duration btw 60 min &amp; 120 min 110/1024 Transcription: total anomaly duration &gt; 120 min 31/1024 Translation: total anomaly duration \\&lt;= 1 min 273/887 Translation: total anomaly duration btw 1 min &amp; 5 min 260/887 Translation: total anomaly duration btw 5 min &amp; 10 min 133/887 Translation: total anomaly duration btw 10 min &amp; 30 min 118/887 Translation: total anomaly duration btw 30 min &amp; 60 min 60/887 Translation: total anomaly duration btw 60 min &amp; 120 min 31/887 Translation: total anomaly duration &gt; 120 min 12/887"},{"location":"working/transfactory/anomaly_study_summary/","title":"Transfactory Anomaly Study Summary","text":""},{"location":"working/transfactory/anomaly_study_summary/#context","title":"Context","text":"<p>Transfactory is the app to process audio files into text using OpenAI's Whisper. Occasionally, the produced result (<code>.srt</code> files generated by Whisper and, subsequently, text array saved to the database) would contain <code>anomalies</code>, which (in most cases) are patches of some repeat line that go on consecutively for some time.</p>"},{"location":"working/transfactory/anomaly_study_summary/#study","title":"Study","text":"<p>A study was undertaken, to figure out possible causes of these anomalies, and ways to handle them. Study includes 15 specific usecases described in more detail here: DVE-A-94. Each usecase is a media episode where either trascription, or translation, or both contained at least 1 anomaly (identified manually). Some of the files from the test set did not contain anomalies.</p>"},{"location":"working/transfactory/anomaly_study_summary/#repeat-lines","title":"Repeat lines","text":"<ul> <li>In half of the studied cases, repeat line was something that actually was (or, at least, may have been) said;</li> <li>Some cases that may not be anomalies are template phrases, such as <code>\u0422\u041e\u0420\u0416\u0415\u0421\u0422\u0412\u0415\u041d\u041d\u0410\u042f \u041c\u0423\u0417\u042b\u041a\u0410</code>, <code>\u0413\u041e\u0412\u041e\u0420\u0418\u0422 \u041d\u0410 \u0418\u041d\u041e\u0421\u0422\u0420\u0410\u041d\u041d\u041e\u041c \u042f\u0417\u042b\u041a\u0415</code>, <code>\u041c\u0423\u0417\u042b\u041a\u0410\u041b\u042c\u041d\u0410\u042f \u0417\u0410\u0421\u0422\u0410\u0412\u041a\u0410</code> etc.</li> <li>Quite a lot of cases were lines that never occurred in the audio in question, or never could have occurred, for example, <code>TASTY DIALOGUE WITH ELENA BAZHENOVA</code> , <code>\u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0421\u0438\u043d\u0435\u0446\u043a\u0430\u044f \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430</code>, <code>\u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430</code> , <code>\u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430</code> etc. My guess is that things containing these lines were used as source material for Whisper (https://www.youtube.com/@bazhenova/), and, possibly, contained leading silence or some other artifacts.</li> </ul>"},{"location":"working/transfactory/anomaly_study_summary/#possible-causes","title":"Possible causes","text":"<p>Some possible causes are:</p> <ul> <li>prolonged silence, especially leading silence;</li> <li>prolonged non-verbal stretches (music, background noise);</li> <li>a system event occurring outside of the transfactory process but one that causes a video memory consumption spike;</li> <li>TBD.</li> </ul>"},{"location":"working/transfactory/anomaly_study_summary/#resolution-and-remaining-issues","title":"Resolution and Remaining Issues","text":"<p>To detect anomalies, a Python function was developed, and then applied to the available usecases. Of the 15 usecases 7 can be considered <code>fully resolved</code> after this, i.e. analysis of both translation and transcription produced correct and expected output.</p> <p>The rest have certain issues that can be grouped like so:</p> <ul> <li>There is actually a single long patch, but the output is a list of consecutive (and overlapping) patches that cover the same (correct) range. Text items were tested to be identical, so the reason for this split remains unclear.</li> <li>Relevant usecases: #2, #8, #13<ul> <li>With usecase #13, the cause of the split is different case of the strings which are otherwise identical</li> </ul> </li> <li>Possible solutions:<ul> <li>Post-hoc merging of produced patches into one patch;</li> <li>TBD</li> </ul> </li> <li>Instead of 1 line being repeated, it's a block of 4 lines.</li> <li>Relevant usecases: #5</li> <li>Possible solutions:<ul> <li>In addition to the repeat lines, there is another pattern related to anomalies: while normally transcribed lines have highly variable duration (difference between start and end time) that rarely repeats from one line to another, the lines in the faulty patches have relatively uniform duration of 28-30 seconds.</li> </ul> </li> <li>The algorythm correctly produces several patches (because repeat line is different), but some of them are so close to each other that it would make sense to merge them.</li> <li>Relevant usecases: #6, #9, #12</li> <li>Possible solutions:<ul> <li>Post-hoc merging of produced patches into one patch, or several non-consecutive patches;</li> </ul> </li> <li>Anomaly is a just 1 random line in the middle of otherwise proper array</li> <li>Relevant usecases: #14</li> <li>Possible solutions:<ul> <li>WTFK</li> </ul> </li> </ul>"},{"location":"working/transfactory/anomaly_study_summary/#next-steps","title":"Next steps","text":"<p>What happens once an anomaly is identified in a given factory job? Suggested flow:</p> <ul> <li>Drop faulty lines from the database</li> <li>Locate original audio file</li> <li>Cut out fragment(s) corresponding to anomaly's parameters</li> <li>Create transfactory job for each such fragment, accounting for initial job parameters.</li> <li>When the re-work job is complete, analyze the result</li> <li>try to detect anomaly, if found, compare to the original anomaly's content - if the same happens again, that should mean something</li> <li>if no new anomaly occurs, i.e. produced lines are valid, they need to be inserted into the database. This is also a challenge, since a different number of lines is to be expected - that is, <code>int_sequence</code> will be a mismatch, and would need to be reconciled.</li> </ul> <p>Alternatively, the episode could be re-processed in full. Upside: no issues with <code>int_sequence</code>. Downside: resources are wasted on processing what was already processed successfully (and might still fail the 2nd time around, btw, there's no guarantee against that); we discard totally proper lines that may have been transcribed better the 1st time.</p>"},{"location":"working/transfactory/anomaly_usecases/","title":"Transfactory Anomaly Usecases","text":"<p>This document lists some specific usecase when the transfactory's main process goes wrong, producing <code>anomalies</code>. As a rule, such anomalies take the form of some line that gets repeated over and over again. The cause of such incidents is unclear.</p>"},{"location":"working/transfactory/anomaly_usecases/#usecase-1","title":"Usecase 1","text":"Parameter Value prabyss path <code>amerikantsy-podtyorlis-gensekom-oon-efir-ot-20072023-2651641-na.mp3</code> prabyss_id 7070 transcript_id 6899 transcribed lines 1587 failed transcribed lines 1-295 translated lines 673 failed transcribed lines 1-673 Status Fully resolved <p>With transcription: problematic line is <code>\u0421\u0443\u0431\u0442\u0438\u0442\u0440\u044b \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u0410.\u0421\u0438\u043d\u0435\u0446\u043a\u0430\u044f \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430</code>; it appears under <code>int_sequence</code>s 1, 2-295 out of total 1587.</p> <p>With translation: problematic line is <code>TASTY DIALOGUE WITH ELENA BAZHENOVA</code>; it appears under 1 and goes to the end.</p>"},{"location":"working/transfactory/anomaly_usecases/#detection-status","title":"Detection status","text":"<p>With transcription: Faulty patch identified correctly, but starting with #3 due to theshold settings.</p> <p>With translation: Faulty patch of the entire length of file identified correctly</p>"},{"location":"working/transfactory/anomaly_usecases/#usecase-2","title":"Usecase 2","text":"Parameter Value prabyss path <code>amerikantsev-ustraivaet-zatyagivanie-konflikta-efir-ot-26052023-2621753-na.mp3</code> prabyss_id 7055 transcript_id 6892 transcribed lines 243 failed transcribed lines 1-243 translated lines 2197 failed transcribed lines None Status Partially resolved ~ TODO: Fix unsolicited patch split <p>With transcription: problematic line is <code>\u0420\u0410\u0417\u0413\u041e\u0412\u041e\u0420 \u041f\u041e-\u041d\u0415\u041c\u0415\u0426\u041a\u0418</code> occasionally followed by <code>\u0410\u041f\u041b\u041e\u0414\u0418\u0421\u041c\u0415\u041d\u0422\u042b</code>; it appears at the beginning at goes till the end (1-243).</p>"},{"location":"working/transfactory/anomaly_usecases/#detection-status_1","title":"Detection status","text":"<p>With transcription: Faulty patch identified overall correcly, but split into 5 different patches for unclear reason. Some of the patches are overlapping.</p> <p>With translation:</p>"},{"location":"working/transfactory/anomaly_usecases/#usecase-3","title":"Usecase 3","text":"Parameter Value prabyss path <code>Rossiya_Ukraina--Tochka-nevozvrata-i-Kaska-s-pechalnym-kontsomDmDzhangirov-v-pryamom-efire-rqbW-bH4FgA-20220203.mp3</code> prabyss_id 5145 transcript_id transcribed lines 294 failed transcribed lines 86-294 translated lines 2381 failed transcribed lines None Status Fully resolved <p>With transcription: problematic line is <code>\u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430,</code>, which start occurring after line 85 <code>\u0447\u0442\u043e\u0431 \u0435\u0435 \u0442\u0443\u0434\u0430 \u0432\u043d\u0435\u0441\u043b\u0438, \u0438 \u0442\u043e, \u0447\u0442\u043e, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430,</code>, presumable, also failed one, and goes to the end.</p>"},{"location":"working/transfactory/anomaly_usecases/#detection-status_2","title":"Detection status","text":"<p>With transcription: Faulty patch identified correctly</p> <p>With translation: Lack of anomaly identified correctly</p>"},{"location":"working/transfactory/anomaly_usecases/#usecase-4","title":"Usecase 4","text":"Parameter Value prabyss path <code>gde-prokhodit-peredovaya-efir-ot-04062023-2626712-na.mp3</code> prabyss_id transcript_id transcribed lines 3140 failed transcribed lines 2021-3140 translated lines 3415 failed transcribed lines 1347-1429 Status Fully resolved <p>With transcription: <code>\u043a\u0430\u043a\u0438\u0435 \u0442\u0435\u0430\u0442\u0440\u044b \u0431\u044b\u043b\u0438,</code></p> <p>With translation: <code>Day Z</code>,</p>"},{"location":"working/transfactory/anomaly_usecases/#detection-status_3","title":"Detection status","text":"<p>With transcription: Known faulty patch was identified correcly. Another was identified between #358 and #413.</p> <p>With translation: First faulty patch between #1347 and #1429 identified correcly. Second patch was identified between #2819 and #2919.</p>"},{"location":"working/transfactory/anomaly_usecases/#usecase-5","title":"Usecase 5","text":"Parameter Value prabyss path <code>briks-trevozhit-zapadnyy-mir-efir-ot-28062023-2640212-na.mp3</code> prabyss_id transcript_id transcribed lines 2163 failed transcribed lines None translated lines 2398 failed transcribed lines 776-895 Status Partially resolved: TODO: special usecase with the repeat block instead of repeat line <p>With translation: <code>We will continue after the news</code> over 4 lines.</p>"},{"location":"working/transfactory/anomaly_usecases/#detection-status_4","title":"Detection status","text":"<p>With transcription: A previously unidentified batch was identified between #12 and #24</p> <p>With translation: A small, previously unidentified faulty patch between #58 and #77, was identified. However, the main case was not identified, because it's not a single line being repeated, but a set of 4 lines.</p>"},{"location":"working/transfactory/anomaly_usecases/#usecase-6","title":"Usecase 6","text":"Parameter Value prabyss path <code>bronepoezd-tovarishcha-kim-chen-yna-efir-ot-17092023-2683340-na.mp3</code> prabyss_id transcript_id transcribed lines 1943 failed transcribed lines 1-321 translated lines 2296 failed transcribed lines None Status Partially resolved; TODO: merge consecutive batches, including with small intervals in between them <p>With transcription: <code>\u0412\u043e\u043b\u0433\u043e\u043b\u0430\u0439\u0444\u0437\u0435\u0442</code>, <code>\u041c\u0430\u043a\u0441\u0438\u043c \u041b\u0430\u0432\u0440\u0443\u0445\u0438\u043d</code> and a few others</p>"},{"location":"working/transfactory/anomaly_usecases/#detection-status_5","title":"Detection status","text":"<p>With transcription: Faulty patch identified almost correctly: range is fine but some lines in between are lost.</p> <p>With translation: Previously unidentified patch identified between #1264 and #1274</p>"},{"location":"working/transfactory/anomaly_usecases/#usecase-7","title":"Usecase 7","text":"Parameter Value prabyss path <code>burning-man-pochemu-tysyachi-lyudey-zastryali-v-pustyne-efir-ot-04092023-2676965-na.mp3</code> prabyss_id transcript_id transcribed lines 1112 failed transcribed lines 972-1112 translated lines 3056 failed transcribed lines None Status Fully resolved <p>With transcription: <code>\u041f\u0415\u0421\u041d\u042f \u041d\u0410 \u041d\u0415\u041c\u0415\u0426\u041a\u041e\u041c \u042f\u0417\u042b\u041a\u0415</code></p>"},{"location":"working/transfactory/anomaly_usecases/#detection-status_6","title":"Detection status","text":"<p>With transcription: faulty patch identified correctly</p> <p>With translation: lack of anomaly identified correctly</p>"},{"location":"working/transfactory/anomaly_usecases/#usecase-8","title":"Usecase 8","text":"Parameter Value prabyss path <code>chelovek-z-ya-ne-uznal-stranu_14129.mp3</code> prabyss_id transcript_id transcribed lines 489 failed transcribed lines 1-60 translated lines 1296 failed transcribed lines 816-1296 Status Partially resolved ~ TODO: Fix unsolicited patch split <p>With transcription: <code>\u041c\u0430\u0441\u0442\u0435\u0440-\u043a\u043b\u0430\u0441\u0441,</code></p> <p>With translation: <code>I</code></p>"},{"location":"working/transfactory/anomaly_usecases/#detection-status_7","title":"Detection status","text":"<p>With transcription: Faulty patch identified correcly</p> <p>With translation:  Faulty patch identified overall correcly, but split into 6 different batches for unclear reason. Some of the batches are overlapping.</p>"},{"location":"working/transfactory/anomaly_usecases/#usecase-9","title":"Usecase 9","text":"Parameter Value prabyss path <code>efir-ot-03042023-2591188-na.mp3</code> prabyss_id transcript_id transcribed lines 4303 failed transcribed lines 1068-1796, 3741-3768 (...) translated lines 4994 failed transcribed lines 497-543?, 4854-4994 Status Partially resolved; TODO: merge consecutive batches, including with small intervals in between them <p>With transcription: <code>\u042f \u0434\u0443\u043c\u0430\u044e, \u0447\u0442\u043e \u044d\u0442\u043e \u043e\u0447\u0435\u043d\u044c \u0432\u0430\u0436\u043d\u043e.</code>, <code>\u041f\u043e\u0434\u043f\u0438\u0441\u044b\u0432\u0430\u0439\u0442\u0435\u0441\u044c \u043d\u0430 \u0442\u0435\u043b\u0435\u0433\u0440\u0430\u043c-\u043a\u0430\u043d\u0430\u043b \u0410\u043d\u043d\u044b.</code>, <code>8 \u0447\u0430\u0441\u043e\u0432 8 \u043c\u0438\u043d\u0443\u0442.</code>, <code>9arenko.</code> ; <code>...</code></p> <p>With translation: <code>To be continued...</code>?, <code>Do not hurry, when you can not hurry.</code>, <code>He got it.</code></p>"},{"location":"working/transfactory/anomaly_usecases/#detection-status_8","title":"Detection status","text":"<p>With transcription: 7 fault patches were identified: 1068-1095, 1094-1121, 1120-1179, 1778-1793, 2240-2276, 2343-2364, 3739-3768, 3785-3803, all seem to be correct</p> <p>With translation: 7 faulty patches were identified: 476-492, 495-543, 2048-2097, 2105-2115, 2575-2585, 3164-3207, 4852-4869, 4871-4993, all seem to be correct.</p>"},{"location":"working/transfactory/anomaly_usecases/#usecase-10","title":"Usecase 10","text":"Parameter Value prabyss path <code>novaya-os-zla-i-skuchnyy-evroparlament-efir-ot-14092023-2682103-na.mp3</code> prabyss_id transcript_id transcribed lines 279 failed transcribed lines 4-14, 106-279 translated lines 1408 failed transcribed lines 1-5 Status Fully resolved <p>With transcription: <code>\u0412\u0440\u0430\u0447\u0435\u0431\u043d\u0430\u044f \u043f\u043e\u043c\u043e\u0449\u044c \u0440\u0430\u043d\u0435\u043d\u044b\u043c \u0431\u043e\u0439\u0446\u0430\u043c \u0441\u0438\u043b\u0430\u043c\u0438 \u043c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u0438\u0445 \u0432\u0437\u0432\u043e\u0434\u043e\u0432.</code>, <code>\u041a\u041e\u041d\u0415\u0426</code></p> <p>With translation: <code>TASTY DIALOGUE WITH ELENA BAZHENOVA</code></p>"},{"location":"working/transfactory/anomaly_usecases/#detection-status_9","title":"Detection status","text":"<p>With transcription: Both known faulty patches identified correctly.</p> <p>With translation: Known faulty patch not identified due to theshold settings; another batch identified between #258 and #276</p>"},{"location":"working/transfactory/anomaly_usecases/#usecase-11","title":"Usecase 11","text":"Parameter Value prabyss path <code>esli-bpla-zapushcheny-s-territorii-rossii-est-kogo-polovit-efir-ot-300523-2623654-na.mp3</code> prabyss_id transcript_id transcribed lines 856 failed transcribed lines 1-131, 215-856 translated lines 2907 failed transcribed lines 1-8 Status Fully resolved <p>With transcription: <code>\u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u041a\u0443\u043b\u0430\u043a\u043e\u0432\u0430</code>, <code>\u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0421\u0438\u043d\u0435\u0446\u043a\u0430\u044f \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430</code>, <code>\u0416\u0435\u043b\u0435\u0437\u043d\u0430\u044f \u043b\u043e\u0433\u0438\u043a\u0430</code></p> <p>With translation: <code>TASTY DIALOGUE WITH ELENA BAZHENOVA</code></p>"},{"location":"working/transfactory/anomaly_usecases/#detection-status_10","title":"Detection status","text":"<p>With transcription: First fault patch identified correctly with start at #4, with first 3 lines being also a repeat, but not identified due to threshold settings. Second fault patch identified correcly.</p> <p>With translation: Faulty patch not identified due to threshold settings.</p>"},{"location":"working/transfactory/anomaly_usecases/#usecase-12","title":"Usecase 12","text":"Parameter Value prabyss path <code>esli-pashinyan-otdast-karabakh-to-otvetstvennost-budet-na-bryussele-efir-ot-25052023-2621029-na_en.mp3</code> prabyss_id transcript_id transcribed lines 1620 failed transcribed lines 1-168, 1071-1165 translated lines 1718 failed transcribed lines 1-28, 780-826 Status Partially resolved; TODO: merge consecutive batches, including with small intervals in between them <p>With transcription: <code>\u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0421\u0438\u043d\u0435\u0446\u043a\u0430\u044f \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430</code>, <code>\u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430</code> , <code>\u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430</code>, <code>\u0417\u0434\u0440\u0430\u0432\u0441\u0442\u0432\u0443\u0439\u0442\u0435, \u0442\u043e\u0432\u0430\u0440\u0438\u0449\u0438 \u0433\u0432\u0430\u0440\u0434\u0435\u0439\u0446\u044b!</code>, <code>\u0416\u0435\u043b\u0435\u0437\u043d\u0430\u044f \u041b\u043e\u0433\u0438\u043a\u0430</code>, <code>\u0422\u041e\u0420\u0416\u0415\u0421\u0422\u0412\u0415\u041d\u041d\u0410\u042f \u041c\u0423\u0417\u042b\u041a\u0410</code>, <code>\u0413\u041e\u0412\u041e\u0420\u0418\u0422 \u041d\u0410 \u0418\u041d\u041e\u0421\u0422\u0420\u0410\u041d\u041d\u041e\u041c \u042f\u0417\u042b\u041a\u0415</code>, <code>\u041c\u0423\u0417\u042b\u041a\u0410\u041b\u042c\u041d\u0410\u042f \u0417\u0410\u0421\u0422\u0410\u0412\u041a\u0410</code></p> <p>With translation: <code>TASTY DIALOGUE WITH ELENA BAZHENOVA</code>, <code>We are proud of you!</code></p>"},{"location":"working/transfactory/anomaly_usecases/#detection-status_11","title":"Detection status","text":"<p>With transcription: First 2 consecutive faulty patches (up to #163) identified almost correctly: there is a small gap in between, and a few lines are not counted at the end of the 2nd patch due to threshold being set to 10. Second 2 consecutive faulty patches (between #1092 and #1165) identified almost correctly: a few lines at the beggining of the 1st are not counted due to the threshold settings.</p> <p>With translation: Both known faulty patches identified correctly; in addition, another patch identifed between #778 and #826</p>"},{"location":"working/transfactory/anomaly_usecases/#usecase-13","title":"Usecase 13","text":"Parameter Value prabyss path <code>genotsid-v-otnoshenii-sovetskogo-naroda-efir-ot-23032023-2585101-na.mp3</code> prabyss_id transcript_id transcribed lines 528 failed transcribed lines 1-528 translated lines 1896 failed transcribed lines None Status Partially resolved; TODO: take case into account? <p>With transcription: <code>\u0416\u0415\u041b\u0415\u0417\u041d\u0410\u042f \u041b\u041e\u0413\u0418\u041a\u0410</code></p>"},{"location":"working/transfactory/anomaly_usecases/#detection-status_12","title":"Detection status","text":"<p>With transcription: Two consecutive faulty patches were identified, first starting with #3 due to threshold settings, second ending at #527. Batches split is due to different case of the repeat line.</p> <p>With translation: Lack of anomaly identified correcly.</p>"},{"location":"working/transfactory/anomaly_usecases/#usecase-14","title":"Usecase 14","text":"Parameter Value prabyss path <code>triumfalnaya-pobeda-edinoy-rossii-efir-ot-11092023-2680088-na.mp3</code> prabyss_id transcript_id transcribed lines 1316 failed transcribed lines 1-134, 320 (\u043d\u0435\u0442) translated lines 4397 failed transcribed lines None Status Partially resolved; TODO: do something with the solitary line <p>With transcription: <code>\u041f\u041e\u0414\u041f\u0418\u0421\u042b\u0412\u0410\u042e\u0429\u0418\u0415\u0421\u042f \u0422\u0415\u041b\u0415\u0424\u041e\u041d\u041d\u042b\u0415 \u0421\u0418\u0413\u041d\u0410\u041b\u0418\u0417\u0410\u0426\u0418\u0418</code>, <code>\u041f\u041e\u0414\u041f\u0418\u0421\u042b\u0412\u0410\u042e\u0429\u0418\u0415 \u0421\u0418\u0413\u041d\u0410\u041b\u0418\u0417\u0410\u0426\u0418\u0418</code></p>"},{"location":"working/transfactory/anomaly_usecases/#detection-status_13","title":"Detection status","text":"<p>With transcription: 2 consecutive faulty patches identified correctly (up to #134). Single faulty item (#320) not identified</p> <p>With translation: Lack of anomaly identified correctly</p>"},{"location":"working/transfactory/anomaly_usecases/#usecase-15","title":"Usecase 15","text":"Parameter Value prabyss path <code>vsyo-vertitsya-vokrug-rossii-efir-ot-11092023-2679995-na.mp3</code> prabyss_id transcript_id transcribed lines 2023 failed transcribed lines 1-99 translated lines 717 failed transcribed lines 1-717 Status Fully resolved <p>With transcription: <code>\u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0421\u0438\u043d\u0435\u0446\u043a\u0430\u044f \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430</code>, <code>\u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u041a\u0443\u043b\u0430\u043a\u043e\u0432\u0430</code>, <code>\u0410.\u041a\u0443\u043b\u0430\u043a\u043e\u0432\u0430</code>, <code>\u0411\u043e\u043b\u0435\u0435!</code>, <code>\u0411\u044b\u0441\u0442\u0440\u0435\u0435!</code></p> <p>With translation: <code>TASTY DIALOGUE WITH ELENA BAZHENOVA</code></p>"},{"location":"working/transfactory/anomaly_usecases/#detection-status_14","title":"Detection status","text":"<p>With transcription: 4 faulty patches identified: 2-26 + 3-99; 581-593, 1052-1113, all seem to be correct</p> <p>With translation: Faulty patch identified correctly</p>"},{"location":"working/transfactory/early_plan/","title":"Speech-to-Text via openai/Whisper. A Subproject Plan","text":"<ol> <li>Set up a PostgreSQL database to store user information, job details, and the results of Whisper AI processing.</li> <li>Install and configure Supabase to provide secure file storage for uploaded files.</li> <li>Develop the web user interface for the application, allowing users to create accounts, log in, submit jobs for Whisper AI processing, and view the results of their jobs.</li> <li>Implement the API for interacting with the GPU farm, allowing the application to send jobs for processing and retrieve the results.</li> <li>Test the web application and API to ensure they are working correctly and can handle the expected workload.</li> <li>Deploy the web application and API to a server, and configure the database and file storage to ensure they are accessible to the application.</li> <li>Monitor the application and make any necessary adjustments to ensure it is running smoothly and efficiently.</li> </ol>"},{"location":"working/transfactory/node_requirements/","title":"Transfactory Node Requirements and Setup","text":"<p>OUTDATED</p>"},{"location":"working/transfactory/node_requirements/#overview","title":"Overview","text":"<p>Transfactory is a distributed application aimed at transforming audio into text using OpenAI's Whisper.</p> <p>Infrastructure-wise, it consists of 2 main entities: 1) a single Supply &amp; Processing Station (SPS), and 2) a number of Transfactory Nodes (TN), that communicate between themselves using a Postgres database. TNs perform the transcribing tasks, and SPS supplies them with new jobs and processes what's been done.</p>"},{"location":"working/transfactory/node_requirements/#transfactory-node-minimal-requirements","title":"Transfactory node minimal requirements","text":"<p>Transcribing is a very resource-demanding process as it is, and using Whisper's <code>large</code> model (default standard) makes it even more so. Below is the approximate minimally acceptable configuration.</p> <ol> <li>GPU (Graphics card): NVIDIA-based with at least 12 GB of memory and non-limited processing speed. Model line RTX 4070 would do well, for example.</li> <li>Watch out for cards that are manufactured with processing speed limitation: if they can't be used by virtual currency miners, they can't be used for machine learning, either;</li> <li>The amount of memory lesser than 12 GB won't allow us to use <code>large</code> Whisper model, and this is problematic not only because smaller models provide lesser quality, but also because in this case we would need to differentiate and account for the model used, and this is an unwanted spike in complexity.</li> <li>If you have a PC with a minimally viable GPU (or a better one), then you probably don't need any more instructions. If you're assembling a new PC, use the GPU as a starting point in some online PC-constructor (take  https://pcbuilder.net/list/ as a generic example, and keep in mind that localized variants are usually better).</li> </ol>"},{"location":"working/transfactory/node_requirements/#setting-up-transfactory-node-on-windows-machine","title":"Setting up transfactory node on Windows machine","text":"<p>This process is kinda tricky, so it's advisable to follow the following steps in the order they are provided: any digression may lead to unwanted complications. Windows 10 or 11 is assumed.</p>"},{"location":"working/transfactory/node_requirements/#preconditions","title":"Preconditions","text":"<ul> <li>An existing <code>Poetry</code>-based project</li> <li><code>whisper</code>, <code>torch</code>, <code>torchaudio</code>, <code>torchvision</code> are not installed. If they are, it's better to remove them.</li> </ul>"},{"location":"working/transfactory/node_requirements/#steps","title":"Steps","text":"<ol> <li> <p>Add <code>llvmlite</code> as explicit dependency. It's a package required by <code>pytorch</code>; when installing just the torch, it may try to use a version of this library that would cause failure. As of writing this, version <code>0.40.1</code> was most recent;</p> </li> <li> <p>Add <code>numba</code> as explicit dependency. Same story as with <code>llvmlite</code>. As of writing this, version <code>0.57.1</code> was the most recent;</p> </li> <li> <p>Add <code>koila</code> as a dependency. This is a thin wrapper over torch that resolves some of its common issues.</p> </li> <li> <p>Add <code>poethepoet</code> as a dependency. This is a task-runner for <code>Poetry</code> that can be used for many things, but here it will be used to install CUDA-based version of <code>pytorch</code>;</p> </li> <li> <p>Add <code>openai-whisper</code> package as a dependency using <code>https://github.com/openai/whisper.git</code>. If <code>llvmlite</code> and <code>numba</code> were pre-installed successfully, this is likely to go without hiccup;</p> </li> <li> <p>Add following section to <code>pyproject.toml</code>:</p> <pre><code>[tool.poe.tasks]\nforce-cuda = \"python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\"\nforce-cuda-out = \"python -m pip uninstall torch torchvision torchaudio\"\n</code></pre> <p>This adds 2 <code>Poe</code> tasks. Unlike regular dependencies, <code>Poe</code> tasks are always installed (ran) manually.</p> </li> <li> <p>Run the <code>uninstall</code> task with the following:</p> <pre><code>poetry run poe force-cuda-out\n</code></pre> <p>This would uninstall torch family if they are present in the system. If they aren't, no harm will be done.</p> </li> <li> <p>Then run the <code>install</code> task:</p> <pre><code>poetry run poe force-cuda\n</code></pre> <p>This would install the version of the torch family referenced in the <code>--index-url</code>. <code>Cu118</code> (i.e. v. 11.8) was the most recent version as of writing this. This workaround with <code>poe</code> is required because <code>Poetry</code>  can only deal with the default index (PyPI), and has not way of dealing with alternative ones.</p> </li> <li> <p>To make sure <code>torch</code> can be properly used, open a console, import <code>whisper</code> and run one of the following commands:</p> <ol> <li><code>whisper.torch.cuda.is_available()</code>. <code>True</code> is good.</li> <li><code>whisper.torch.version.cuda</code>. CUDA version (<code>11.8</code> in this case) is good.</li> <li><code>whisper.torch.version.__version__</code>. This prints out the version of <code>torch</code> used; the one including something like <code>+cu118</code> is good.</li> </ol> </li> <li> <p>Finally, in the main script, when using <code>whisper</code> or <code>torch</code>, wrap them into <code>lazy()</code> method of the <code>koila</code> library. In case of <code>whisper</code> it looks like this:</p> <pre><code>from koila import lazy\n\nmodel = lazy(whisper.load_model(name=\"medium\", device=\"cuda\", download_root=\"D:/openai\"), batch=0)\n</code></pre> <p>This should resolve the infamous <code>CUDA out of memory</code> error (more details here) If you're still getting the error while using <code>lazy()</code> method, you probably don't have enough memory (for example, this still occurs when trying to load <code>large</code> model with only 8 GB of memory in the system).</p> </li> </ol>"},{"location":"working/transfactory/stats_derivatives/","title":"Transfactory Stats Derivatives","text":"<p>Table <code>factory_jobs_run_details</code> in <code>service</code> schema contains various data related to the processing of audio files, which is collected for the purposes of anomaly detection. <code>transfactory</code> and <code>transprocessor</code> scripts are set up to add stats records during their run, but also dump the same data to a local logfile.</p>"},{"location":"working/transfactory/stats_derivatives/#collected-data","title":"Collected data","text":"<ul> <li><code>known_duration</code> INT Duration retrieved when fetching original data, i.e. returned by the source, in seconds.</li> <li><code>calculated_duration</code> INT Duration retrieved by reading the audiofile, in seconds.</li> <li><code>file_size</code> INT Size of the audiofile in bytes</li> <li><code>job_start_timestamp</code> DATETIME Timestamp for when the transcript job was started</li> <li><code>job_end_timestamp</code> DATETIME Timestamp for when the translation job was finished, or when the transcription job was finished in case translation was not run.</li> <li><code>transcription_task_duration</code> INT, in seconds.</li> <li><code>transcription_endtime</code> INT Value of <code>endtime</code> key of the last line in the transcription's <code>.srt</code> file</li> <li><code>transcribed_lines_count</code> INT Number of produced lines</li> <li><code>translation_task_duration</code> INT, in seconds.</li> <li><code>translation_endtime</code> INT Value of <code>endtime</code> key of the last line in the translation's <code>.srt</code> file</li> <li><code>translated_lines_count</code> INT Number of produced lines</li> </ul> <p>Fields <code>prabyss_id</code>, <code>transcrip_id</code>, <code>origin_id</code> and <code>media_table</code> are kept for matching purposes.</p>"},{"location":"working/transfactory/stats_derivatives/#derivatives-and-possible-anomaly-indicators","title":"Derivatives and possible anomaly indicators","text":"<ul> <li>Difference between <code>known_duration</code> and <code>calculated_duration</code></li> <li>?? Healthy value:<ul> <li>from -10 to 10</li> </ul> </li> <li>Significant difference might indicate:<ul> <li>file download was incomlete; or</li> <li>returned data is bs because parser contains some error; or</li> <li>returned data is bs because of something else.</li> </ul> </li> <li>Difference between <code>calculated_duration</code> and <code>transcription_endtime</code> / <code>translation_endtime</code>.</li> <li>Healthy value:<ul> <li>TBD</li> </ul> </li> <li>Significant difference might indicate:<ul> <li>corresponding job (<code>transcription</code> or <code>translation</code>) was botched</li> </ul> </li> <li>Ratio of <code>translated_lines_count</code> / <code>transcribed_lines_count</code> to <code>calculated_duration</code></li> <li>Healthy value:<ul> <li>TBD</li> </ul> </li> <li>Significant difference might indicate:<ul> <li>TBD</li> </ul> </li> <li>Ratio of <code>translated_lines_count</code> / <code>transcribed_lines_count</code> to <code>translation_task_duration</code> / <code>transcription_task_duration</code></li> <li>Healthy value:<ul> <li>TBD</li> </ul> </li> <li>Significant difference might indicate:<ul> <li>TBD</li> </ul> </li> </ul>"},{"location":"working/transfactory/transcript_processing_flow/","title":"Transcript Processing Flow","text":"<p>The transcript flow consists of the following parts:</p> <ol> <li>Transfactory - a script that accepts audio files as input and produces transcript files for output. It's a subproject that has to do with automatic transcription of the media episodes with <code>openai/Whisper</code>.</li> <li>Associated status: <code>uploaded</code>. Changes to status: <code>in progress</code>, <code>processed</code></li> <li><code>Transprocessor</code> - a script that accepts raw transcript files produced by the factory as input, and outputs their final storage versions.</li> <li>Associated status: <code>processed</code>. Changes to status: <code>cleanup</code></li> <li><code>Transcleanup</code> - a script that occasionally scans the working bucket and removes files that were processed.</li> <li>Associated status: <code>cleanup</code>. Changes to status: <code>finalized</code></li> <li><code>Transsupply</code> - a script that makes sure there are always enough records in the <code>prabyss</code> table with initial status.</li> </ol> <p>Communication between scripts is indirect and is achieved via the dedicated <code>prabyss</code> table. Only one script within the flow has (or should have) the permissions to remove files.</p>"},{"location":"working/transfactory/transcript_processing_flow/#flow","title":"Flow","text":"<ol> <li>The <code>transsuply</code> is responsible for adding new files to the <code>prabyss</code> bucket, and, correspondingly, new records to the <code>prabyss</code> table. After an initial batch of files (100) is uploaded, TS would occasionally query the table for number of records with <code>uploaded</code> status, and upload just enough to have 100 of them.</li> <li>The <code>transfactory</code> queries <code>prabyss</code> table and fetches one or more records with <code>uploaded</code> status. TF then uses record fields to access and download corresponding audio files, followed by the <code>whisper</code> magic. The start of the process is marked with updating record's status to <code>in progress</code>. Once done with a file, TF would upload the produced transcript file (<code>.srt</code>) and update the status of its record in the table to <code>processed</code>.</li> <li>The <code>transprocessor</code> queries <code>prabyss</code> table and fetches all records with <code>processed</code> status. TP downloads and processes the files. Once done with a record, TP updates its status to <code>cleanup</code>.</li> <li>The <code>transcleanup</code> queries <code>prabyss</code> table and fetches all records with <code>cleanup</code> status, to remove them, along with accompanying <code>.srt</code> files, from the bucket. Once done, TC updates record's status to <code>finalized</code>.</li> </ol>"}]}