{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"apps/","text":"Application Registry :::info This is a comprehensive list of internal applications used in the Wapaganda project ::: Structure of an Applicaiton Registry Record (ARR) :::info Application Registry concept (and, correspondingly, ARRs) are inspired by Architecture Decision Recods (ADRs) ::: Application Registry Record is a document that describes an application that constitutes a part of a larger project. Each such document must contain 4 sections: General In this section, describe general purpose and point of the application. This is a place to share broader context if necessary. Code In this section, describe relevant repositories/modules, any nuances related to the code implementation. Entities In this section, describe what entities are involved, and how they are connected. Flow Diagram In this section, put flow diagram that describes how the application works. It could be an image, but recommended approach is Mermaid, where either flow or block diagram types could be used. (Note that block type is in beta, and isn't supported on most platforms) Applications ARR-3: Transfactory Web Database Search ARR-1: Ave Media ARR-2: Periodicals SyncManager Relevance service Queue service ad hoc Transsuply2 Cooperation EntryBot ARR-4: Tgram Internal Review Voice Collection","title":"Applications"},{"location":"apps/#application-registry","text":":::info This is a comprehensive list of internal applications used in the Wapaganda project :::","title":"Application Registry"},{"location":"apps/#structure-of-an-applicaiton-registry-record-arr","text":":::info Application Registry concept (and, correspondingly, ARRs) are inspired by Architecture Decision Recods (ADRs) ::: Application Registry Record is a document that describes an application that constitutes a part of a larger project. Each such document must contain 4 sections: General In this section, describe general purpose and point of the application. This is a place to share broader context if necessary. Code In this section, describe relevant repositories/modules, any nuances related to the code implementation. Entities In this section, describe what entities are involved, and how they are connected. Flow Diagram In this section, put flow diagram that describes how the application works. It could be an image, but recommended approach is Mermaid, where either flow or block diagram types could be used. (Note that block type is in beta, and isn't supported on most platforms)","title":"Structure of an Applicaiton Registry Record (ARR)"},{"location":"apps/#applications","text":"ARR-3: Transfactory Web Database Search ARR-1: Ave Media ARR-2: Periodicals SyncManager Relevance service Queue service ad hoc Transsuply2 Cooperation EntryBot ARR-4: Tgram Internal Review Voice Collection","title":"Applications"},{"location":"architecture/","text":"About ADRs Architecture Decision Record is a document that describes a particular decision or solution related to the fundamental functionality of the product. Each such document must contain 4 sections: Context In this section, describe the relevant circumstances, be they of technical or general nature, to answer the question why the change is/was necessary. Decision In this section, describe what was implemented, and how. Be as specific as possible: this should be a comprehesive statement of a technical solution. Status This should be just a single word or expression indicating current status of the document. Possible values: accepted , in discussion , succeeded by . Consequences In this section, describe briefly expected or actualized consequences of the decision. Negative consequences should be described in detail; if there are no negative ones, this should be mentioned. Aside from this 4-section structure, the shape of the information remains up to the author. Legend These symbols should be used in the title of an ADR document as a quick indication of its status. Symbol Meaning \u221e Cooking (in progress, in discussion) \ud83d\uddf6\u227d Deprecated in favor of other ADR \ud83d\uddf6 Deprecated \ud83d\uddf8 Accepted (active) Accepted & Active ADR-27 describes current structure of the project. ADR-25 describes Infisical-based secrets management within the project. ADR-10 describes the standard of personal images handling. ADR-17 describes nuances of the ISCO-08 adoption. ADR-19 describes nuances of a classification approach to organizations. ADR-21 describes automatic provenance mechanism. ADR-22 describes implementation of the custom database type. ADR-23 describes general approach to maintaining the project in 3 languages. ADR-18 describes JavaScript coding standards. ADR-30 describes approach and implementation of the website's Theory section. ADR-24 addresses nuances of subprojects' dockerization in polylith architechture. ADR-33 describes nuances of the private Docker repository setup. In Progress | Under Revision | Planned ADR-20 describes technical details of the Ruskymir Creed implementation. ADR-7 ==would== describe detailed view for an instance of a media episode. ADR-8 ==would== describe the mechanism of automatic mark-up of transcribed texts. ADR-13 ==would== describe theory and reasoning for the general approach to media segments. ADR-14 describes approach to handling printed data, such as books and articles. ADR-29 ==would== accumulate nuances related to non-DB data storage. ADR-31 ==would== describe centralized logging with Logfire. ADR-32 ==would== describe the set up and process for backing up the database with pgbackrest . Abandoned | Rejected | Deprecated ADR-28 describes the previous approach to backing up database, one based on JSON structured export. ADR-1 describes the original set up of the project. Superceded with ADR-12 . ADR-12 describes 2nd stage in the project's evolution. Superceded with ADR-27 . ADR-5 Deprecated due to removal of MongoDB from the stack. ADR-11 was about database summary stats. Rejected because not ADR material (moved elsewhere). ADR-15 was an attempt to collect common terms. Rejected because not ADR material (moved elsewhere). ADR-2 Used to describe how secrets are managed with the use of Doppler. Superceded with ADR-25 . ADR-6 aimed to describe approach to transcripts, but got outdated before it was finished. Superceded with ADR-26 . ADR-3 attempted to impose a desired git flow on the project. Rejected. ADR-9 aimed to describe localization approach, but got outdated before it was finished. Superceded with ADR-23 . ADR-4 describes the web project's Django-based REST API. This was deprecated in favor of the new FastAPI-based backend. ADR-16 would've described the point and approach behind the public Theory section, but got outdated before it was finished. Superceded with ADR-30 . ADR-26 was dedicated to Transfactory application before Application registry was created. ADR-36 was dedicated to Periodicals application before Application registry was created. ADR-37 was dedicated to Tgram application before Application registry was created. ADR-38 was dedicated to Ave Media application before Application registry was created.","title":"ADRs"},{"location":"architecture/#about-adrs","text":"Architecture Decision Record is a document that describes a particular decision or solution related to the fundamental functionality of the product. Each such document must contain 4 sections: Context In this section, describe the relevant circumstances, be they of technical or general nature, to answer the question why the change is/was necessary. Decision In this section, describe what was implemented, and how. Be as specific as possible: this should be a comprehesive statement of a technical solution. Status This should be just a single word or expression indicating current status of the document. Possible values: accepted , in discussion , succeeded by . Consequences In this section, describe briefly expected or actualized consequences of the decision. Negative consequences should be described in detail; if there are no negative ones, this should be mentioned. Aside from this 4-section structure, the shape of the information remains up to the author.","title":"About ADRs"},{"location":"architecture/#legend","text":"These symbols should be used in the title of an ADR document as a quick indication of its status. Symbol Meaning \u221e Cooking (in progress, in discussion) \ud83d\uddf6\u227d Deprecated in favor of other ADR \ud83d\uddf6 Deprecated \ud83d\uddf8 Accepted (active)","title":"Legend"},{"location":"architecture/#accepted-active","text":"ADR-27 describes current structure of the project. ADR-25 describes Infisical-based secrets management within the project. ADR-10 describes the standard of personal images handling. ADR-17 describes nuances of the ISCO-08 adoption. ADR-19 describes nuances of a classification approach to organizations. ADR-21 describes automatic provenance mechanism. ADR-22 describes implementation of the custom database type. ADR-23 describes general approach to maintaining the project in 3 languages. ADR-18 describes JavaScript coding standards. ADR-30 describes approach and implementation of the website's Theory section. ADR-24 addresses nuances of subprojects' dockerization in polylith architechture. ADR-33 describes nuances of the private Docker repository setup.","title":"Accepted &amp; Active"},{"location":"architecture/#in-progress-under-revision-planned","text":"ADR-20 describes technical details of the Ruskymir Creed implementation. ADR-7 ==would== describe detailed view for an instance of a media episode. ADR-8 ==would== describe the mechanism of automatic mark-up of transcribed texts. ADR-13 ==would== describe theory and reasoning for the general approach to media segments. ADR-14 describes approach to handling printed data, such as books and articles. ADR-29 ==would== accumulate nuances related to non-DB data storage. ADR-31 ==would== describe centralized logging with Logfire. ADR-32 ==would== describe the set up and process for backing up the database with pgbackrest .","title":"In Progress | Under Revision | Planned"},{"location":"architecture/#abandoned-rejected-deprecated","text":"ADR-28 describes the previous approach to backing up database, one based on JSON structured export. ADR-1 describes the original set up of the project. Superceded with ADR-12 . ADR-12 describes 2nd stage in the project's evolution. Superceded with ADR-27 . ADR-5 Deprecated due to removal of MongoDB from the stack. ADR-11 was about database summary stats. Rejected because not ADR material (moved elsewhere). ADR-15 was an attempt to collect common terms. Rejected because not ADR material (moved elsewhere). ADR-2 Used to describe how secrets are managed with the use of Doppler. Superceded with ADR-25 . ADR-6 aimed to describe approach to transcripts, but got outdated before it was finished. Superceded with ADR-26 . ADR-3 attempted to impose a desired git flow on the project. Rejected. ADR-9 aimed to describe localization approach, but got outdated before it was finished. Superceded with ADR-23 . ADR-4 describes the web project's Django-based REST API. This was deprecated in favor of the new FastAPI-based backend. ADR-16 would've described the point and approach behind the public Theory section, but got outdated before it was finished. Superceded with ADR-30 . ADR-26 was dedicated to Transfactory application before Application registry was created. ADR-36 was dedicated to Periodicals application before Application registry was created. ADR-37 was dedicated to Tgram application before Application registry was created. ADR-38 was dedicated to Ave Media application before Application registry was created.","title":"Abandoned | Rejected | Deprecated"},{"location":"db/","text":"Basics The master database of the Wapaganda project is == PostgreSQL ==. Current version: == 16 ==. Extensions The following extensions are installed: pgroonga . Support for full-text search, based on the Groonga project . pg_cron . Running scheduled jobs inside the DB pg_prewarm. Not really used. Supposed to speed up the db. plpgsql. Procedural language. Previously, default PL for complex functions. plpython3u. Procedural language. Current default PL for complex functions. postgres_fdw . Foreign wrapper allowing inter-database operations. See below for installation details. Documentation As Comments Documentation on the database is maintained in the form of comments to the tables and fields. All relevant and non-obvious columns were commented on. General DB flow now includes writing a comment on any such relevant column when adding it. Comments can be added in a standard PostgreSQL fashion . Comments can be retrieved by running a stored function get_column_comment(table_name text, table_column text) . Full-text search v2 Full-text search is implemented on the database directly using pgroonga extension. pgroonga indices were created for the following tables: Table Fields Index name Search func people fullname pgroonga_people_fullname - search_in_people_skim - search_in_people public.organizations name , short_name pgroonga_orgs_name_shortname - search_in_orgs public.youtube_vids title , description pgroonga_youtube_vids_title_desc_index search_in_youtube_vids public.komso_episodes title , description pgroonga_komso_vids_title_desc_index search_in_komso_episodes public.ntv_episodes title , description pgroonga_ntv_episodes_title_desc_index search_in_ntv_episodes public.smotrim_episodes title , description pgroonga_smotrim_episodes_title_desc_index search_in_smotrim_episodes data.transcribed_content content pgroonga_transcribed_content_index search_in_transcribed data.printed_content raw_content pgroonga_printed_content_index search_in_printed data.text_media title , excerpt author_data content pgroonga_text_media_title_excerpt_index pgroonga_text_media_author_data_index pgroonga_text_media_content_index search_in_text_media enums.isco08_taxonomy term pgroonga_isco08_taxonomy_term_index search_in_isco8 enums.isco08_index name pgroonga_isco08_index_name_index search_in_isco8_index enums.orgs_taxonomy term pgroonga_orgs_taxonomy_term_index search_in_orgs_taxonomy enums.rucr_taxonomy content pgroonga_rucr_taxonomy_content_index search_in_rucr_taxonomy data.telegram_messages content pgroonga_telegram_messages_content Each of the search functions takes a string as the only argument, and returns a table with results. Naming policy: every pgroonga index name must start with word pgroonga An index can be created with the following query: sql create index pgroonga_table_name_index on table_name using pgroonga(column1, column2); * Existing indices could be viewed by running the following query: sql SELECT indexname, tablename FROM pg_indexes WHERE indexname LIKE 'pgroonga%' ORDER BY tablename, indexname; Triple language format Used to refer to dictionary (aka JSON)-like format with set keys en , ru and uk until it was converted into a costom composite type saved under name triple_lang in December 2023: create type triple_lang as ( en text, ru text, uk text ); Custom type required some adaptations to the codebase. ==TBD== Database Queue Queuing mechanism was established as a way of resolving telegram_messages pgroonga index. It allows running heavy queries in the background, independent of any client applications. Table job_queue was created in schema service with following columns: id command : SQL command to run status : current job's status - one of queued , processing , completed or failed added_on started_on finished_on failed_on fail_details - text of the error Function enqueue_job(command_text::TEXT) was created to add jobs to the table sql SELECT enqueue_job('INSERT INTO some_table (column1, column2) VALUES(value1, value2)') * Function process_job() was created with plpython3u to dequeue a job and process it, changing status along the way. * A cron job was scheduled with pg_cron to run process_job() every hour. Secondary databases The following secondary databases are maintained on the same instance as wapadb : meta is for storing important secondary data, specifically, logs and provenance records. ==DEPRECATED== logs table in public schema is for saving potentially important logs from all related applications. logs contains log data, including the timestamp ( emitted_at ), log_level , log_message , application from where the log message originates, and context_data with additional information as the case may be. provenance table in public schema is for storing data generated by the provenance flow. . to enable saving wapadb 's tables' data, a foreign table was created that mirrors the meta.provenance in service schema. windmill holds data of the Windmill app. The schema and content are maintained by Windmill; Foreign Tables By default, Postgres does not allow inter-database communication, but it can be enabled with foreign wrappers. To make it so that meta and wapadb databases could exchange data, postgres_fwd extention was installed. Here's the general flow for this: -- install extention CREATE EXTENTION postgres_fwd; -- create server CREATE SERVER meta_server FOREIGN DATA WRAPPER postgres_fdw OPTIONS (host 'localhost', port '6002', dbname 'meta'); -- create user mapping CREATE USER MAPPIN FOR warp SERVER meta_server OPTIONS (user 'warp', password 'long_and_windy_password'); -- given that a table to be mirrored already exists IMPORT FOREIGN SCHEMA source_schema_ie_public LIMIT TO (target_table1) FROM SERVER meta_server INTO target_schema_ie_service; -- sequence for the foreign table might not get created automatically, so CREATE SEQUENCE target_schema_ie_service_target_table1_sequence_id START 1; After this you can work with target_schema_ie_service.target_table1 as if it was a regular table. Installation Database ==TBD== Updating DB SSL certificate ==TBD== openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout /etc/ssl/private/ssl-cert-snakeoil.key -out /etc/ssl/certs/ssl-cert-snakeoil.pem pgroonga Starting page on the installation in general. For installing on Ubuntu-based service, follow How to install for the official PostgreSQL section on this page . Copy of the instruction: $ sudo apt install -y software-properties-common $ sudo add-apt-repository -y universe $ sudo add-apt-repository -y ppa:groonga/ppa $ sudo apt install -y wget lsb-release $ wget https://packages.groonga.org/ubuntu/groonga-apt-source-latest-$(lsb_release --codename --short).deb $ sudo apt install -y -V ./groonga-apt-source-latest-$(lsb_release --codename --short).deb $ echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release --codename --short)-pgdg main\" | sudo tee /etc/apt/sources.list.d/pgdg.list $ wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - $ sudo apt update $ sudo apt install -y -V postgresql-15-pgdg-pgroonga # update to actual version of Postgres CREATE EXTENSION pgroonga; pg_cron Official page . sudo apt-get -y install postgresql-15-cron # update to actual version of Postgres Update DB configuration ( postgresql.conf ): shared_preload_libraries = 'pg_cron' cron.database_name = 'postgres' # set to actual name of the db cron.timezone = 'PRC' # optionally set the TZ CREATE EXTENSION pg_cron pg_prewarm TBD plpython3u sudo apt-get update sudo apt-get install postgresql-server-dev-all sudo apt-get install python3 python3-dev sudo apt-get install postgresql-plpython3 CREATE EXTENSION plpython3u;","title":"General Guide"},{"location":"db/#basics","text":"The master database of the Wapaganda project is == PostgreSQL ==. Current version: == 16 ==.","title":"Basics"},{"location":"db/#extensions","text":"The following extensions are installed: pgroonga . Support for full-text search, based on the Groonga project . pg_cron . Running scheduled jobs inside the DB pg_prewarm. Not really used. Supposed to speed up the db. plpgsql. Procedural language. Previously, default PL for complex functions. plpython3u. Procedural language. Current default PL for complex functions. postgres_fdw . Foreign wrapper allowing inter-database operations. See below for installation details.","title":"Extensions"},{"location":"db/#documentation-as-comments","text":"Documentation on the database is maintained in the form of comments to the tables and fields. All relevant and non-obvious columns were commented on. General DB flow now includes writing a comment on any such relevant column when adding it. Comments can be added in a standard PostgreSQL fashion . Comments can be retrieved by running a stored function get_column_comment(table_name text, table_column text) .","title":"Documentation As Comments"},{"location":"db/#full-text-search-v2","text":"Full-text search is implemented on the database directly using pgroonga extension. pgroonga indices were created for the following tables: Table Fields Index name Search func people fullname pgroonga_people_fullname - search_in_people_skim - search_in_people public.organizations name , short_name pgroonga_orgs_name_shortname - search_in_orgs public.youtube_vids title , description pgroonga_youtube_vids_title_desc_index search_in_youtube_vids public.komso_episodes title , description pgroonga_komso_vids_title_desc_index search_in_komso_episodes public.ntv_episodes title , description pgroonga_ntv_episodes_title_desc_index search_in_ntv_episodes public.smotrim_episodes title , description pgroonga_smotrim_episodes_title_desc_index search_in_smotrim_episodes data.transcribed_content content pgroonga_transcribed_content_index search_in_transcribed data.printed_content raw_content pgroonga_printed_content_index search_in_printed data.text_media title , excerpt author_data content pgroonga_text_media_title_excerpt_index pgroonga_text_media_author_data_index pgroonga_text_media_content_index search_in_text_media enums.isco08_taxonomy term pgroonga_isco08_taxonomy_term_index search_in_isco8 enums.isco08_index name pgroonga_isco08_index_name_index search_in_isco8_index enums.orgs_taxonomy term pgroonga_orgs_taxonomy_term_index search_in_orgs_taxonomy enums.rucr_taxonomy content pgroonga_rucr_taxonomy_content_index search_in_rucr_taxonomy data.telegram_messages content pgroonga_telegram_messages_content Each of the search functions takes a string as the only argument, and returns a table with results. Naming policy: every pgroonga index name must start with word pgroonga An index can be created with the following query: sql create index pgroonga_table_name_index on table_name using pgroonga(column1, column2); * Existing indices could be viewed by running the following query: sql SELECT indexname, tablename FROM pg_indexes WHERE indexname LIKE 'pgroonga%' ORDER BY tablename, indexname;","title":"Full-text search v2"},{"location":"db/#triple-language-format","text":"Used to refer to dictionary (aka JSON)-like format with set keys en , ru and uk until it was converted into a costom composite type saved under name triple_lang in December 2023: create type triple_lang as ( en text, ru text, uk text ); Custom type required some adaptations to the codebase. ==TBD==","title":"Triple language format"},{"location":"db/#database-queue","text":"Queuing mechanism was established as a way of resolving telegram_messages pgroonga index. It allows running heavy queries in the background, independent of any client applications. Table job_queue was created in schema service with following columns: id command : SQL command to run status : current job's status - one of queued , processing , completed or failed added_on started_on finished_on failed_on fail_details - text of the error Function enqueue_job(command_text::TEXT) was created to add jobs to the table sql SELECT enqueue_job('INSERT INTO some_table (column1, column2) VALUES(value1, value2)') * Function process_job() was created with plpython3u to dequeue a job and process it, changing status along the way. * A cron job was scheduled with pg_cron to run process_job() every hour.","title":"Database Queue"},{"location":"db/#secondary-databases","text":"The following secondary databases are maintained on the same instance as wapadb : meta is for storing important secondary data, specifically, logs and provenance records. ==DEPRECATED== logs table in public schema is for saving potentially important logs from all related applications. logs contains log data, including the timestamp ( emitted_at ), log_level , log_message , application from where the log message originates, and context_data with additional information as the case may be. provenance table in public schema is for storing data generated by the provenance flow. . to enable saving wapadb 's tables' data, a foreign table was created that mirrors the meta.provenance in service schema. windmill holds data of the Windmill app. The schema and content are maintained by Windmill;","title":"Secondary databases"},{"location":"db/#foreign-tables","text":"By default, Postgres does not allow inter-database communication, but it can be enabled with foreign wrappers. To make it so that meta and wapadb databases could exchange data, postgres_fwd extention was installed. Here's the general flow for this: -- install extention CREATE EXTENTION postgres_fwd; -- create server CREATE SERVER meta_server FOREIGN DATA WRAPPER postgres_fdw OPTIONS (host 'localhost', port '6002', dbname 'meta'); -- create user mapping CREATE USER MAPPIN FOR warp SERVER meta_server OPTIONS (user 'warp', password 'long_and_windy_password'); -- given that a table to be mirrored already exists IMPORT FOREIGN SCHEMA source_schema_ie_public LIMIT TO (target_table1) FROM SERVER meta_server INTO target_schema_ie_service; -- sequence for the foreign table might not get created automatically, so CREATE SEQUENCE target_schema_ie_service_target_table1_sequence_id START 1; After this you can work with target_schema_ie_service.target_table1 as if it was a regular table.","title":"Foreign Tables"},{"location":"db/#installation","text":"","title":"Installation"},{"location":"db/#database","text":"==TBD==","title":"Database"},{"location":"db/#updating-db-ssl-certificate","text":"==TBD== openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout /etc/ssl/private/ssl-cert-snakeoil.key -out /etc/ssl/certs/ssl-cert-snakeoil.pem","title":"Updating DB SSL certificate"},{"location":"db/#pgroonga","text":"Starting page on the installation in general. For installing on Ubuntu-based service, follow How to install for the official PostgreSQL section on this page . Copy of the instruction: $ sudo apt install -y software-properties-common $ sudo add-apt-repository -y universe $ sudo add-apt-repository -y ppa:groonga/ppa $ sudo apt install -y wget lsb-release $ wget https://packages.groonga.org/ubuntu/groonga-apt-source-latest-$(lsb_release --codename --short).deb $ sudo apt install -y -V ./groonga-apt-source-latest-$(lsb_release --codename --short).deb $ echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release --codename --short)-pgdg main\" | sudo tee /etc/apt/sources.list.d/pgdg.list $ wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - $ sudo apt update $ sudo apt install -y -V postgresql-15-pgdg-pgroonga # update to actual version of Postgres CREATE EXTENSION pgroonga;","title":"pgroonga"},{"location":"db/#pg_cron","text":"Official page . sudo apt-get -y install postgresql-15-cron # update to actual version of Postgres Update DB configuration ( postgresql.conf ): shared_preload_libraries = 'pg_cron' cron.database_name = 'postgres' # set to actual name of the db cron.timezone = 'PRC' # optionally set the TZ CREATE EXTENSION pg_cron","title":"pg_cron"},{"location":"db/#pg_prewarm","text":"TBD","title":"pg_prewarm"},{"location":"db/#plpython3u","text":"sudo apt-get update sudo apt-get install postgresql-server-dev-all sudo apt-get install python3 python3-dev sudo apt-get install postgresql-plpython3 CREATE EXTENSION plpython3u;","title":"plpython3u"},{"location":"infra/","text":"In October 2023 the infra setup was changed completely: 1) Supabase was dropped as DB provider in favor of self-hosted PostgreSQL database (Hetzner); 2) PostgreSQL instance was setup on Railway to be used as dev DB; 3) MinIO was installed on the same Hetzner server to operate over the storage (100 Gb). Hetzner TBD MinIO mc tool Install: curl https://dl.min.io/client/mc/release/linux-amd64/mc \\ --create-dirs \\ -o /etc/minio/mc chmod +x /etc/minio/mc export PATH=$PATH:/etc/minio/ mc --help Set alias: mc alias set ALIAS HOSTNAME ACCESS_KEY SECRET_KEY mc admin info myminio","title":"Infrastructure"},{"location":"infra/#hetzner","text":"TBD","title":"Hetzner"},{"location":"infra/#minio","text":"mc tool Install: curl https://dl.min.io/client/mc/release/linux-amd64/mc \\ --create-dirs \\ -o /etc/minio/mc chmod +x /etc/minio/mc export PATH=$PATH:/etc/minio/ mc --help Set alias: mc alias set ALIAS HOSTNAME ACCESS_KEY SECRET_KEY mc admin info myminio","title":"MinIO"},{"location":"todo/","text":"Outstanding challenges","title":"Outstanding challenges"},{"location":"todo/#outstanding-challenges","text":"","title":"Outstanding challenges"},{"location":"active/general_data/","text":"General Data refers to a range of flows and processes around adding new personalities and other information to the database Primary goal Formalization of process, reduction of manual parts, more automation. Progress Log Feb 1, 2023 Repository wapaganda_tools was compiled to have all the relevant scripts in one place. update-smotrim and update-youtube flows were developed, along with transcript , pickle-backup , insert relationships , add person with photo , and upsert photo flows to handle recurring tasks. Already existing telegram-messages , telegram-stats resolve telegram and resolve youtube flows were improved and updated. In addition, scripts for downloading media data from smotrim and youtube were added, too. Tools were isolated into 2 modules, utils and common , each with a number of submodules for various tasks. Every function producing a list of anything was supplemented with a decorator pickle-backup , which is a part of the flow of the same name. All relevant scripts were updated to write logs. March 19, 2023 GitHub organization was created, and 3 repositories were moved there, including wapaganda_tools @atatatko's scripts were added to the utils module; Internal modules were re-organized and functions were documented; Flow for adding new patients was improved to work over a list of new objects; komso episodes are now taken from the komsomolka website, which offers better structure and higher bandwidth; Many new smotrim segments were added, including purely audio-based; Youtube update flow was improved to start with segments; Download scripts were generally improved; sql folder was created to store SQL code for functions, views and certain other actions. July 2, 2023 Yurii Cherkasov 's log_helper was fully integrated into the codebase; Full text search in the DB was implemented with pgroonga against multiple tables, including people , ogranizations , etc; Theory module was added, including corresponding section on the website, DB infrastructure, and Python scripts for adding and resolving new texts, taking into advantage newly developed OOP web parsing module; Edit markdown flow was implemented to retrieve texts stored in the DB and upload the updated version; Scripts, lookup_data , lookup_image and lookup_org_data were implemented to retrieve corresponding info and display it for informational purposes; The script for retrieval and update of Meduza's daily news stream was developed; Supabase API was completely removed from the codebase (99%). Instead, the code now relies on more generic psycopg , requests and requests_html libraries; New media flows were added, for DenTV , NTV and Rodniki ; Existing media flows were refactored and improved; OpenAI API was partially integrated into the code, for purposes of text translation. More purposes should be identified; An algorithm for initial parsing/treatment of EPUB book files was developed; Development of the ontology for the project has begun, which includes adoption of the existing tech stack and data storage to the OWL framework, creation and elaboration of classes, relationships etc., as well as inclusion of existing ontologies and taxonomies. ISCO08 taxonomy (professions) was saved to the database, with a view on setting strict order to the patient -to- organization relationship. November 25, 2023 Database was moved off Supabase to standalone server for good. A few things fell off during the transition, and were all fixed. A flow for text sources was developed with the use of factory programming pattern. Still WIP. Lookup scripts were all integrated into one CLI tool. Logging was improved in a number of way, including structured logging, context-manager based logging, and saving logs into the dedicated db. Railway DB instance was set up and \"migrated to\" with the use of Snaplet. Screenshot-monitor script was set up to automatically parse certain data from captured screenshots. Windmill was introduced into the tool-stack to handle internal flows and apps. Databases were created for logging ( logs ), to support the tool framework ( tool_framework ), and for experimenting ( unrelated ) Left to do [ ] Add all currently unintegrated text sources. [ ] Finalize auto translation flow. [ ] Work on the books-related scripts. [ ] Work with RUCR and other ontologies.","title":"Active Initiatives. General Data"},{"location":"active/general_data/#primary-goal","text":"Formalization of process, reduction of manual parts, more automation.","title":"Primary goal"},{"location":"active/general_data/#progress-log","text":"","title":"Progress Log"},{"location":"active/general_data/#feb-1-2023","text":"Repository wapaganda_tools was compiled to have all the relevant scripts in one place. update-smotrim and update-youtube flows were developed, along with transcript , pickle-backup , insert relationships , add person with photo , and upsert photo flows to handle recurring tasks. Already existing telegram-messages , telegram-stats resolve telegram and resolve youtube flows were improved and updated. In addition, scripts for downloading media data from smotrim and youtube were added, too. Tools were isolated into 2 modules, utils and common , each with a number of submodules for various tasks. Every function producing a list of anything was supplemented with a decorator pickle-backup , which is a part of the flow of the same name. All relevant scripts were updated to write logs.","title":"Feb 1, 2023"},{"location":"active/general_data/#march-19-2023","text":"GitHub organization was created, and 3 repositories were moved there, including wapaganda_tools @atatatko's scripts were added to the utils module; Internal modules were re-organized and functions were documented; Flow for adding new patients was improved to work over a list of new objects; komso episodes are now taken from the komsomolka website, which offers better structure and higher bandwidth; Many new smotrim segments were added, including purely audio-based; Youtube update flow was improved to start with segments; Download scripts were generally improved; sql folder was created to store SQL code for functions, views and certain other actions.","title":"March 19, 2023"},{"location":"active/general_data/#july-2-2023","text":"Yurii Cherkasov 's log_helper was fully integrated into the codebase; Full text search in the DB was implemented with pgroonga against multiple tables, including people , ogranizations , etc; Theory module was added, including corresponding section on the website, DB infrastructure, and Python scripts for adding and resolving new texts, taking into advantage newly developed OOP web parsing module; Edit markdown flow was implemented to retrieve texts stored in the DB and upload the updated version; Scripts, lookup_data , lookup_image and lookup_org_data were implemented to retrieve corresponding info and display it for informational purposes; The script for retrieval and update of Meduza's daily news stream was developed; Supabase API was completely removed from the codebase (99%). Instead, the code now relies on more generic psycopg , requests and requests_html libraries; New media flows were added, for DenTV , NTV and Rodniki ; Existing media flows were refactored and improved; OpenAI API was partially integrated into the code, for purposes of text translation. More purposes should be identified; An algorithm for initial parsing/treatment of EPUB book files was developed; Development of the ontology for the project has begun, which includes adoption of the existing tech stack and data storage to the OWL framework, creation and elaboration of classes, relationships etc., as well as inclusion of existing ontologies and taxonomies. ISCO08 taxonomy (professions) was saved to the database, with a view on setting strict order to the patient -to- organization relationship.","title":"July 2, 2023"},{"location":"active/general_data/#november-25-2023","text":"Database was moved off Supabase to standalone server for good. A few things fell off during the transition, and were all fixed. A flow for text sources was developed with the use of factory programming pattern. Still WIP. Lookup scripts were all integrated into one CLI tool. Logging was improved in a number of way, including structured logging, context-manager based logging, and saving logs into the dedicated db. Railway DB instance was set up and \"migrated to\" with the use of Snaplet. Screenshot-monitor script was set up to automatically parse certain data from captured screenshots. Windmill was introduced into the tool-stack to handle internal flows and apps. Databases were created for logging ( logs ), to support the tool framework ( tool_framework ), and for experimenting ( unrelated )","title":"November 25, 2023"},{"location":"active/general_data/#left-to-do","text":"[ ] Add all currently unintegrated text sources. [ ] Finalize auto translation flow. [ ] Work on the books-related scripts. [ ] Work with RUCR and other ontologies.","title":"Left to do"},{"location":"active/transcript_factory/","text":"Transcript Factory refers to the mass production of transcripts of the relevant media episodes using openai/Whisper . Progress Log July 2, 2023 Box was ousted as the storage component quite a long time ago. Instead, Supabase storage is used. Supabase API was removed as a dependency in favor of psycopg library (DB connection) and requests (storage operations) Note: using Supabase API for storage connection may have been the cause of some downloading issues on the Transfactory node side. The scripts underwent a number of adjustments and improvements, but the flow essentially remained unchanged since the moment it was established. November 25, 2023 The flow underwent a major refactoring: Transprocessor was updated with a view at running it in a Docker container on server (WIP). Clean-up functionality was integrated into the processor to simplify the setup (except for the resulting .srt files, for which an expiration policy was set at 1 week as a safety fallback measure). Transcleaner was deprecated due to its functionality being merged into the processor. Transsuply wasn't changed much on the code level, but the decision was made to run it no more than once a week, uploading 50-60 Gb of audiofiles in one go. Factory node script is undergoing the process of refactoring into a CLI. In addition, minio client is now being used to communicate with storage and not requests . Goals [ ] Finish refactoring Factory node script into a CLI [ ] Refactor transsuply into a CLI [ ] Spin up Docker container with Transprocessor on server.","title":"Active Initiatives. Transcript Factory"},{"location":"active/transcript_factory/#progress-log","text":"","title":"Progress Log"},{"location":"active/transcript_factory/#july-2-2023","text":"Box was ousted as the storage component quite a long time ago. Instead, Supabase storage is used. Supabase API was removed as a dependency in favor of psycopg library (DB connection) and requests (storage operations) Note: using Supabase API for storage connection may have been the cause of some downloading issues on the Transfactory node side. The scripts underwent a number of adjustments and improvements, but the flow essentially remained unchanged since the moment it was established.","title":"July 2, 2023"},{"location":"active/transcript_factory/#november-25-2023","text":"The flow underwent a major refactoring: Transprocessor was updated with a view at running it in a Docker container on server (WIP). Clean-up functionality was integrated into the processor to simplify the setup (except for the resulting .srt files, for which an expiration policy was set at 1 week as a safety fallback measure). Transcleaner was deprecated due to its functionality being merged into the processor. Transsuply wasn't changed much on the code level, but the decision was made to run it no more than once a week, uploading 50-60 Gb of audiofiles in one go. Factory node script is undergoing the process of refactoring into a CLI. In addition, minio client is now being used to communicate with storage and not requests .","title":"November 25, 2023"},{"location":"active/transcript_factory/#goals","text":"[ ] Finish refactoring Factory node script into a CLI [ ] Refactor transsuply into a CLI [ ] Spin up Docker container with Transprocessor on server.","title":"Goals"},{"location":"active/web_app/","text":"Web Application refers to wapaganda , aka W , which is the web interface to the database and accompanying storage entities. Progress Log July 2, 2023 Endpoint for the patients is implemented; endpoints for theory and organizations are under way; Principal design is applied to the list of the patients; work on the patient's page design is under way, as well as an article page, and dark/light mode switcher; Issues with CORS were overcome, and lots of other crap. November 25, 2023 Logo was created. Multiple design improvements, usually following the changes in the data structures. RUCR diagram was created and integrated into the UI. Goals [ ] Finalize RUCR [ ] Finalize the translation flow of RUCR","title":"Active Initiatives. Web Application"},{"location":"active/web_app/#progress-log","text":"","title":"Progress Log"},{"location":"active/web_app/#july-2-2023","text":"Endpoint for the patients is implemented; endpoints for theory and organizations are under way; Principal design is applied to the list of the patients; work on the patient's page design is under way, as well as an article page, and dark/light mode switcher; Issues with CORS were overcome, and lots of other crap.","title":"July 2, 2023"},{"location":"active/web_app/#november-25-2023","text":"Logo was created. Multiple design improvements, usually following the changes in the data structures. RUCR diagram was created and integrated into the UI.","title":"November 25, 2023"},{"location":"active/web_app/#goals","text":"[ ] Finalize RUCR [ ] Finalize the translation flow of RUCR","title":"Goals"},{"location":"adr/1/","text":"Context A long and tedious conflict between Russia and Ukraine broke into an acute phase on February 24, 2022, when Russia started the invasion of Ukraine from several directions at the same time. Simultaneously (perhaps, even prior to the event), the machine of Russian propaganda significantly ramped up its activity. Political talk shows, already abundant in the Russian media space, became prevalent, almost pushing out the entertaining content entirely. The Russian propaganda machine is a loose amalgamation of media outlets (TV channels, radio stations, internet platforms, Telegram channels) and personalities, who apply effort at spreading, re-affirming, maintaining and developing the narratives invented by a small circle of most exclusive people, including Vladimir Putin, Sergey Lavrov, Ramzan Kadyrov, Dmitry Kiselyov, Margarita Simonyan and Vladimir Solovyov. This system is fueled by massive financial allocations, and counts over one thousand willing participants (registered as of the moment of me writing this). The motivation for keeping tabs on them is as follows: The impact of propaganda is significant in any war, and totally overwhelming in this one. Even though it is directed mainly against the native Russian population to keep them from revolting, the resulting damage to Ukrainian people is beyond imagination; Every single person who took part in a propagandistic activity with a media component starting with February 2022 (at least) is responsible for a share of that damage. This share is different for each, and depends on multiple factors, such as the frequency of TV appearances, social media following, etc. (a complete list is to be compiled at a later date); Everyone who bears a share of this responsibility must be held accountable at some point; To make sure it's even possible, every bit of useful information relevant to every person responsible must be collected and stored in a way that would make the retrieval and usage of this information as simple for researchers and investigators as possible. Decision A database was created to capture all the relevant data. It was decided to use relational model, and specifically PostgreSQL database engine for the advanced features it offers (such as full text search ). Supabase was selected to host the database. It's a relatively new service that is an alternative to Firebase, is build around PostgreSQL DB, and offers a range of advanced features and services (including in-built AWS Storage). To provide a publicly accessible interface for the data collected, a service application was created in Python using the web2py framework. ( web2py book ) The application is hosted on Railway ( Railway documentation ). Status Succeeded by DVE-A-52. Consequences Due to Supabase being quite new, their Python wrapper does not have all the functionality of the general library, the general library itself is being actively developed, and the documentation is far from ideal. This may cause delays in solving certain kinds of problems. Web2py architecture, although very efficient, somewhat limits the selection of tools that could be used in the project (for example, adding Poetry (dependency management tool) proved to be too much of a nuisance).","title":"\ud83d\uddf6\u227d ADR-1. Wapaganda Origin & [Initial] Structure [1]"},{"location":"adr/1/#context","text":"A long and tedious conflict between Russia and Ukraine broke into an acute phase on February 24, 2022, when Russia started the invasion of Ukraine from several directions at the same time. Simultaneously (perhaps, even prior to the event), the machine of Russian propaganda significantly ramped up its activity. Political talk shows, already abundant in the Russian media space, became prevalent, almost pushing out the entertaining content entirely. The Russian propaganda machine is a loose amalgamation of media outlets (TV channels, radio stations, internet platforms, Telegram channels) and personalities, who apply effort at spreading, re-affirming, maintaining and developing the narratives invented by a small circle of most exclusive people, including Vladimir Putin, Sergey Lavrov, Ramzan Kadyrov, Dmitry Kiselyov, Margarita Simonyan and Vladimir Solovyov. This system is fueled by massive financial allocations, and counts over one thousand willing participants (registered as of the moment of me writing this). The motivation for keeping tabs on them is as follows: The impact of propaganda is significant in any war, and totally overwhelming in this one. Even though it is directed mainly against the native Russian population to keep them from revolting, the resulting damage to Ukrainian people is beyond imagination; Every single person who took part in a propagandistic activity with a media component starting with February 2022 (at least) is responsible for a share of that damage. This share is different for each, and depends on multiple factors, such as the frequency of TV appearances, social media following, etc. (a complete list is to be compiled at a later date); Everyone who bears a share of this responsibility must be held accountable at some point; To make sure it's even possible, every bit of useful information relevant to every person responsible must be collected and stored in a way that would make the retrieval and usage of this information as simple for researchers and investigators as possible.","title":"Context"},{"location":"adr/1/#decision","text":"A database was created to capture all the relevant data. It was decided to use relational model, and specifically PostgreSQL database engine for the advanced features it offers (such as full text search ). Supabase was selected to host the database. It's a relatively new service that is an alternative to Firebase, is build around PostgreSQL DB, and offers a range of advanced features and services (including in-built AWS Storage). To provide a publicly accessible interface for the data collected, a service application was created in Python using the web2py framework. ( web2py book ) The application is hosted on Railway ( Railway documentation ).","title":"Decision"},{"location":"adr/1/#status","text":"Succeeded by DVE-A-52.","title":"Status"},{"location":"adr/1/#consequences","text":"Due to Supabase being quite new, their Python wrapper does not have all the functionality of the general library, the general library itself is being actively developed, and the documentation is far from ideal. This may cause delays in solving certain kinds of problems. Web2py architecture, although very efficient, somewhat limits the selection of tools that could be used in the project (for example, adding Poetry (dependency management tool) proved to be too much of a nuisance).","title":"Consequences"},{"location":"adr/10/","text":"Context Images are an intrinsic part of the project, and as such, they need to be properly stored and handled. Initial temporary solution, has drawbacks, such as limited scalability, hence a more robust approach is in order. Decision Reference set A set of photos in their original sizes that is to serve as a reference point for the project in general. Reference set is to be stored on a private bucket ( reference-id-photos ). File format is .png . Images of the reference set are NOT to be used in the project directly. New photos are added to the reference set, following the naming format 000000id_fullname_en , where the id part is patient's database ID with as many additional zeros as needed to make it 8 characters long; and fullname_en must be the same as the one stored in the database in the corresponding field, lowercase, with non-conventional characters replaced via the unicodedata.normalize . Resized images Resized images are to be stored in .jpg format. File names for the resized images are to follow this format: 000000id.jpg , where the id part is a patient's database ID with as many additional zeros as needed to make it 8 characters long; Resized images are to be stored in the separate photos bucket, in a folder containing three (3) subfolders, 2 of them corresponding to size presets ( thumbnails for thumbnails and large for the primary version), and the 3rd containing the default images, which are to be shown for every patient without a photo. Adding new images A: When a new person is added to the database, and a face photo for them is available: An add person with photo flow includes 3 main steps: https://github.com/betterthanever2/wapaganda_tools/blob/master/umisc/add_person.py Adding a patient to the table people . Uploading reference image; Generating a patient photoset and uploading the resized images, also coupled with writing corresponding records to the photos and people_on_photos tables. B: When a face photo is found for a person already in db but so far without a face photo / When a face photo needs to be replaced An add/update person photo flow includes 4 main steps: https://github.com/betterthanever2/wapaganda_tools/blob/master/umisc/photo_upsert.py Generating a patient photoset; Upload each of the 3 photos (including the reference one) with replacement if necessary; Insert corresponding records to the tables if necessary. C: Non-face photo A separate ADR would be devised as an addition to this one, or a replacement thereof. Status Accepted. Consequences Since the photos are stored in Supabase Storage, their availability is subject to the availability of the service. Reference photo set serves as both a backup (also providing non-compressed versions \\~ png ), and, with the Supabase UI, a way to work with the photos manually, should such need ever arises.","title":"\ud83d\uddf8 ARD-10. Photos"},{"location":"adr/10/#context","text":"Images are an intrinsic part of the project, and as such, they need to be properly stored and handled. Initial temporary solution, has drawbacks, such as limited scalability, hence a more robust approach is in order.","title":"Context"},{"location":"adr/10/#decision","text":"","title":"Decision"},{"location":"adr/10/#reference-set","text":"A set of photos in their original sizes that is to serve as a reference point for the project in general. Reference set is to be stored on a private bucket ( reference-id-photos ). File format is .png . Images of the reference set are NOT to be used in the project directly. New photos are added to the reference set, following the naming format 000000id_fullname_en , where the id part is patient's database ID with as many additional zeros as needed to make it 8 characters long; and fullname_en must be the same as the one stored in the database in the corresponding field, lowercase, with non-conventional characters replaced via the unicodedata.normalize .","title":"Reference set"},{"location":"adr/10/#resized-images","text":"Resized images are to be stored in .jpg format. File names for the resized images are to follow this format: 000000id.jpg , where the id part is a patient's database ID with as many additional zeros as needed to make it 8 characters long; Resized images are to be stored in the separate photos bucket, in a folder containing three (3) subfolders, 2 of them corresponding to size presets ( thumbnails for thumbnails and large for the primary version), and the 3rd containing the default images, which are to be shown for every patient without a photo.","title":"Resized images"},{"location":"adr/10/#adding-new-images","text":"A: When a new person is added to the database, and a face photo for them is available: An add person with photo flow includes 3 main steps: https://github.com/betterthanever2/wapaganda_tools/blob/master/umisc/add_person.py Adding a patient to the table people . Uploading reference image; Generating a patient photoset and uploading the resized images, also coupled with writing corresponding records to the photos and people_on_photos tables. B: When a face photo is found for a person already in db but so far without a face photo / When a face photo needs to be replaced An add/update person photo flow includes 4 main steps: https://github.com/betterthanever2/wapaganda_tools/blob/master/umisc/photo_upsert.py Generating a patient photoset; Upload each of the 3 photos (including the reference one) with replacement if necessary; Insert corresponding records to the tables if necessary. C: Non-face photo A separate ADR would be devised as an addition to this one, or a replacement thereof.","title":"Adding new images"},{"location":"adr/10/#status","text":"Accepted.","title":"Status"},{"location":"adr/10/#consequences","text":"Since the photos are stored in Supabase Storage, their availability is subject to the availability of the service. Reference photo set serves as both a backup (also providing non-compressed versions \\~ png ), and, with the Supabase UI, a way to work with the photos manually, should such need ever arises.","title":"Consequences"},{"location":"adr/11/","text":"Context Early version of the W features a \"Useless Stats\" block, which contained a few basic computed numbers on the database in general, and for each person specifically. The final version of the product should as well include statistical figures. This document is to serve as a primer on this subject. Decision Following stats are to be computed. General Stats Data Notes Number of patients (population) Average/Median age across the db Number of registered media hours Number of transcripted media hours Number of registered media segments Number of relevant organizations Number of Telegram channels on file Number of written works on file (articles, books) Current day of war Per Person Status Rejected. Not ADR material Consequences New functions should be written to compute the stats.","title":"\ud83d\uddf6 ADR-11. Database Stats"},{"location":"adr/11/#context","text":"Early version of the W features a \"Useless Stats\" block, which contained a few basic computed numbers on the database in general, and for each person specifically. The final version of the product should as well include statistical figures. This document is to serve as a primer on this subject.","title":"Context"},{"location":"adr/11/#decision","text":"Following stats are to be computed.","title":"Decision"},{"location":"adr/11/#general","text":"Stats Data Notes Number of patients (population) Average/Median age across the db Number of registered media hours Number of transcripted media hours Number of registered media segments Number of relevant organizations Number of Telegram channels on file Number of written works on file (articles, books) Current day of war","title":"General"},{"location":"adr/11/#per-person","text":"","title":"Per Person"},{"location":"adr/11/#status","text":"Rejected. Not ADR material","title":"Status"},{"location":"adr/11/#consequences","text":"New functions should be written to compute the stats.","title":"Consequences"},{"location":"adr/12/","text":"Context DVE-A-2 describes the structure of the project as it was conceived, and that should now be referred to as v0 . Current document succeeds it, and describes the structure of Wapaganda v1 , which is being developed by @atatatko. Decision Backend We use Django as a primary backend framework, and Django Rest Framework as a primary API framework. However, we do not use Django views for page generation, rather we serve the React web app as a set of static files, which serves as a client for the API, populating data about personalities, organizations and other entities. The API is not used for managing users, authentication, authorization, or creating/updating data, meaning the application itself is read-only. However, there is an admin panel for the API, which is used to manage data in the database, located at https://api.subjective.agency/admin/ Frontend We use React as a primary frontend framework, as it offers active support, large community, documentation and examples. We minimize use of 3rd party dependencies, as we don't have much time to maintain security and vulnerability updates. React and JS offer pretty much everything we need: asynchronous REST client, localization, working with local cache and database. Status Backend As of mid-April 2023, the backend could be considered a stable beta, REST API core is already created and tested, which reduces further development to adding new endpoints and new types of requests. However, any changes in database schema require significant work on backend side, so release could be possible only when we have a more or less stable schema. Frontend As of mid-April 2023, frontend Single-Page Application (SPA) is a solid alpha, with partially implemented design and localization, and fully implemented caching, searching tools and HTTP client. Frontend does not depend much on any database schema changes. The core of REST API protocol is already implemented. Consequences Relying on Django and React is beneficial in the sense that both frameworks are thriving ecosystems, so solving a specific issue would take less time on average and would be relatively simpler. We should keep in mind, however, that both Django and React have vulnerabilities and limitations, which are applicable to the W project.","title":"\ud83d\uddf6\u227d ADR-12. Wapaganda Structure [2]"},{"location":"adr/12/#context","text":"DVE-A-2 describes the structure of the project as it was conceived, and that should now be referred to as v0 . Current document succeeds it, and describes the structure of Wapaganda v1 , which is being developed by @atatatko.","title":"Context"},{"location":"adr/12/#decision","text":"","title":"Decision"},{"location":"adr/12/#backend","text":"We use Django as a primary backend framework, and Django Rest Framework as a primary API framework. However, we do not use Django views for page generation, rather we serve the React web app as a set of static files, which serves as a client for the API, populating data about personalities, organizations and other entities. The API is not used for managing users, authentication, authorization, or creating/updating data, meaning the application itself is read-only. However, there is an admin panel for the API, which is used to manage data in the database, located at https://api.subjective.agency/admin/","title":"Backend"},{"location":"adr/12/#frontend","text":"We use React as a primary frontend framework, as it offers active support, large community, documentation and examples. We minimize use of 3rd party dependencies, as we don't have much time to maintain security and vulnerability updates. React and JS offer pretty much everything we need: asynchronous REST client, localization, working with local cache and database.","title":"Frontend"},{"location":"adr/12/#status","text":"","title":"Status"},{"location":"adr/12/#backend_1","text":"As of mid-April 2023, the backend could be considered a stable beta, REST API core is already created and tested, which reduces further development to adding new endpoints and new types of requests. However, any changes in database schema require significant work on backend side, so release could be possible only when we have a more or less stable schema.","title":"Backend"},{"location":"adr/12/#frontend_1","text":"As of mid-April 2023, frontend Single-Page Application (SPA) is a solid alpha, with partially implemented design and localization, and fully implemented caching, searching tools and HTTP client. Frontend does not depend much on any database schema changes. The core of REST API protocol is already implemented.","title":"Frontend"},{"location":"adr/12/#consequences","text":"Relying on Django and React is beneficial in the sense that both frameworks are thriving ecosystems, so solving a specific issue would take less time on average and would be relatively simpler. We should keep in mind, however, that both Django and React have vulnerabilities and limitations, which are applicable to the W project.","title":"Consequences"},{"location":"adr/13/","text":"Parent to DVE-A-27: \ud83d\uddf8 ADR-6: Transcripts , DVE-A-29: \u221e ADR-7: Media Episodes Interface , DVE-A-30: \u221e ADR-8: Transcript Mark-Up Context The domain of media activity covers numerous media platforms, each of which has its own specific peculiarities and context and requires a custom approach. This document describes a working model that encapsulates all relevant nuances of the terrain. Decision Entities Media Platforms Media platform is an online service that allows a subset of its users to publish media items such as videos or audios for public access. In the database, platforms are represented with enums.media_platforms table. Media Targets Media target, in the context of the system, is a generic term for a platform-dependent account entity, such as channel on YouTube, RuTube etc., or brand on Smotrim.ru. In the database, targets are represented with platform-dependent tables: youtube_channels smotrim_brands rutube_channels komso_categories ntv_categories dentv_categories Media Episodes Media episode is a specific media item, such as video, published on a media platform, under a media target, for public access. In the database, media episodes are collected into platform-specific tables: youtube_vids smotrim_episodes komso_episodes ntv_episodes dentv_episodes rutube_vids Media episode is connected to a target in corresponding table via a foreign key. Media Segments Platforms, targets and episodes are organized into a set of similarly structured hierarchies. Media segment is a distinct series of media episodes that could be published across multiple platforms and targets while retaining sequence within the series. Since numerous platforms and targets could be involved, the implementation provides a layer of abstraction on top of that set, allowing cross-platform matching. In the database, media segments are collected in media_segments table, and connect to the platform hierarchies on the episode level via segment_id attribute. There are several distinct types of media segments: named is the most common type, and represents a sequence of episodes intentionally designed to be a part of it. composite type represents a situation, when a segment is published across a number of targets, usually on a single platform, and usually territory-specific. personal is person-specific catch-all segment that encapsulates all media episodes that could be tied to a specific author and don't belong to any of the named segments. complex is expected to be tangled with a number of other segments. Media Episodes Clusters Clusters are another layer of abstraction required to account for duplicates accross platforms/targets without dismissing them. We don't want to dismiss them because, in the context of the project, duplicates (provided they weren't a result of our internal system's malfunction) still work to increase the propaganda coverage. In the database, clusters of media episodes are collected in media_episodes_clusters table, and connect to the platform hierarchies on the episode level via cluster_id attribute. When first added, a media episode is not assigned to any cluster (i.e. has cluster_id attribute = null). If it is determined to be relevant, new entry is added to the service.clustering table, which serves as a job queue for the clustering service. Services Update Service Media segment serves as a starting point. Its database representation is loaded with MediaSegmentService class, which fetches all relevant platform_links (i.e. MediaSegmentOnPlatform objects, many-to-many mapping between segments and targets ), iterates over them and shapes a list of targets , i.e. subclass instances of MediaPlatformService . The list of targets is iterated over, calling on each fetch_and_parse_updates method that would apply target-specific logic to collect the updates. After collecting updates, a new record is created in service.media_queue table with job_type set to update . By default, all new items are persisted with cluster_id set to null and relevance_status_id set to 2 ( undefined ), in order for respective services to pick them up for resolution. If the type of segment is complex , then segment_id attribute will be nullified. These episodes would later be resolved with segment detection service. Clustering Service Clustering service picks up records from the service.media_queue table with job_type = clustering , and collects existing media clusters into scope parameter. Then relevant episode is compared to each item in the scope. If no match is detected, the service creates new cluster . If one match is found, episode's cluster_id is updated correspondingly. If more than one match is found, a new record is created in service.media_conflicts table, with information about the episode and all matching clusters. Matching Matching takes into account title , description (if present) and duration (if present). Stats Collection Service TBD Media Queue Service Queue service is meant to create media jobs (records in service.media_queue table) where they cannot be created in a simpler way, and specifically for: Clustering service; Relevance service; Initial jobs for update service: in the regular flow, new jobs are schedule by the update service itself, but when a new segment is added, they need to be inserted into that flow. Relevance Service TBD Segment Detection Service TBD Interface-Enabled Entrypoints There are several points in this system where the need for manual intervention is inescapeable. The clustering process may produce conflicting matching results; Segments may not be linked to any of the targets / A target may not be assigned to any of the segments. Items may have been clustered together that aren't actually related; ... For these, UI-enabled routes should be developed in the ave_media application, that would allow resolution of those issues. Status Cooking Consequences","title":"\u221e ADR-13. Media Segments Primer"},{"location":"adr/13/#context","text":"The domain of media activity covers numerous media platforms, each of which has its own specific peculiarities and context and requires a custom approach. This document describes a working model that encapsulates all relevant nuances of the terrain.","title":"Context"},{"location":"adr/13/#decision","text":"","title":"Decision"},{"location":"adr/13/#entities","text":"","title":"Entities"},{"location":"adr/13/#media-platforms","text":"Media platform is an online service that allows a subset of its users to publish media items such as videos or audios for public access. In the database, platforms are represented with enums.media_platforms table.","title":"Media Platforms"},{"location":"adr/13/#media-targets","text":"Media target, in the context of the system, is a generic term for a platform-dependent account entity, such as channel on YouTube, RuTube etc., or brand on Smotrim.ru. In the database, targets are represented with platform-dependent tables: youtube_channels smotrim_brands rutube_channels komso_categories ntv_categories dentv_categories","title":"Media Targets"},{"location":"adr/13/#media-episodes","text":"Media episode is a specific media item, such as video, published on a media platform, under a media target, for public access. In the database, media episodes are collected into platform-specific tables: youtube_vids smotrim_episodes komso_episodes ntv_episodes dentv_episodes rutube_vids Media episode is connected to a target in corresponding table via a foreign key.","title":"Media Episodes"},{"location":"adr/13/#media-segments","text":"Platforms, targets and episodes are organized into a set of similarly structured hierarchies. Media segment is a distinct series of media episodes that could be published across multiple platforms and targets while retaining sequence within the series. Since numerous platforms and targets could be involved, the implementation provides a layer of abstraction on top of that set, allowing cross-platform matching. In the database, media segments are collected in media_segments table, and connect to the platform hierarchies on the episode level via segment_id attribute. There are several distinct types of media segments: named is the most common type, and represents a sequence of episodes intentionally designed to be a part of it. composite type represents a situation, when a segment is published across a number of targets, usually on a single platform, and usually territory-specific. personal is person-specific catch-all segment that encapsulates all media episodes that could be tied to a specific author and don't belong to any of the named segments. complex is expected to be tangled with a number of other segments.","title":"Media Segments"},{"location":"adr/13/#media-episodes-clusters","text":"Clusters are another layer of abstraction required to account for duplicates accross platforms/targets without dismissing them. We don't want to dismiss them because, in the context of the project, duplicates (provided they weren't a result of our internal system's malfunction) still work to increase the propaganda coverage. In the database, clusters of media episodes are collected in media_episodes_clusters table, and connect to the platform hierarchies on the episode level via cluster_id attribute. When first added, a media episode is not assigned to any cluster (i.e. has cluster_id attribute = null). If it is determined to be relevant, new entry is added to the service.clustering table, which serves as a job queue for the clustering service.","title":"Media Episodes Clusters"},{"location":"adr/13/#services","text":"","title":"Services"},{"location":"adr/13/#update-service","text":"Media segment serves as a starting point. Its database representation is loaded with MediaSegmentService class, which fetches all relevant platform_links (i.e. MediaSegmentOnPlatform objects, many-to-many mapping between segments and targets ), iterates over them and shapes a list of targets , i.e. subclass instances of MediaPlatformService . The list of targets is iterated over, calling on each fetch_and_parse_updates method that would apply target-specific logic to collect the updates. After collecting updates, a new record is created in service.media_queue table with job_type set to update . By default, all new items are persisted with cluster_id set to null and relevance_status_id set to 2 ( undefined ), in order for respective services to pick them up for resolution. If the type of segment is complex , then segment_id attribute will be nullified. These episodes would later be resolved with segment detection service.","title":"Update Service"},{"location":"adr/13/#clustering-service","text":"Clustering service picks up records from the service.media_queue table with job_type = clustering , and collects existing media clusters into scope parameter. Then relevant episode is compared to each item in the scope. If no match is detected, the service creates new cluster . If one match is found, episode's cluster_id is updated correspondingly. If more than one match is found, a new record is created in service.media_conflicts table, with information about the episode and all matching clusters.","title":"Clustering Service"},{"location":"adr/13/#matching","text":"Matching takes into account title , description (if present) and duration (if present).","title":"Matching"},{"location":"adr/13/#stats-collection-service","text":"TBD","title":"Stats Collection Service"},{"location":"adr/13/#media-queue-service","text":"Queue service is meant to create media jobs (records in service.media_queue table) where they cannot be created in a simpler way, and specifically for: Clustering service; Relevance service; Initial jobs for update service: in the regular flow, new jobs are schedule by the update service itself, but when a new segment is added, they need to be inserted into that flow.","title":"Media Queue Service"},{"location":"adr/13/#relevance-service","text":"TBD","title":"Relevance Service"},{"location":"adr/13/#segment-detection-service","text":"TBD","title":"Segment Detection Service"},{"location":"adr/13/#interface-enabled-entrypoints","text":"There are several points in this system where the need for manual intervention is inescapeable. The clustering process may produce conflicting matching results; Segments may not be linked to any of the targets / A target may not be assigned to any of the segments. Items may have been clustered together that aren't actually related; ... For these, UI-enabled routes should be developed in the ave_media application, that would allow resolution of those issues.","title":"Interface-Enabled Entrypoints"},{"location":"adr/13/#status","text":"Cooking","title":"Status"},{"location":"adr/13/#consequences","text":"","title":"Consequences"},{"location":"adr/14/","text":"Context There is a subset of patients with a history of writing books, articles, poems, etc. many of which were subsequently published. A significant portion of what was published by them has direct relation to the mission of the W project in that they promote, in one way or another, the messages of the Russian propaganda. This printed data = an umbrella term that incorporates books, articles, blog posts, etc. - should be properly stored in the DB and used in the project. Decision Following DB setup was implemented to store the printed data: Table public.printed stores primary entities (books, articles, etc.); Table public.printed_to_people_mapping stores authorship relationships; Table data.printed_content stores the content of respective printed items, divided by chapters, one record per raw chapter; For EPUB files, raw chapter is defined as a distinct part of the archive with internal type chapter . However, it should be taken into account that many EPUB files have a suboptimal structure; for example, an actual chapter in the file could be split into several separate raw chapters , or several actual chapters could be merged into a single raw chapter . printed_content table also has a field parsed_content which is supposed to contain the edited version of the raw_content field. Editing in this context means 2 things: 1) either split or merge, depending on the specific situation; 2) cleaning up - removing garbage code (such as <p class=\"empty-line\"> ), extra spaces, extra newlines, etc. When editing, additional precautions should be taken to avoid accidental alteration of the actual text of the printed piece. Required routines The following use cases should be handled in the future: Book was added by accident: remove all artifacts (in this order) items from public.printed_to_people_mapping items from data.printed_content items from public.printed object from storage Author was not properly identified: indicate author manually Chapter is actually a piece of the previous one: when editing the raw HTML, the clean text should be merged with the clean text of the 1st record pertaining to the chapter in question, so that only one chapter record contains the full clean text. This initial infrastructure allows for storage of the content, but in order to use it in the project, parsed_content should be produced first. Status Accepted. Consequences Even more data.","title":"\ud83d\uddf8 ADR-14. Printed Data"},{"location":"adr/14/#context","text":"There is a subset of patients with a history of writing books, articles, poems, etc. many of which were subsequently published. A significant portion of what was published by them has direct relation to the mission of the W project in that they promote, in one way or another, the messages of the Russian propaganda. This printed data = an umbrella term that incorporates books, articles, blog posts, etc. - should be properly stored in the DB and used in the project.","title":"Context"},{"location":"adr/14/#decision","text":"Following DB setup was implemented to store the printed data: Table public.printed stores primary entities (books, articles, etc.); Table public.printed_to_people_mapping stores authorship relationships; Table data.printed_content stores the content of respective printed items, divided by chapters, one record per raw chapter; For EPUB files, raw chapter is defined as a distinct part of the archive with internal type chapter . However, it should be taken into account that many EPUB files have a suboptimal structure; for example, an actual chapter in the file could be split into several separate raw chapters , or several actual chapters could be merged into a single raw chapter . printed_content table also has a field parsed_content which is supposed to contain the edited version of the raw_content field. Editing in this context means 2 things: 1) either split or merge, depending on the specific situation; 2) cleaning up - removing garbage code (such as <p class=\"empty-line\"> ), extra spaces, extra newlines, etc. When editing, additional precautions should be taken to avoid accidental alteration of the actual text of the printed piece.","title":"Decision"},{"location":"adr/14/#required-routines","text":"The following use cases should be handled in the future: Book was added by accident: remove all artifacts (in this order) items from public.printed_to_people_mapping items from data.printed_content items from public.printed object from storage Author was not properly identified: indicate author manually Chapter is actually a piece of the previous one: when editing the raw HTML, the clean text should be merged with the clean text of the 1st record pertaining to the chapter in question, so that only one chapter record contains the full clean text. This initial infrastructure allows for storage of the content, but in order to use it in the project, parsed_content should be produced first.","title":"Required routines"},{"location":"adr/14/#status","text":"Accepted.","title":"Status"},{"location":"adr/14/#consequences","text":"Even more data.","title":"Consequences"},{"location":"adr/15/","text":"Context Continuing research into the subject of Russian propaganda and description thereof requires certain terminology that might not be immediately obvious to an external user. This could be either existing terms of some narrow application, or terms created/adapted for the purposes of the W project. Decision Glossary, a special document listing these special terms, must be created as part of the Theory section of the W web app. Status Rejected. Not ADR material Consequences Glossary is expected to improve external user's experience with regard to understanding the essence of the W approach. However, there must be an additional solution for automatically identifying terms included into the glossary and attaching corresponding links to them.","title":"\ud83d\uddf6 ADR-15. Glossary"},{"location":"adr/15/#context","text":"Continuing research into the subject of Russian propaganda and description thereof requires certain terminology that might not be immediately obvious to an external user. This could be either existing terms of some narrow application, or terms created/adapted for the purposes of the W project.","title":"Context"},{"location":"adr/15/#decision","text":"Glossary, a special document listing these special terms, must be created as part of the Theory section of the W web app.","title":"Decision"},{"location":"adr/15/#status","text":"Rejected. Not ADR material","title":"Status"},{"location":"adr/15/#consequences","text":"Glossary is expected to improve external user's experience with regard to understanding the essence of the W approach. However, there must be an additional solution for automatically identifying terms included into the glossary and attaching corresponding links to them.","title":"Consequences"},{"location":"adr/16/","text":"Context All the textual items relevant for the W project can be put into one of the following categories: core : Texts written specifically for the W project, or for the Dum Vita Est blog, its predecessor. Most of these describe various aspects of the Copium Theory of Propaganda . companions : Texts written by independent authors that contribute to the elucidation and research of propaganda in general, of Russian propaganda instance, or other things related in any way to the subject area of the W project. copium : Texts written by Russian propagandists that shed light on their line of thinking. Decision In the context of the W project, theory is a special section that consists of 3 text collections named core , companions and copium . Theory is a top-level navigation target on the W website, alongside people , organizations and media segments . The initial body of companion and copium texts was translated from Russian into English for the Dum Vita Est blog, and, in April 2023, translated from Russian into Ukrainian using openai/gpt-3.5 . Going forward, translations from one language to another should be delegated entirely to AI, with human responsible for 1) selecting translation targets and preparing prompts; and 2) editing and verifying the AI-generated output. Both content and metadata for all the theory items are stored in the theory table ( public schema), with the following structure: { \"title\": { \"en\": \"\", \"ru\": \"\", \"uk\": \"\" }, \"type\": \"\", \"excerpt\": { \"en\": \"\", \"ru\": \"\", \"uk\": \"\" }, \"images\": [], \"content\" : { \"en\": { \"translated_by\": {\"i\": \"\", \"author\": \"\"}, \"markdown\": \"\", \"edit_history\": [{\"who\": \"\", \"when\": \"\"}] }, \"ru\": {\"markdown\": \"\"}, \"uk\": { \"translated_by\": {\"i\": \"\", \"author\": \"\"}, \"markdown\": \"\", \"edit_history\": [{\"who\": \"\", \"when\": \"\"}] } }, \"original_content_metadata\": [ {\"date_published\": \"\",\"url\": \"\", \"author\": \"\"} ] } type is a string, one of core , copium , companion , system ; system is a set of auxiliary articles, including FAQ, About Page, and Glossary. title and excerpt are triple-lang JSONBs with keys containing plain text; images is a list of URLs by the look of https://svfizyfozagyqkkjzqdc.supabase.co/storage/v1/object/public/photos/theory/default/default-thumb.png original_content_metadata is a list of JSONBs, each element containing at least keys date_published author , and url where possible; content is a triple-lang JSONB with keys containing dictionaries. If a lang key refers to the original text, it contains only markdown key; otherwise, it also contains translated_by and edit_history keys. markdown contains complete text of the item in Markdown format; translated_by is a dictionary with 2 required keys: i stands for intelligence and can be either a ( artificial ) or h ( human ); author contains original name of the author or reference to the AI; If this key is absent, this is an original (unless something went terribly wrong). * edit_history is meant to serve as a minimalistic edit log. It's a list of dictionaries with (preliminary) two required keys: * who contains name of the editor; * when - date of the edit. If this key is absent, the text was not edited. If this key is present but translated_by key is absent, something went wrong. Status Cooking. Consequences","title":"\ud83d\uddf6\u227d ADR-16. Theory"},{"location":"adr/16/#context","text":"All the textual items relevant for the W project can be put into one of the following categories: core : Texts written specifically for the W project, or for the Dum Vita Est blog, its predecessor. Most of these describe various aspects of the Copium Theory of Propaganda . companions : Texts written by independent authors that contribute to the elucidation and research of propaganda in general, of Russian propaganda instance, or other things related in any way to the subject area of the W project. copium : Texts written by Russian propagandists that shed light on their line of thinking.","title":"Context"},{"location":"adr/16/#decision","text":"In the context of the W project, theory is a special section that consists of 3 text collections named core , companions and copium . Theory is a top-level navigation target on the W website, alongside people , organizations and media segments . The initial body of companion and copium texts was translated from Russian into English for the Dum Vita Est blog, and, in April 2023, translated from Russian into Ukrainian using openai/gpt-3.5 . Going forward, translations from one language to another should be delegated entirely to AI, with human responsible for 1) selecting translation targets and preparing prompts; and 2) editing and verifying the AI-generated output. Both content and metadata for all the theory items are stored in the theory table ( public schema), with the following structure: { \"title\": { \"en\": \"\", \"ru\": \"\", \"uk\": \"\" }, \"type\": \"\", \"excerpt\": { \"en\": \"\", \"ru\": \"\", \"uk\": \"\" }, \"images\": [], \"content\" : { \"en\": { \"translated_by\": {\"i\": \"\", \"author\": \"\"}, \"markdown\": \"\", \"edit_history\": [{\"who\": \"\", \"when\": \"\"}] }, \"ru\": {\"markdown\": \"\"}, \"uk\": { \"translated_by\": {\"i\": \"\", \"author\": \"\"}, \"markdown\": \"\", \"edit_history\": [{\"who\": \"\", \"when\": \"\"}] } }, \"original_content_metadata\": [ {\"date_published\": \"\",\"url\": \"\", \"author\": \"\"} ] } type is a string, one of core , copium , companion , system ; system is a set of auxiliary articles, including FAQ, About Page, and Glossary. title and excerpt are triple-lang JSONBs with keys containing plain text; images is a list of URLs by the look of https://svfizyfozagyqkkjzqdc.supabase.co/storage/v1/object/public/photos/theory/default/default-thumb.png original_content_metadata is a list of JSONBs, each element containing at least keys date_published author , and url where possible; content is a triple-lang JSONB with keys containing dictionaries. If a lang key refers to the original text, it contains only markdown key; otherwise, it also contains translated_by and edit_history keys. markdown contains complete text of the item in Markdown format; translated_by is a dictionary with 2 required keys: i stands for intelligence and can be either a ( artificial ) or h ( human ); author contains original name of the author or reference to the AI; If this key is absent, this is an original (unless something went terribly wrong). * edit_history is meant to serve as a minimalistic edit log. It's a list of dictionaries with (preliminary) two required keys: * who contains name of the editor; * when - date of the edit. If this key is absent, the text was not edited. If this key is present but translated_by key is absent, something went wrong.","title":"Decision"},{"location":"adr/16/#status","text":"Cooking.","title":"Status"},{"location":"adr/16/#consequences","text":"","title":"Consequences"},{"location":"adr/17/","text":"Context As the project aims to ground its methods and approaches in existing and accepted practices, it makes sense to adopt some relevant taxonomies that have been developed by the world community. The first of these would be ISCO-08 taxonomy that provides a hierarchical description of occupations and professions. ISCO-08 was adopted in 2008; it replaced ISCO-88, the previous version. ISCO-08 is currently under revision, with the updated version to be released some time between 2028 and 2030. It is maintained by the ILO (International Labor Organization). ISCO standard is based on the concept of skill , which is numerical and can be between 1 (lowest) and 4 (highest). ISCO-08 is composed of 10 major groups, 43 sub-major groups, 130 minor groups and 436 unit groups, i.e. has 4 levels in the hierarchy (note: the hierarchy levels DO NOT correspond directly to the skill levels). Each group in the hierarchy is indicated with an ISCO code. Code for major groups is 1 digit (i.e. 0 through 9), for sub-major groups - 2 digits, for minor groups - 3 digits, and for unit groups - 4 digits. For example, here's a part from the beginning of the hierarchy: 1 Managers 11 Chief Executives, Senior Officials and Legislators 111 Legislators and Senior Officials 1111 Legislators 1112 Senior Government Officials 1113 Traditional Chiefs and Heads of Villages 1114 Senior Officials of Special-interest Organizations 112 Managing Directors and Chief Executives 1120 Managing Directors and Chief Executives Managers is a major (aka, root) group indicated with 1 , Chief Executives, Senior Officials and Legislators is a sub-major group indicated with 11 , Legislators and Senior Officials and Managing Directors and Chief Executives are minor groups ( 111 and 112 respectively), and all the rest are unit groups, indicated with 4-digit codes. A few things to note: Terms could be identical. The idea is that every branch must end with a unit group, so in the case when there is no way to determine subcategories, like with 112 , the unit group code will have 0 at the end and the term will be the same as with the parent category; Codes also start with 0 , so when processing, they should be always treated as strings, not numbers. FYI, category 0 is about Armed Forces Occupations . Decision In order to use the taxonomy efficiently, it should be stored in the database. There are several approaches to this: adjacency list model, path enumeration model, nested set model, and closure table model. Closure table model was selected for the project. This approach involves the use of an additional table to represent the relationships between nodes in the hierarchy. The closure table captures all the ancestor-descendant relationships between nodes, allowing for efficient querying and traversal of the hierarchy. Here's how it works: Main table: The main table represents the nodes in the hierarchy. It contains the primary key column and other attributes related to each node. Closure table: The closure table is an auxiliary table that stores the ancestor-descendant relationships between nodes. It consists of two columns, usually named ancestor and descendant , which are foreign keys referencing the primary key of the main table. For the ISCO-08 taxonomy, the following tables were created in the enums schema: isco08_taxonomy - main table that contains term names, definitions and other attributes; isco08_taxonomy_closure - closure table with all the relationships. It has 2 columns, ancestor and descendant , which combination serves as the primary key (i.e. must be unique); isco08_index - a list of \\~7,000 professions with assigned category codes. This table is intended for reference, when defining a patient's position in an organization. In addition, table org_roles in the same schema will be maintained for professions outside the taxonomy/index. Table people_in_orgs of the public schema would undergo following updates: existing column role is to be deprecated once the processing of existing data is finished; new column is added role_category that points to enums.isco08_taxonomy ; new column is added role_ref1 that points to enums.isco08_index ; new column is added role_ref2 that points to enums.org_roles ; new column is added role_details (JSONB) with content being a dictionary with keys of , for , at , from , with , etc. (all optional), each of which, in its turn, holds a triple-lang structure ( {\"en\": \"\", \"ru\": \"\", \"uk\": \"\"} ). Example: json {\"at\": {\"en\": \"world history department\", \"ru\": \"\u0444\u0430\u043a\u0443\u043b\u044c\u0442\u0435\u0442 \u043c\u0438\u0440\u043e\u0432\u043e\u0439 \u0438\u0441\u0442\u043e\u0440\u0438\u0438\", \"uk\": \"\u0444\u0430\u043a\u0443\u043b\u044c\u0442\u0435\u0442 \u0441\u0432\u0456\u0442\u043e\u0432\u043e\u0457 \u0456\u0441\u0442\u043e\u0440\u0456\u0457\"} The algorithm for populating the closure table would be saved in the wapatools repository ( development/onto/isco08.py ). Status In progress. Consequences The flow for adding a person-to-organization relationship becomes somewhat more complicated, since it requires identification of the profession code. To simplify this, a SQL function search_in_isco8_index was implemented that allows full text search on the isco_08_index table returning term_id (i.e. category ID in isco_08_taxonomy ), index_id (i.e. ID of the profession in isco_08_index ), as well as codes and definitions for reference. Existing reference (content of the role column) is to be reworked into the updated format (in progress). Taxonomy terms should be translated into Russian and Ukrainian. In the long term, rooting in the existing taxonomies elaborated this deeply, would bring a little more order to the data, allowing to query it more efficiently.","title":"\ud83d\uddf8 ADR-17. Taxonomies. ISCO-08"},{"location":"adr/17/#context","text":"As the project aims to ground its methods and approaches in existing and accepted practices, it makes sense to adopt some relevant taxonomies that have been developed by the world community. The first of these would be ISCO-08 taxonomy that provides a hierarchical description of occupations and professions. ISCO-08 was adopted in 2008; it replaced ISCO-88, the previous version. ISCO-08 is currently under revision, with the updated version to be released some time between 2028 and 2030. It is maintained by the ILO (International Labor Organization). ISCO standard is based on the concept of skill , which is numerical and can be between 1 (lowest) and 4 (highest). ISCO-08 is composed of 10 major groups, 43 sub-major groups, 130 minor groups and 436 unit groups, i.e. has 4 levels in the hierarchy (note: the hierarchy levels DO NOT correspond directly to the skill levels). Each group in the hierarchy is indicated with an ISCO code. Code for major groups is 1 digit (i.e. 0 through 9), for sub-major groups - 2 digits, for minor groups - 3 digits, and for unit groups - 4 digits. For example, here's a part from the beginning of the hierarchy: 1 Managers 11 Chief Executives, Senior Officials and Legislators 111 Legislators and Senior Officials 1111 Legislators 1112 Senior Government Officials 1113 Traditional Chiefs and Heads of Villages 1114 Senior Officials of Special-interest Organizations 112 Managing Directors and Chief Executives 1120 Managing Directors and Chief Executives Managers is a major (aka, root) group indicated with 1 , Chief Executives, Senior Officials and Legislators is a sub-major group indicated with 11 , Legislators and Senior Officials and Managing Directors and Chief Executives are minor groups ( 111 and 112 respectively), and all the rest are unit groups, indicated with 4-digit codes. A few things to note: Terms could be identical. The idea is that every branch must end with a unit group, so in the case when there is no way to determine subcategories, like with 112 , the unit group code will have 0 at the end and the term will be the same as with the parent category; Codes also start with 0 , so when processing, they should be always treated as strings, not numbers. FYI, category 0 is about Armed Forces Occupations .","title":"Context"},{"location":"adr/17/#decision","text":"In order to use the taxonomy efficiently, it should be stored in the database. There are several approaches to this: adjacency list model, path enumeration model, nested set model, and closure table model. Closure table model was selected for the project. This approach involves the use of an additional table to represent the relationships between nodes in the hierarchy. The closure table captures all the ancestor-descendant relationships between nodes, allowing for efficient querying and traversal of the hierarchy. Here's how it works: Main table: The main table represents the nodes in the hierarchy. It contains the primary key column and other attributes related to each node. Closure table: The closure table is an auxiliary table that stores the ancestor-descendant relationships between nodes. It consists of two columns, usually named ancestor and descendant , which are foreign keys referencing the primary key of the main table. For the ISCO-08 taxonomy, the following tables were created in the enums schema: isco08_taxonomy - main table that contains term names, definitions and other attributes; isco08_taxonomy_closure - closure table with all the relationships. It has 2 columns, ancestor and descendant , which combination serves as the primary key (i.e. must be unique); isco08_index - a list of \\~7,000 professions with assigned category codes. This table is intended for reference, when defining a patient's position in an organization. In addition, table org_roles in the same schema will be maintained for professions outside the taxonomy/index. Table people_in_orgs of the public schema would undergo following updates: existing column role is to be deprecated once the processing of existing data is finished; new column is added role_category that points to enums.isco08_taxonomy ; new column is added role_ref1 that points to enums.isco08_index ; new column is added role_ref2 that points to enums.org_roles ; new column is added role_details (JSONB) with content being a dictionary with keys of , for , at , from , with , etc. (all optional), each of which, in its turn, holds a triple-lang structure ( {\"en\": \"\", \"ru\": \"\", \"uk\": \"\"} ). Example: json {\"at\": {\"en\": \"world history department\", \"ru\": \"\u0444\u0430\u043a\u0443\u043b\u044c\u0442\u0435\u0442 \u043c\u0438\u0440\u043e\u0432\u043e\u0439 \u0438\u0441\u0442\u043e\u0440\u0438\u0438\", \"uk\": \"\u0444\u0430\u043a\u0443\u043b\u044c\u0442\u0435\u0442 \u0441\u0432\u0456\u0442\u043e\u0432\u043e\u0457 \u0456\u0441\u0442\u043e\u0440\u0456\u0457\"} The algorithm for populating the closure table would be saved in the wapatools repository ( development/onto/isco08.py ).","title":"Decision"},{"location":"adr/17/#status","text":"In progress.","title":"Status"},{"location":"adr/17/#consequences","text":"The flow for adding a person-to-organization relationship becomes somewhat more complicated, since it requires identification of the profession code. To simplify this, a SQL function search_in_isco8_index was implemented that allows full text search on the isco_08_index table returning term_id (i.e. category ID in isco_08_taxonomy ), index_id (i.e. ID of the profession in isco_08_index ), as well as codes and definitions for reference. Existing reference (content of the role column) is to be reworked into the updated format (in progress). Taxonomy terms should be translated into Russian and Ukrainian. In the long term, rooting in the existing taxonomies elaborated this deeply, would bring a little more order to the data, allowing to query it more efficiently.","title":"Consequences"},{"location":"adr/18/","text":"Context This set of rules is a meaningful subset of Google's and Mozilla's coding standards. Decision In order to keep codebase clean and organized, the following rules are suggested as a general guideline. Files File structure Use src folder for source files; Use test folder for test files; Use src/js folder for JS files; Use src/css folder for CSS files. File content Use UTF-8 encoding for all files. Import Use following order for import statements: import statements for React components; import statements for libraries; import statements for project files; Exactly 1 (one) blank line must separate existing sections; Import statements must not be line-wrapped; The .js file extension is not optional in import paths and must always be included: ```javascript // Wrong! import '../directory/file'; // Right import '../directory/file.js'; ``` * Technically, ECMAScript specification allows import cycles between ES modules, but don't do it . Export Use named exports everywhere. The export keyword can be applied to a declaration, or used in the export {name}; syntax: ```javascript // Wrong! export default function () { // ... } import name from './file.js'; // Right export function name() { // ... } import {name} from './file.js'; ``` * Exported variables must not be mutated outside of module initialization. ```javascript // Wrong! import {name} from './file.js'; name = 'new name'; ``` * Do not use default exports. Importing modules must give a name to these values, which can lead to inconsistencies in naming across modules. ```javascript // Wrong! export default function () { // ... } ``` File content Naming Use FileName.js notation for JS files, file-name.css and file-name.html for CSS and HTML files Use variableName notation for JS variables and functions Use ClassName notation for JS classes Use CONSTANT_NAME notation for JS constants Give as descriptive a name as possible, within reason. Do not worry about saving horizontal space, as it is far more important to make your code immediately understandable by a new reader. Do not use abbreviations that are ambiguous or unfamiliar to readers outside your project, and do not abbreviate by deleting letters within a word. // Incorrect n // Meaningless. nErr // Ambiguous abbreviation. nCompConns // Ambiguous abbreviation. wgcConnections // Only your group knows what this stands for. pcReader // Lots of things can be abbreviated \"pc\". cstmrId // Deletes internal letters. kSecondsPerDay // Do not use Hungarian notation. // Correct errorCount // No abbreviation. dnsConnectionIndex // Most people know what \"DNS\" stands for. referrerUrl // Ditto for \"URL\". customerId // \"Id\" is both ubiquitous and unlikely to be misunderstood. Formatting Use clang-format. The JavaScript community has invested effort to make sure clang-format \"does the right thing\" on JavaScript files. clang-format has integration with several popular editors. Braces are required for all control structures (i.e. if , else , for , do , while , as well as any others), even if the body contains only a single statement. The first statement of a non-empty block must begin on its own line. // Incorrect if (test) return false; // Incorrect if (test) return false; // Correct if (test) { return false; } One statement per line Each statement is followed by a line-break Semicolons are required Horizontal alignment: discouraged. Consider a future change that needs to touch just one line. This change may leave the formerly-pleasing formatting mangled, and that is allowed. More often, it prompts the coder (perhaps you) to adjust whitespace on nearby lines as well, possibly triggering a cascading series of re-formattings. // Incorrect const a = 1; const bb = 2; const ccc = 3; // Correct const a = 1; const bb = 2; const ccc = 3; Variables Use const for all of your references by default Use let if needed Don't use var One variable per declaration // Incorrect const items = getItems(), goSportsTeam = true, dragonball = 'z'; // Correct const items = getItems(); const goSportsTeam = true; const dragonball = 'z'; Declared when needed, initialized as soon as possible Local variables are not habitually declared at the start of their containing block or block-like construct. Instead, local variables are declared close to the point they are first used (within reason), to minimize their scope. Arrays Do not use the variadic Array constructor. The constructor is error-prone if arguments are added or removed. Use a literal instead. // Incorrect const a1 = new Array(x1, x2, x3); const a2 = new Array(x1, x2); const a3 = new Array(x1); const a4 = new Array(); // This works as expected except for the third case: if x1 is a whole number // then a3 is an array of size x1 where all elements are undefined. // If x1 is any other number, then an exception will be thrown, // and if it is anything else then it will be a single-element array. // Correct const a1 = [x1, x2, x3]; const a2 = [x1, x2]; const a3 = [x1]; const a4 = []; Explicitly allocating an array of a given length using new Array(length) is allowed when appropriate. const a1 = new Array(foo.length); Objects Do not mix quoted and unquoted keys Object literals may represent either structs (with unquoted keys and/or symbols) or dictionaries (with quoted and/or computed keys). Do not mix these key types in a single object literal. // Incorrect { width: 42, // struct-style unquoted key 'maxWidth': 43, // dict-style quoted key } // Correct { width: 42, // struct-style unquoted key maxWidth: 43, // struct-style unquoted key } Enumerations are defined by adding the @enum annotation to an object literal. Additional properties may not be added to an enum after it is defined. Enums must be constant, and all enum values must be deeply immutable. /** * Supported temperature scales. * @enum {string} */ const TemperatureScale = { CELSIUS: 'celsius', FAHRENHEIT: 'fahrenheit', }; Strings Use single quotes for strings Use template strings instead of concatenation // Incorrect const name = 'Bob Parr'; const time = 12; const greeting = 'Good afternoon, ' + name + ', it is ' + time + ' now.'; // Correct const name = 'Bob Parr'; const time = 12; const greeting = `Good afternoon, ${name}, it is ${time} now.`; Do not use line continuations (that is, ending a line inside a string literal with a backslash) in either ordinary or template string literals. Even though ES5 allows this, it can lead to tricky errors if any trailing whitespace comes after the slash, and is less obvious to readers. // Incorrect const longString = 'This is a very long string that \\ far exceeds the 80-column limit. It unfortunately \\ contains long stretches of spaces due to how the \\ continued lines are indented.'; // Correct const longString = 'This is a very long string that ' + 'far exceeds the 80-column limit. It does not ' + 'contain long stretches of spaces since the ' + 'continued lines are indented.'; Functions Parameter name comments should be used whenever the value and method name do not sufficiently convey the meaning, and refactoring the method to be clearer is infeasible. Preferred format: 1) before the value, 2) includes = sign: someFunction(obviousParam, /* shouldRender= */ true, /* name= */ 'hello'); Functions may contain nested function definitions. If it is useful to give the function a name, it should be assigned to a local constant Arrow functions provide a concise function syntax and simplify scoping this for nested functions. Prefer arrow functions over the function keyword, particularly for nested functions /** * Arrow functions can be documented just like normal functions. * @param {number} numParam A number to add. * @param {string} strParam Another number to add that happens to be a string. * @return {number} The sum of the two parameters. */ const moduleLocalFunc = (numParam, strParam) => numParam + Number(strParam); Default parameters are supported but discouraged Optional parameters are permitted using the = operator in the parameter list. Optional parameters must include spaces on both sides of the = operator, be named exactly like required parameters (i.e., not prefixed with opt_ ), use the = suffix in their JSDoc type, come after required parameters, and not use initializers that produce observable side effects. All optional parameters for concrete functions must have default values, even if that value is undefined. In contrast to concrete functions, abstract and interface methods must omit default parameter values. Classes Type names are typically nouns or noun phrases. For example, Request , ImmutableList , or VisibilityMode Interface names may sometimes be adjectives or adjective phrases instead (for example, Readable ) Method names are typically verbs or verb phrases. For example, sendMessage or stop _ Getter and setter methods for properties are never required, but if they are used they should be named getFoo , isFoo or hasFoo for booleans Constructor, static and prototype methods, and properties must be documented with the @constructor , @static , @method , and @type annotations respectively. Constructors are required for all ES6 classes. The constructor may be omitted only if the class is a mixin or a base class. Set all of a concrete object\u2019s fields (i.e. all properties other than methods) in the constructor. Otherwise, VMs\u2019 ability to optimize is constrained. class Foo { constructor() { /** @private @const {!Bar} */ this.bar_ = computeBar(); /** @protected @const {!Baz} */ this.baz = computeBaz(); } } Where it does not interfere with readability, prefer module-local functions over private static methods Do not use JavaScript get and set class properties. They are potentially surprising and difficult to reason about, and have limited support in the compiler. Provide ordinary methods instead. // Incorrect class C { get foo() { return this.foo_; } set foo(val) { this.foo_ = val; } } // Correct class C { getFoo() { return this.foo_; } setFoo(val) { this.foo_ = val; } } JSDoc JSDoc is used on all classes, fields, and methods. /** * Multiple lines of JSDoc text are written here, * wrapped normally. * @param {number} arg A number to do something to. */ function doSomething(arg) { } or, a one-liner: /** @const @private {!Foo} A short bit of JSDoc. */ this.foo_ = foo; JSDoc type annotations are encouraged for all variables and return values, and required for all parameters. /** @type {number} */ const foo = 1; const /** number */ foo = 1; const /** !Array<number> */ data = []; /** * Some description. * @type {!Array<number>} */ const data = []; JSDoc type support Markdown syntax. Use it to add links, emphasis, and other formatting to your comments. Classes, interfaces and records must be documented with a description and any template parameters, implemented interfaces, visibility, or other appropriate tags The class description should provide the reader with enough information to know how and when to use the class, as well as any additional considerations necessary to correctly use the class Textual descriptions may be omitted on the constructor. @constructor and @extends annotations are not used with the class keyword unless the class is being used to declare an @interface or it extends a generic class. /** * A fancier event target that does cool things. * @implements {Iterable<string>} */ class MyFancyTarget extends EventTarget { /** * @param {string} arg1 An argument that makes this more interesting. * @param {!Array<number>} arg2 List of numbers to be processed. */ constructor(arg1, arg2) { // ... } }; /** * Records are also helpful. * @extends {Iterator<TYPE>} * @record * @template TYPE */ class Listable { /** @return {TYPE} The next item in line to be returned. */ next() {} } All enums and typedefs must be documented with appropriate JSDoc tags ( @typedef or @enum ) on the preceding line Public enums and typedefs must also have a description Individual enum items may be documented with a JSDoc comment on the preceding line /** * A useful type union, which is reused often. * @typedef {!Bandersnatch|!BandersnatchType} */ let CoolUnionType; /** * Types of bandersnatches. * @enum {string} */ const BandersnatchType = { /** This kind is really frumious. */ FRUMIOUS: 'frumious', /** The less-frumious kind. */ MANXOME: 'manxome', }; In methods and named functions, parameter and return types must be documented, except in the case of same-signature Return type may be omitted if the function has no non-empty return statements Method, parameter, and return descriptions (but not types) may be omitted if they are obvious If a method overrides a super class method, it must include an @override annotation /** A class that does something. */ class SomeClass extends SomeBaseClass { /** * Operates on an instance of MyClass and returns something. * @param {!MyClass} obj An object that for some reason needs detailed * explanation that spans multiple lines. * @param {!OtherClass} obviousOtherClass * @return {boolean} Whether something occurred. */ someMethod(obj, obviousOtherClass) { } /** @override */ overriddenMethod(param) { } } /** * Demonstrates how top-level functions follow the same rules. This one * makes an array. * @param {TYPE} arg * @return {!Array<TYPE>} * @template TYPE */ function makeArray(arg) { } Always specify template parameters. This way, the compiler can do a better job, and it makes it easier for readers to understand what code does. // Incorrect const /** !Object */ users = {}; const /** !Array */ books = []; const /** !Promise */ response = null; // Correct const /** !Object<string, !User> */ users = {}; const /** !Array<!Book> */ books = []; const /** !Promise<!Response> */ response = null; Status Accepted. Consequences Hopefully, better JavaScript code.","title":"\ud83d\uddf8 ADR-18. JavaScript Coding Standards"},{"location":"adr/18/#context","text":"This set of rules is a meaningful subset of Google's and Mozilla's coding standards.","title":"Context"},{"location":"adr/18/#decision","text":"In order to keep codebase clean and organized, the following rules are suggested as a general guideline.","title":"Decision"},{"location":"adr/18/#files","text":"","title":"Files"},{"location":"adr/18/#file-structure","text":"Use src folder for source files; Use test folder for test files; Use src/js folder for JS files; Use src/css folder for CSS files.","title":"File structure"},{"location":"adr/18/#file-content","text":"Use UTF-8 encoding for all files.","title":"File content"},{"location":"adr/18/#import","text":"Use following order for import statements: import statements for React components; import statements for libraries; import statements for project files; Exactly 1 (one) blank line must separate existing sections; Import statements must not be line-wrapped; The .js file extension is not optional in import paths and must always be included: ```javascript // Wrong! import '../directory/file'; // Right import '../directory/file.js'; ``` * Technically, ECMAScript specification allows import cycles between ES modules, but don't do it .","title":"Import"},{"location":"adr/18/#export","text":"Use named exports everywhere. The export keyword can be applied to a declaration, or used in the export {name}; syntax: ```javascript // Wrong! export default function () { // ... } import name from './file.js'; // Right export function name() { // ... } import {name} from './file.js'; ``` * Exported variables must not be mutated outside of module initialization. ```javascript // Wrong! import {name} from './file.js'; name = 'new name'; ``` * Do not use default exports. Importing modules must give a name to these values, which can lead to inconsistencies in naming across modules. ```javascript // Wrong! export default function () { // ... } ```","title":"Export"},{"location":"adr/18/#file-content_1","text":"","title":"File content"},{"location":"adr/18/#naming","text":"Use FileName.js notation for JS files, file-name.css and file-name.html for CSS and HTML files Use variableName notation for JS variables and functions Use ClassName notation for JS classes Use CONSTANT_NAME notation for JS constants Give as descriptive a name as possible, within reason. Do not worry about saving horizontal space, as it is far more important to make your code immediately understandable by a new reader. Do not use abbreviations that are ambiguous or unfamiliar to readers outside your project, and do not abbreviate by deleting letters within a word. // Incorrect n // Meaningless. nErr // Ambiguous abbreviation. nCompConns // Ambiguous abbreviation. wgcConnections // Only your group knows what this stands for. pcReader // Lots of things can be abbreviated \"pc\". cstmrId // Deletes internal letters. kSecondsPerDay // Do not use Hungarian notation. // Correct errorCount // No abbreviation. dnsConnectionIndex // Most people know what \"DNS\" stands for. referrerUrl // Ditto for \"URL\". customerId // \"Id\" is both ubiquitous and unlikely to be misunderstood.","title":"Naming"},{"location":"adr/18/#formatting","text":"Use clang-format. The JavaScript community has invested effort to make sure clang-format \"does the right thing\" on JavaScript files. clang-format has integration with several popular editors. Braces are required for all control structures (i.e. if , else , for , do , while , as well as any others), even if the body contains only a single statement. The first statement of a non-empty block must begin on its own line. // Incorrect if (test) return false; // Incorrect if (test) return false; // Correct if (test) { return false; } One statement per line Each statement is followed by a line-break Semicolons are required Horizontal alignment: discouraged. Consider a future change that needs to touch just one line. This change may leave the formerly-pleasing formatting mangled, and that is allowed. More often, it prompts the coder (perhaps you) to adjust whitespace on nearby lines as well, possibly triggering a cascading series of re-formattings. // Incorrect const a = 1; const bb = 2; const ccc = 3; // Correct const a = 1; const bb = 2; const ccc = 3;","title":"Formatting"},{"location":"adr/18/#variables","text":"Use const for all of your references by default Use let if needed Don't use var One variable per declaration // Incorrect const items = getItems(), goSportsTeam = true, dragonball = 'z'; // Correct const items = getItems(); const goSportsTeam = true; const dragonball = 'z'; Declared when needed, initialized as soon as possible Local variables are not habitually declared at the start of their containing block or block-like construct. Instead, local variables are declared close to the point they are first used (within reason), to minimize their scope.","title":"Variables"},{"location":"adr/18/#arrays","text":"Do not use the variadic Array constructor. The constructor is error-prone if arguments are added or removed. Use a literal instead. // Incorrect const a1 = new Array(x1, x2, x3); const a2 = new Array(x1, x2); const a3 = new Array(x1); const a4 = new Array(); // This works as expected except for the third case: if x1 is a whole number // then a3 is an array of size x1 where all elements are undefined. // If x1 is any other number, then an exception will be thrown, // and if it is anything else then it will be a single-element array. // Correct const a1 = [x1, x2, x3]; const a2 = [x1, x2]; const a3 = [x1]; const a4 = []; Explicitly allocating an array of a given length using new Array(length) is allowed when appropriate. const a1 = new Array(foo.length);","title":"Arrays"},{"location":"adr/18/#objects","text":"Do not mix quoted and unquoted keys Object literals may represent either structs (with unquoted keys and/or symbols) or dictionaries (with quoted and/or computed keys). Do not mix these key types in a single object literal. // Incorrect { width: 42, // struct-style unquoted key 'maxWidth': 43, // dict-style quoted key } // Correct { width: 42, // struct-style unquoted key maxWidth: 43, // struct-style unquoted key } Enumerations are defined by adding the @enum annotation to an object literal. Additional properties may not be added to an enum after it is defined. Enums must be constant, and all enum values must be deeply immutable. /** * Supported temperature scales. * @enum {string} */ const TemperatureScale = { CELSIUS: 'celsius', FAHRENHEIT: 'fahrenheit', };","title":"Objects"},{"location":"adr/18/#strings","text":"Use single quotes for strings Use template strings instead of concatenation // Incorrect const name = 'Bob Parr'; const time = 12; const greeting = 'Good afternoon, ' + name + ', it is ' + time + ' now.'; // Correct const name = 'Bob Parr'; const time = 12; const greeting = `Good afternoon, ${name}, it is ${time} now.`; Do not use line continuations (that is, ending a line inside a string literal with a backslash) in either ordinary or template string literals. Even though ES5 allows this, it can lead to tricky errors if any trailing whitespace comes after the slash, and is less obvious to readers. // Incorrect const longString = 'This is a very long string that \\ far exceeds the 80-column limit. It unfortunately \\ contains long stretches of spaces due to how the \\ continued lines are indented.'; // Correct const longString = 'This is a very long string that ' + 'far exceeds the 80-column limit. It does not ' + 'contain long stretches of spaces since the ' + 'continued lines are indented.';","title":"Strings"},{"location":"adr/18/#functions","text":"Parameter name comments should be used whenever the value and method name do not sufficiently convey the meaning, and refactoring the method to be clearer is infeasible. Preferred format: 1) before the value, 2) includes = sign: someFunction(obviousParam, /* shouldRender= */ true, /* name= */ 'hello'); Functions may contain nested function definitions. If it is useful to give the function a name, it should be assigned to a local constant Arrow functions provide a concise function syntax and simplify scoping this for nested functions. Prefer arrow functions over the function keyword, particularly for nested functions /** * Arrow functions can be documented just like normal functions. * @param {number} numParam A number to add. * @param {string} strParam Another number to add that happens to be a string. * @return {number} The sum of the two parameters. */ const moduleLocalFunc = (numParam, strParam) => numParam + Number(strParam); Default parameters are supported but discouraged Optional parameters are permitted using the = operator in the parameter list. Optional parameters must include spaces on both sides of the = operator, be named exactly like required parameters (i.e., not prefixed with opt_ ), use the = suffix in their JSDoc type, come after required parameters, and not use initializers that produce observable side effects. All optional parameters for concrete functions must have default values, even if that value is undefined. In contrast to concrete functions, abstract and interface methods must omit default parameter values.","title":"Functions"},{"location":"adr/18/#classes","text":"Type names are typically nouns or noun phrases. For example, Request , ImmutableList , or VisibilityMode Interface names may sometimes be adjectives or adjective phrases instead (for example, Readable ) Method names are typically verbs or verb phrases. For example, sendMessage or stop _ Getter and setter methods for properties are never required, but if they are used they should be named getFoo , isFoo or hasFoo for booleans Constructor, static and prototype methods, and properties must be documented with the @constructor , @static , @method , and @type annotations respectively. Constructors are required for all ES6 classes. The constructor may be omitted only if the class is a mixin or a base class. Set all of a concrete object\u2019s fields (i.e. all properties other than methods) in the constructor. Otherwise, VMs\u2019 ability to optimize is constrained. class Foo { constructor() { /** @private @const {!Bar} */ this.bar_ = computeBar(); /** @protected @const {!Baz} */ this.baz = computeBaz(); } } Where it does not interfere with readability, prefer module-local functions over private static methods Do not use JavaScript get and set class properties. They are potentially surprising and difficult to reason about, and have limited support in the compiler. Provide ordinary methods instead. // Incorrect class C { get foo() { return this.foo_; } set foo(val) { this.foo_ = val; } } // Correct class C { getFoo() { return this.foo_; } setFoo(val) { this.foo_ = val; } }","title":"Classes"},{"location":"adr/18/#jsdoc","text":"JSDoc is used on all classes, fields, and methods. /** * Multiple lines of JSDoc text are written here, * wrapped normally. * @param {number} arg A number to do something to. */ function doSomething(arg) { } or, a one-liner: /** @const @private {!Foo} A short bit of JSDoc. */ this.foo_ = foo; JSDoc type annotations are encouraged for all variables and return values, and required for all parameters. /** @type {number} */ const foo = 1; const /** number */ foo = 1; const /** !Array<number> */ data = []; /** * Some description. * @type {!Array<number>} */ const data = []; JSDoc type support Markdown syntax. Use it to add links, emphasis, and other formatting to your comments. Classes, interfaces and records must be documented with a description and any template parameters, implemented interfaces, visibility, or other appropriate tags The class description should provide the reader with enough information to know how and when to use the class, as well as any additional considerations necessary to correctly use the class Textual descriptions may be omitted on the constructor. @constructor and @extends annotations are not used with the class keyword unless the class is being used to declare an @interface or it extends a generic class. /** * A fancier event target that does cool things. * @implements {Iterable<string>} */ class MyFancyTarget extends EventTarget { /** * @param {string} arg1 An argument that makes this more interesting. * @param {!Array<number>} arg2 List of numbers to be processed. */ constructor(arg1, arg2) { // ... } }; /** * Records are also helpful. * @extends {Iterator<TYPE>} * @record * @template TYPE */ class Listable { /** @return {TYPE} The next item in line to be returned. */ next() {} } All enums and typedefs must be documented with appropriate JSDoc tags ( @typedef or @enum ) on the preceding line Public enums and typedefs must also have a description Individual enum items may be documented with a JSDoc comment on the preceding line /** * A useful type union, which is reused often. * @typedef {!Bandersnatch|!BandersnatchType} */ let CoolUnionType; /** * Types of bandersnatches. * @enum {string} */ const BandersnatchType = { /** This kind is really frumious. */ FRUMIOUS: 'frumious', /** The less-frumious kind. */ MANXOME: 'manxome', }; In methods and named functions, parameter and return types must be documented, except in the case of same-signature Return type may be omitted if the function has no non-empty return statements Method, parameter, and return descriptions (but not types) may be omitted if they are obvious If a method overrides a super class method, it must include an @override annotation /** A class that does something. */ class SomeClass extends SomeBaseClass { /** * Operates on an instance of MyClass and returns something. * @param {!MyClass} obj An object that for some reason needs detailed * explanation that spans multiple lines. * @param {!OtherClass} obviousOtherClass * @return {boolean} Whether something occurred. */ someMethod(obj, obviousOtherClass) { } /** @override */ overriddenMethod(param) { } } /** * Demonstrates how top-level functions follow the same rules. This one * makes an array. * @param {TYPE} arg * @return {!Array<TYPE>} * @template TYPE */ function makeArray(arg) { } Always specify template parameters. This way, the compiler can do a better job, and it makes it easier for readers to understand what code does. // Incorrect const /** !Object */ users = {}; const /** !Array */ books = []; const /** !Promise */ response = null; // Correct const /** !Object<string, !User> */ users = {}; const /** !Array<!Book> */ books = []; const /** !Promise<!Response> */ response = null;","title":"JSDoc"},{"location":"adr/18/#status","text":"Accepted.","title":"Status"},{"location":"adr/18/#consequences","text":"Hopefully, better JavaScript code.","title":"Consequences"},{"location":"adr/19/","text":"Context Organization is one of the major entities of the project. The initial way of clustering the organizations on record was implemented with the organization_type table, which included 33 types derived from available data. This, of course, soon proved to be suboptimal, as it wasn't properly reflecting the actual situation. Decision It was decided to create a proper taxonomy following the closure pattern, for which purpose tables org_taxonomy and orgs_taxonomy_closure were created in enums schema. The new taxonomy is based on the ISIC Rev. 4 ( International Standard Industrial Classification of All Economic Activities ), with economic activities converted into organization types, and some important missing ones added, - such as criminal or military organizations. Overall, 297 types are identified as of October 10, 2023. The overall structure is hierarchical with following nuances: Top-level types are as abstract as possible. The term includes word organizations , for example, Advertising and market research organizations or Delivery organizations . Type's place in the hierarchy is reflected in the length of the code, adding 2 characters per level. In effect, top-level concepts have the shortest codes ( A1 , D5 , etc), where the letter comes from the type's name, and digit is a consequtive number of top-level types for that letter. Subsequent levels are coded with digits only. This implies a restriction on the number of top-level types that could be added - no more than 9 per letter. Admittedly, this is quite a lot (top-level types are hard to miss, and max we have currently is S7 , with the other letter being significantly less occupied), but this is still a drawback. There is a special top-level catch-all category, Other organizations with code O0 and no subtypes. Every level starting with 2nd may optionally have an Other category, which should be indicated with 00 level code. For example, Criminal organizations type has code C2 with a number of subtypes ( Gangs is C201 , etc.) and a dedicated catch-all type Other criminal organizations with code C200 . Initial placement of types/subtypes is largely alphabetical, but the new ones should be added strictly at the end of a relevant category with no regard for the alphabet. Codes can be used to figure out the relationships between types, but closure table provides a better way of doing it. Consult DVE-A-78 for information on how to work with closure tables. New types should not be added into the taxonomy table manually, because closure relationships won't be registered then. Use script orgs_taxonomy_append in the taxo module of wapatools repo to do it properly. Status Accepted Consequences The described approach provides a better way of handling different kinds of organizations than the previous one, but it's far from perfect. This should be revisited in the future, preferrably with a team of people.","title":"\ud83d\uddf8 ADR-19. Taxonomies. Organization Types"},{"location":"adr/19/#context","text":"Organization is one of the major entities of the project. The initial way of clustering the organizations on record was implemented with the organization_type table, which included 33 types derived from available data. This, of course, soon proved to be suboptimal, as it wasn't properly reflecting the actual situation.","title":"Context"},{"location":"adr/19/#decision","text":"It was decided to create a proper taxonomy following the closure pattern, for which purpose tables org_taxonomy and orgs_taxonomy_closure were created in enums schema. The new taxonomy is based on the ISIC Rev. 4 ( International Standard Industrial Classification of All Economic Activities ), with economic activities converted into organization types, and some important missing ones added, - such as criminal or military organizations. Overall, 297 types are identified as of October 10, 2023. The overall structure is hierarchical with following nuances: Top-level types are as abstract as possible. The term includes word organizations , for example, Advertising and market research organizations or Delivery organizations . Type's place in the hierarchy is reflected in the length of the code, adding 2 characters per level. In effect, top-level concepts have the shortest codes ( A1 , D5 , etc), where the letter comes from the type's name, and digit is a consequtive number of top-level types for that letter. Subsequent levels are coded with digits only. This implies a restriction on the number of top-level types that could be added - no more than 9 per letter. Admittedly, this is quite a lot (top-level types are hard to miss, and max we have currently is S7 , with the other letter being significantly less occupied), but this is still a drawback. There is a special top-level catch-all category, Other organizations with code O0 and no subtypes. Every level starting with 2nd may optionally have an Other category, which should be indicated with 00 level code. For example, Criminal organizations type has code C2 with a number of subtypes ( Gangs is C201 , etc.) and a dedicated catch-all type Other criminal organizations with code C200 . Initial placement of types/subtypes is largely alphabetical, but the new ones should be added strictly at the end of a relevant category with no regard for the alphabet. Codes can be used to figure out the relationships between types, but closure table provides a better way of doing it. Consult DVE-A-78 for information on how to work with closure tables. New types should not be added into the taxonomy table manually, because closure relationships won't be registered then. Use script orgs_taxonomy_append in the taxo module of wapatools repo to do it properly.","title":"Decision"},{"location":"adr/19/#status","text":"Accepted","title":"Status"},{"location":"adr/19/#consequences","text":"The described approach provides a better way of handling different kinds of organizations than the previous one, but it's far from perfect. This should be revisited in the future, preferrably with a team of people.","title":"Consequences"},{"location":"adr/2/","text":"Context Security is always quite a concern these days, and it is particularly important because the project may become a subject of interest from unfriendly people. As one measure to enhance security, a secrets management solution is to be introduced into the project setup. Decision Doppler.com is an established service provider, with existing integrations with GitHub Actions, Supabase, etc., and exceptionally nice tech support, according to whom integration with Railway would be planned as soon as they release their API (some time in the next couple of months). UPD: Integration with Railway has, indeed, been released, and worked fine ever since. Account was opened on a free tier, and necessary integrations were set up with GitHub, Railway and PyCharm. Status Accepted Consequence It might be a pain if there's ever a need to switch to a different service provider. See DVE-A-5","title":"\ud83d\uddf6\u227d ADR-2. Secrets Management with Doppler [1]"},{"location":"adr/2/#context","text":"Security is always quite a concern these days, and it is particularly important because the project may become a subject of interest from unfriendly people. As one measure to enhance security, a secrets management solution is to be introduced into the project setup.","title":"Context"},{"location":"adr/2/#decision","text":"Doppler.com is an established service provider, with existing integrations with GitHub Actions, Supabase, etc., and exceptionally nice tech support, according to whom integration with Railway would be planned as soon as they release their API (some time in the next couple of months). UPD: Integration with Railway has, indeed, been released, and worked fine ever since. Account was opened on a free tier, and necessary integrations were set up with GitHub, Railway and PyCharm.","title":"Decision"},{"location":"adr/2/#status","text":"Accepted","title":"Status"},{"location":"adr/2/#consequence","text":"It might be a pain if there's ever a need to switch to a different service provider. See DVE-A-5","title":"Consequence"},{"location":"adr/20/","text":"Context Russian propaganda scene is pretty diverse, with several thousand individuals commenting the everchanging stream of events in a variety of voices and accents, offering all kinds of emotional and intellectual fillers to the audience - yet, a closer examination shows that the entirety of its content can be reduced to several hundred rather specific ideas (or, concepts, as they would be called here) that are repeated in various forms and combinations over and over again. At the same time, the project requires a structure against which the body of accumulated text could be analyzed. The main idea here is that since every media fragment (appearance on a talk-show, a published article, etc.) contributes to at least one of these concepts (usually, more than one), we can map existing (proven) contributions to known concepts. And, because we treat these concepts as first-grade citizens in the system, we can develop them further, providing extensive definitions and notes, as well as numerical measure of harm. Decision RUCR, aka Ruskymir Creed, is an attempt to exhaustively list all these concepts, and to define possible relationships between them. SKOS RUCR from the start was being developed with the SKOS framework in mind. SKOS ( Simple Knowledge Organization System ) is an OWL-based data model for sharing and linking knowledge organization systems via the Web ; it's been a W3 Consortium recommendation for quite a while now. For the purposes of this document, let it be known that SKOS works with entities such as Concept , ConceptScheme and Collection , and 2 major types of relationships between them: the hierarchical is defined with broader , narrower , broaderTransitive and narrowerTransitive properties; the 2nd kind is defined with a symmetric related property. XML However, the initial take on it was made using a visualization tool https://app.diagrams.net , which allows, on the one hand, creation of nicely looking diagrams, and, on the other, saving those diagrams in XML format. A special flow was developed around these XML diagrams, allowing automatic generation of RU and UK versions from the EN version. These diagrams would then be presented for public access on the project's website. The initial versions of the diagrams offer a limited visualizations experience, in the sense that only primary broader-narrower relationships are shown, to avoid visual clutter. Database Table rucr_taxonomy was created in the enums schema to provide persistent storage. Each concept has 3 lang-based content versions, set of tags (correspond to Collection s in SKOS), xml_id and xml_data for tying DB entities to XML entities, and status , which is verified for all proper concepts. owlready2 owlready2 is a Python library that allows working with OWL ontologies. General SKOS entities and properties are defined in the skos_general.py script as Python classes inheriting from either Thing or Property base class, and defining a relationship to a corresponding SKOS entity (SKOS itself is loaded and attached to the primary ontology as imported ). Names of some classes are altered (with respect to original SKOS) to better reflect the nuances of the relationship, specifically, broader is defined as HasBroader , narrower - as HasNarrower , related - as IsRelatedTo and so on. RUCR concepts are explicitly listed in the rucr_skos.py file with the following basic structure: Collections are defined at the top; then follow top concepts; then the rest of the concepts. Primary broaderTransitive relationships (they are used instead of simple broader everywhere) are defined immediately after the concept declaration, which forces their order to some degree. At the bottom, related connections are defined in the same order concepts are defined above. Status In progress Consequences","title":"\u221e ADR-20. Ruskymir Creed [RUCR]"},{"location":"adr/20/#context","text":"Russian propaganda scene is pretty diverse, with several thousand individuals commenting the everchanging stream of events in a variety of voices and accents, offering all kinds of emotional and intellectual fillers to the audience - yet, a closer examination shows that the entirety of its content can be reduced to several hundred rather specific ideas (or, concepts, as they would be called here) that are repeated in various forms and combinations over and over again. At the same time, the project requires a structure against which the body of accumulated text could be analyzed. The main idea here is that since every media fragment (appearance on a talk-show, a published article, etc.) contributes to at least one of these concepts (usually, more than one), we can map existing (proven) contributions to known concepts. And, because we treat these concepts as first-grade citizens in the system, we can develop them further, providing extensive definitions and notes, as well as numerical measure of harm.","title":"Context"},{"location":"adr/20/#decision","text":"RUCR, aka Ruskymir Creed, is an attempt to exhaustively list all these concepts, and to define possible relationships between them.","title":"Decision"},{"location":"adr/20/#skos","text":"RUCR from the start was being developed with the SKOS framework in mind. SKOS ( Simple Knowledge Organization System ) is an OWL-based data model for sharing and linking knowledge organization systems via the Web ; it's been a W3 Consortium recommendation for quite a while now. For the purposes of this document, let it be known that SKOS works with entities such as Concept , ConceptScheme and Collection , and 2 major types of relationships between them: the hierarchical is defined with broader , narrower , broaderTransitive and narrowerTransitive properties; the 2nd kind is defined with a symmetric related property.","title":"SKOS"},{"location":"adr/20/#xml","text":"However, the initial take on it was made using a visualization tool https://app.diagrams.net , which allows, on the one hand, creation of nicely looking diagrams, and, on the other, saving those diagrams in XML format. A special flow was developed around these XML diagrams, allowing automatic generation of RU and UK versions from the EN version. These diagrams would then be presented for public access on the project's website. The initial versions of the diagrams offer a limited visualizations experience, in the sense that only primary broader-narrower relationships are shown, to avoid visual clutter.","title":"XML"},{"location":"adr/20/#database","text":"Table rucr_taxonomy was created in the enums schema to provide persistent storage. Each concept has 3 lang-based content versions, set of tags (correspond to Collection s in SKOS), xml_id and xml_data for tying DB entities to XML entities, and status , which is verified for all proper concepts.","title":"Database"},{"location":"adr/20/#owlready2","text":"owlready2 is a Python library that allows working with OWL ontologies. General SKOS entities and properties are defined in the skos_general.py script as Python classes inheriting from either Thing or Property base class, and defining a relationship to a corresponding SKOS entity (SKOS itself is loaded and attached to the primary ontology as imported ). Names of some classes are altered (with respect to original SKOS) to better reflect the nuances of the relationship, specifically, broader is defined as HasBroader , narrower - as HasNarrower , related - as IsRelatedTo and so on. RUCR concepts are explicitly listed in the rucr_skos.py file with the following basic structure: Collections are defined at the top; then follow top concepts; then the rest of the concepts. Primary broaderTransitive relationships (they are used instead of simple broader everywhere) are defined immediately after the concept declaration, which forces their order to some degree. At the bottom, related connections are defined in the same order concepts are defined above.","title":"owlready2"},{"location":"adr/20/#status","text":"In progress","title":"Status"},{"location":"adr/20/#consequences","text":"","title":"Consequences"},{"location":"adr/21/","text":"Context In the context of data management, provenance refers to the metadata that describes the origin and processing history of a dataset . It is helpful for establishing the reliability and trustworthiness of the data. This is something that should've been introduced from the beginning, but better late than never. Decision Persistent metadata Table provenance was created in the service schema of the master DB with the following structure: Field name Data type Notes uid UUID operation TEXT tbl TEXT schm TEXT agent TEXT happenned_at TIMESTAMP affected_row_id INTEGER updated JSONB Relevant for updates only. Contains names of updated keys and their old value inserted TEXT[] Relevant for inserts only. Contains an array of non-null column names Mechanism To populate the provenance table automatically, the following generic trigger function was developed using PL/Python procedural language: CREATE OR REPLACE FUNCTION trg_change() RETURNS TRIGGER AS $$ import json operation = TD[\"event\"].lower() old_rec = TD[\"old\"] if operation in(\"update\", \"delete\") else None new_rec = TD[\"new\"] if operation in(\"update\", \"insert\") else None tbl = TD[\"table_name\"] sch = TD[\"table_schema\"] current_data_q = plpy.prepare(\"SELECT current_user, current_timestamp;\") current_data = plpy.execute(current_data_q) current_user, current_timestamp = current_data[0].values() if operation == \"insert\": rec_id = new_rec.get(\"id\") or new_rec.get(\"uid\") notnull = [k for k,v in new_rec.items() if v] query = plpy.prepare(\"INSERT INTO service.provenance (operation, tbl, schm, agent, happenned_at, affected_row_id, inserted) VALUES($1, $2, $3, $4, $5, $6, $7)\", [\"tbl_operation\", \"text\", \"text\", \"text\", \"timestamp\", \"int\", \"text[]\"]) plpy.execute(query, [operation, tbl, sch, current_user, current_timestamp, rec_id, notnull]) elif operation == \"update\": rec_id = new_rec.get(\"id\") or new_rec.get(\"uid\") updated = dict() for k, val in old_rec.items(): if new_rec[k] != val: updated[k] = val query = plpy.prepare(\"INSERT INTO service.provenance (operation, tbl, schm, agent, happenned_at, affected_row_id, updated) VALUES($1, $2, $3, $4, $5, $6, $7)\", [\"tbl_operation\", \"text\", \"text\", \"text\", \"timestamp\", \"int\", \"jsonb\"]) plpy.execute(query, [operation, tbl, sch, current_user, current_timestamp, rec_id, json.dumps(updated)]) elif operation == \"delete\": rec_id = old_rec.get(\"id\") or old_rec.get(\"uid\") query = plpy.prepare(\"INSERT INTO service.provenance (operation, tbl, schm, agent, happenned_at, affected_row_id) VALUES($1, $2, $3, $4, $5, $6)\", [\"tbl_operation\", \"text\", \"text\", \"text\", \"timestamp\", \"int\"]) plpy.execute(query, [operation, tbl, sch, current_user, current_timestamp, rec_id]) $$ LANGUAGE plpython3u; With this, any table that requires data provenance can be hooked up by creating a single trigger for it: CREATE OR REPLACE TRIGGER trg_<table_name> AFTER UPDATE OR DELETE OR INSERT ON <schema_name.table_name> FOR EACH ROW EXECUTE FUNCTION trg_change(); And, as an additional safety mechanism, the following meta-trigger was set up, as well: CREATE OR REPLACE TRIGGER trg_prov AFTER UPDATE OR DELETE ON service.provenance FOR EACH ROW EXECUTE FUNCTION trg_change(); Meaning, that if anybody would attempt to alter or remove a row from the provenance table, it would be registered in the provenance table via the usual flow. Note, that insert operation is omitted from this one. If added, it would create an infinite loop of database transactions. INSERT Exceptions There are other tables, for which INSERT operation is not registered in the provenance table: lists of media, such as youtube_vids , and data tables, such as telegram_messages - because inserts here are done by automated scripts which have their own logs and usually insert in bulk. Here's the full list of such tables: public: dentv_episodes, komso_episodes, ntv_episodes, rutube_vids, smotrim_episodes, text_media, youtube_vids data: printed_content, telegram_channel_stats, telegram_messages, transcribed_content, transcribed_content_translation_en, transcripts future: meduza_dow_stream, rodniki, e_mash_mapdata service: factory_jobs_run_details, media_segments_stats enums: orgs_taxonomy_closure, isco08_taxonomy_closure (see below) Closures Relevant for tables whose names end with _closure . Closure tables don't have id columns, so the standard trigger won't work. Another version of the above trigger function was customized and saved under name trg_change_closure . It is different in 2 respects: It doesn't have code for handling INSERT operations; For update operation, instead of updated columns, the entire OLD record is saved. Test rows During the development of the flow, 5 rows were inserted into the provenance table that do not conform to the established standard: uid reasoning what's wrong with it 71a5cefc-d84b-469f-88bb-d4ed67b71835 This is a genuine update made for testing purposes. Updated field: known_for . Previous value was null user: postgres - generic superuser should not be used for regular operations ` updated: null` - this record was inserted by an earlier version of the trigger function that didn't account for updated columns 84942cc2-0bab-4fc8-847a-2bfdb4e892fa This is a genuine update made for testing purposes. Updated field: known_for . Previous value was null user: postgres - generic superuser should not be used for regular operations ` updated: null` - this record was inserted by an earlier version of the trigger function that didn't account for updated columns e742763a-dc19-45c0-b715-2323531eddc5 This is a genuine update made for testing purposes. Updated field: known_for . Previous value was null user: postgres - generic superuser should not be used for regular operations a2f7165f-4a4a-4305-b6f7-78e0afd59c32 This is a dummy update: a test record was inserted into the table organizations from under the ata user; the insert operation triggered creation of the provenance record. Was inserted purely for testing purposes 6f4ae1a6-bb43-400c-840c-db02e711cf0e This is a dummy update, and refers to the removal of the test record inserted into the organizations table (see a2f7165f-4a4a-4305-b6f7-78e0afd59c32) Was inserted purely for testing purposes They were not removed, because nothing should be removed from the provenance table. Status The complete list of tables for which provenance will be enabled can be found in DVE-266 https://subjective.youtrack.cloud/issue/ As of the moment of submitting this, triggers are set up for people and organizations tables in the public schema of the master DB. Consequences Provenance mechanism is for the people to trust the project data a little bit more.","title":"\ud83d\uddf8 ADR-21. Data Provenance"},{"location":"adr/21/#context","text":"In the context of data management, provenance refers to the metadata that describes the origin and processing history of a dataset . It is helpful for establishing the reliability and trustworthiness of the data. This is something that should've been introduced from the beginning, but better late than never.","title":"Context"},{"location":"adr/21/#decision","text":"","title":"Decision"},{"location":"adr/21/#persistent-metadata","text":"Table provenance was created in the service schema of the master DB with the following structure: Field name Data type Notes uid UUID operation TEXT tbl TEXT schm TEXT agent TEXT happenned_at TIMESTAMP affected_row_id INTEGER updated JSONB Relevant for updates only. Contains names of updated keys and their old value inserted TEXT[] Relevant for inserts only. Contains an array of non-null column names","title":"Persistent metadata"},{"location":"adr/21/#mechanism","text":"To populate the provenance table automatically, the following generic trigger function was developed using PL/Python procedural language: CREATE OR REPLACE FUNCTION trg_change() RETURNS TRIGGER AS $$ import json operation = TD[\"event\"].lower() old_rec = TD[\"old\"] if operation in(\"update\", \"delete\") else None new_rec = TD[\"new\"] if operation in(\"update\", \"insert\") else None tbl = TD[\"table_name\"] sch = TD[\"table_schema\"] current_data_q = plpy.prepare(\"SELECT current_user, current_timestamp;\") current_data = plpy.execute(current_data_q) current_user, current_timestamp = current_data[0].values() if operation == \"insert\": rec_id = new_rec.get(\"id\") or new_rec.get(\"uid\") notnull = [k for k,v in new_rec.items() if v] query = plpy.prepare(\"INSERT INTO service.provenance (operation, tbl, schm, agent, happenned_at, affected_row_id, inserted) VALUES($1, $2, $3, $4, $5, $6, $7)\", [\"tbl_operation\", \"text\", \"text\", \"text\", \"timestamp\", \"int\", \"text[]\"]) plpy.execute(query, [operation, tbl, sch, current_user, current_timestamp, rec_id, notnull]) elif operation == \"update\": rec_id = new_rec.get(\"id\") or new_rec.get(\"uid\") updated = dict() for k, val in old_rec.items(): if new_rec[k] != val: updated[k] = val query = plpy.prepare(\"INSERT INTO service.provenance (operation, tbl, schm, agent, happenned_at, affected_row_id, updated) VALUES($1, $2, $3, $4, $5, $6, $7)\", [\"tbl_operation\", \"text\", \"text\", \"text\", \"timestamp\", \"int\", \"jsonb\"]) plpy.execute(query, [operation, tbl, sch, current_user, current_timestamp, rec_id, json.dumps(updated)]) elif operation == \"delete\": rec_id = old_rec.get(\"id\") or old_rec.get(\"uid\") query = plpy.prepare(\"INSERT INTO service.provenance (operation, tbl, schm, agent, happenned_at, affected_row_id) VALUES($1, $2, $3, $4, $5, $6)\", [\"tbl_operation\", \"text\", \"text\", \"text\", \"timestamp\", \"int\"]) plpy.execute(query, [operation, tbl, sch, current_user, current_timestamp, rec_id]) $$ LANGUAGE plpython3u; With this, any table that requires data provenance can be hooked up by creating a single trigger for it: CREATE OR REPLACE TRIGGER trg_<table_name> AFTER UPDATE OR DELETE OR INSERT ON <schema_name.table_name> FOR EACH ROW EXECUTE FUNCTION trg_change(); And, as an additional safety mechanism, the following meta-trigger was set up, as well: CREATE OR REPLACE TRIGGER trg_prov AFTER UPDATE OR DELETE ON service.provenance FOR EACH ROW EXECUTE FUNCTION trg_change(); Meaning, that if anybody would attempt to alter or remove a row from the provenance table, it would be registered in the provenance table via the usual flow. Note, that insert operation is omitted from this one. If added, it would create an infinite loop of database transactions.","title":"Mechanism"},{"location":"adr/21/#insert-exceptions","text":"There are other tables, for which INSERT operation is not registered in the provenance table: lists of media, such as youtube_vids , and data tables, such as telegram_messages - because inserts here are done by automated scripts which have their own logs and usually insert in bulk. Here's the full list of such tables: public: dentv_episodes, komso_episodes, ntv_episodes, rutube_vids, smotrim_episodes, text_media, youtube_vids data: printed_content, telegram_channel_stats, telegram_messages, transcribed_content, transcribed_content_translation_en, transcripts future: meduza_dow_stream, rodniki, e_mash_mapdata service: factory_jobs_run_details, media_segments_stats enums: orgs_taxonomy_closure, isco08_taxonomy_closure (see below)","title":"INSERT Exceptions"},{"location":"adr/21/#closures","text":"Relevant for tables whose names end with _closure . Closure tables don't have id columns, so the standard trigger won't work. Another version of the above trigger function was customized and saved under name trg_change_closure . It is different in 2 respects: It doesn't have code for handling INSERT operations; For update operation, instead of updated columns, the entire OLD record is saved.","title":"Closures"},{"location":"adr/21/#test-rows","text":"During the development of the flow, 5 rows were inserted into the provenance table that do not conform to the established standard: uid reasoning what's wrong with it 71a5cefc-d84b-469f-88bb-d4ed67b71835 This is a genuine update made for testing purposes. Updated field: known_for . Previous value was null user: postgres - generic superuser should not be used for regular operations ` updated: null` - this record was inserted by an earlier version of the trigger function that didn't account for updated columns 84942cc2-0bab-4fc8-847a-2bfdb4e892fa This is a genuine update made for testing purposes. Updated field: known_for . Previous value was null user: postgres - generic superuser should not be used for regular operations ` updated: null` - this record was inserted by an earlier version of the trigger function that didn't account for updated columns e742763a-dc19-45c0-b715-2323531eddc5 This is a genuine update made for testing purposes. Updated field: known_for . Previous value was null user: postgres - generic superuser should not be used for regular operations a2f7165f-4a4a-4305-b6f7-78e0afd59c32 This is a dummy update: a test record was inserted into the table organizations from under the ata user; the insert operation triggered creation of the provenance record. Was inserted purely for testing purposes 6f4ae1a6-bb43-400c-840c-db02e711cf0e This is a dummy update, and refers to the removal of the test record inserted into the organizations table (see a2f7165f-4a4a-4305-b6f7-78e0afd59c32) Was inserted purely for testing purposes They were not removed, because nothing should be removed from the provenance table.","title":"Test rows"},{"location":"adr/21/#status","text":"The complete list of tables for which provenance will be enabled can be found in DVE-266 https://subjective.youtrack.cloud/issue/ As of the moment of submitting this, triggers are set up for people and organizations tables in the public schema of the master DB.","title":"Status"},{"location":"adr/21/#consequences","text":"Provenance mechanism is for the people to trust the project data a little bit more.","title":"Consequences"},{"location":"adr/22/","text":"Context Due to tri-language nature of the project, where English, Russian and Ukrainian languages are treated as top-level citizens, a common pattern arouse in the database usage, where tables would have lang variants of entity names, either saved as text in different columns (like fullname_en , fullname_ru , fullname_uk in people ), or in a JSONB column with en , ru and uk keys. Decision Composite type with name triple_lang was created in December 2023: create type triple_lang as ( en text, ru text, uk text ); Custom type required following adaptations in the code: Django Django handles composite type with a dedicated library django-postgres-composite-types . Given that it's installed and added to Django modules, following classes should be added to the code: from postgres_composite_types import CompositeType # to models class TripleLang(CompositeType): # https://github.com/danni/django-postgres-composite-types \"\"\"Text value in 3 languages: en, uk, ru\"\"\" en = models.CharField() ru = models.CharField() uk = models.CharField() class Meta: db_type = 'triple_lang' # to serializers class TripleLangSerializer(serializers.Serializer): en = serializers.CharField(allow_null=True) ru = serializers.CharField(allow_null=True) uk = serializers.CharField(allow_null=True) class Meta: model = models.TripleLang fields = ('en', 'ru', 'uk') Then it can be used like so: class EnumsISCOIndex(models.Model): id = models.BigAutoField(primary_key=True) created_at = models.DateTimeField(blank=True, null=True) isco08 = models.ForeignKey(EnumsISCOTaxonomy, models.DO_NOTHING, blank=True, null=True) name = TripleLang.Field(blank=True, null=True) appended = models.BooleanField(blank=True, null=True) class Meta: managed = True db_table = 'enums_isco08_index' psycopg With psycopg the only thing necessary is registration of the type after creating a connection object: from psycopg.types.composite import CompositeInfo, register_composite info = CompositeInfo.fetch(connection, \"triple_lang\") register_composite(info, connection) This functionality can be added to the connection creation procedure. SQLAlchemy + Pydantic With the combination of SQLAlchemy and pydantic , extention package sqlalchemy_utils should be used to define CompositeType columns. Given a pydantic model from pydantic import BaseModel class TripleLang(BaseModel): en: Optional[str] ru: Optional[str] uk: Optional[str] A table with triple_lang fields can be defined like so: from datetime import datetime from typing import Optional from pydantic import BaseModel, ConfigDict from sqlalchemy import Column, Integer, Text, DateTime, MetaData from sqlalchemy.orm import relationship from sqlalchemy_utils import CompositeType from wapaganda.database.models.base import Base, TripleLang class BundleTypeDB(Base): metadata = MetaData(schema=\"enums\") __tablename__ = 'bundle_types' __table_args__ = {'extend_existing': True} id = Column(Integer, primary_key=True) created_at = Column(DateTime) code = Column(Text) updated_on = Column(DateTime) description = Column(CompositeType(name=\"triple_lang\", columns=[Column(\"en\", Text), Column(\"ru\", Text), Column(\"uk\", Text)])) bundles = relationship(\"BundleDB\", back_populates=\"bundle_type_\") class BundleTypeDTO(BaseModel): model_config = ConfigDict(from_attributes=True, arbitrary_types_allowed=True) # id: Optional[int] = None created_at: datetime code: str updated_on: Optional[datetime] description: TripleLang With setup like this, on the app level, one needs to register custom types like so: from sqlalchemy_utils import register_composites from wapaganda.database.db_client import DBClient client = DBClient(driver='psycopg2') with client.get_connection() as conn: register_composites(conn) As of January 2024, psycopg2 needs to be used for CompositeType class; newer psycopg isn't supported. Status Accepted Consequences Custom types can be tricky to use, which makes them a possible source of bugs. On the other hand, they enable more flexible work with the db.","title":"\ud83d\uddf8 ADR-22. TripleLang Composite Type"},{"location":"adr/22/#context","text":"Due to tri-language nature of the project, where English, Russian and Ukrainian languages are treated as top-level citizens, a common pattern arouse in the database usage, where tables would have lang variants of entity names, either saved as text in different columns (like fullname_en , fullname_ru , fullname_uk in people ), or in a JSONB column with en , ru and uk keys.","title":"Context"},{"location":"adr/22/#decision","text":"Composite type with name triple_lang was created in December 2023: create type triple_lang as ( en text, ru text, uk text ); Custom type required following adaptations in the code:","title":"Decision"},{"location":"adr/22/#django","text":"Django handles composite type with a dedicated library django-postgres-composite-types . Given that it's installed and added to Django modules, following classes should be added to the code: from postgres_composite_types import CompositeType # to models class TripleLang(CompositeType): # https://github.com/danni/django-postgres-composite-types \"\"\"Text value in 3 languages: en, uk, ru\"\"\" en = models.CharField() ru = models.CharField() uk = models.CharField() class Meta: db_type = 'triple_lang' # to serializers class TripleLangSerializer(serializers.Serializer): en = serializers.CharField(allow_null=True) ru = serializers.CharField(allow_null=True) uk = serializers.CharField(allow_null=True) class Meta: model = models.TripleLang fields = ('en', 'ru', 'uk') Then it can be used like so: class EnumsISCOIndex(models.Model): id = models.BigAutoField(primary_key=True) created_at = models.DateTimeField(blank=True, null=True) isco08 = models.ForeignKey(EnumsISCOTaxonomy, models.DO_NOTHING, blank=True, null=True) name = TripleLang.Field(blank=True, null=True) appended = models.BooleanField(blank=True, null=True) class Meta: managed = True db_table = 'enums_isco08_index'","title":"Django"},{"location":"adr/22/#psycopg","text":"With psycopg the only thing necessary is registration of the type after creating a connection object: from psycopg.types.composite import CompositeInfo, register_composite info = CompositeInfo.fetch(connection, \"triple_lang\") register_composite(info, connection) This functionality can be added to the connection creation procedure.","title":"psycopg"},{"location":"adr/22/#sqlalchemy-pydantic","text":"With the combination of SQLAlchemy and pydantic , extention package sqlalchemy_utils should be used to define CompositeType columns. Given a pydantic model from pydantic import BaseModel class TripleLang(BaseModel): en: Optional[str] ru: Optional[str] uk: Optional[str] A table with triple_lang fields can be defined like so: from datetime import datetime from typing import Optional from pydantic import BaseModel, ConfigDict from sqlalchemy import Column, Integer, Text, DateTime, MetaData from sqlalchemy.orm import relationship from sqlalchemy_utils import CompositeType from wapaganda.database.models.base import Base, TripleLang class BundleTypeDB(Base): metadata = MetaData(schema=\"enums\") __tablename__ = 'bundle_types' __table_args__ = {'extend_existing': True} id = Column(Integer, primary_key=True) created_at = Column(DateTime) code = Column(Text) updated_on = Column(DateTime) description = Column(CompositeType(name=\"triple_lang\", columns=[Column(\"en\", Text), Column(\"ru\", Text), Column(\"uk\", Text)])) bundles = relationship(\"BundleDB\", back_populates=\"bundle_type_\") class BundleTypeDTO(BaseModel): model_config = ConfigDict(from_attributes=True, arbitrary_types_allowed=True) # id: Optional[int] = None created_at: datetime code: str updated_on: Optional[datetime] description: TripleLang With setup like this, on the app level, one needs to register custom types like so: from sqlalchemy_utils import register_composites from wapaganda.database.db_client import DBClient client = DBClient(driver='psycopg2') with client.get_connection() as conn: register_composites(conn) As of January 2024, psycopg2 needs to be used for CompositeType class; newer psycopg isn't supported.","title":"SQLAlchemy + Pydantic"},{"location":"adr/22/#status","text":"Accepted","title":"Status"},{"location":"adr/22/#consequences","text":"Custom types can be tricky to use, which makes them a possible source of bugs. On the other hand, they enable more flexible work with the db.","title":"Consequences"},{"location":"adr/23/","text":"Context This document succeeds and replaces ADR-9 DVE-A-33, and describes approach to maintaining the website in 3 languages (En, Ru, Uk). Decision Data Composite database type triple_lang was created to hold values that are either translatable or transcriptable; it has 3 TEXT fields corresponding the the 3 supported languages. Type adaptation was implemented for both Django and FastAPI-based backends. See more in DVE-A-120. Frontend Website components are translated using native React tools ( useTranslation function), with file i18n.js containing translatable options in resources constant. Status Accepted Consequences triple_lang type allows for more reliable and manageable approach to handling multilang data, both retrival and update. On the other hand, frontend keeping all text in a single file would make it hard to manage as the volume of content grows. TODO: Research possible options.","title":"\ud83d\uddf8 ADR-23. Wapaganda Localization [2]"},{"location":"adr/23/#context","text":"This document succeeds and replaces ADR-9 DVE-A-33, and describes approach to maintaining the website in 3 languages (En, Ru, Uk).","title":"Context"},{"location":"adr/23/#decision","text":"","title":"Decision"},{"location":"adr/23/#data","text":"Composite database type triple_lang was created to hold values that are either translatable or transcriptable; it has 3 TEXT fields corresponding the the 3 supported languages. Type adaptation was implemented for both Django and FastAPI-based backends. See more in DVE-A-120.","title":"Data"},{"location":"adr/23/#frontend","text":"Website components are translated using native React tools ( useTranslation function), with file i18n.js containing translatable options in resources constant.","title":"Frontend"},{"location":"adr/23/#status","text":"Accepted","title":"Status"},{"location":"adr/23/#consequences","text":"triple_lang type allows for more reliable and manageable approach to handling multilang data, both retrival and update. On the other hand, frontend keeping all text in a single file would make it hard to manage as the volume of content grows. TODO: Research possible options.","title":"Consequences"},{"location":"adr/24/","text":"Context Primary project's repository, wapatools , is a shared monorepo that follows the polylith structure. This structure is not directly consistent with the Docker approach, and requires certain additional steps. Decision Services in the W ecosystem can be run either on ==Railway==, or on our own infrastructure. Railway's upside is simplicity and reliability; its downside is only the cost. General Build Procedure A shell-script can be used to perform all actions necessary. Approximate procedure includes the following steps: Define paths to the files required for the project (healthcheck script, log config etc.); Drop old wheel file if it exists; Copy files to the project's directory ( projects/{project_name} ); Run poetry build-project command from the project directory. This would create dist folder in the projects' directory with files that can be used to install the project, including a new wheel file; Get the name of the new wheel file; Modify Dockerfile with the new wheel 's name. This solution relies on that the name of the wheel file is located on a specific line in the Dockerfile; :::warning With services that are supposed to run on Railway , the process stops here. Railway can build their own docker images, thank you very much. ::: :::info The rest of the steps are only required for services that are supposed to run on our own infrastructure. ::: 7. Build Docker image; 8. Tag new image; 9. Push new image to Docker Hub or private repository. Dockerfile :::info Dockerfile is almost the same for both deployment cases. The 2 main differences are: You should install Infisical CLI in non-Railway Dockerfile; When copying wheel file, in Railway one should start in the repository's root, and in self-hosted case - in the project's root. ::: Project's Dockerfile must be composed with these things in mind: Index of the line on which the name of the wheel is defined must be aligned with the builder. If using a template, no changes are necessary; Final image must include Infisical CLI for any application that requires supply of secrets; When using multistage builds, watch out for what components should be copied to the final image. This depends on what exactly was installed; As a rule, last line in the Dockerfile switches work directory to where the main executable is located; As a rule, Dockerfile doesn't include a CMD or entrypoint command, mainly because it would be overridden in the compose file anyway. Deployment Self-Hosted with docker compose compose.yml or docker-compose.yml file is used to launch the service which may be a single container or multiple. Main container's command must start with infisical run -- followed by the path to Python's interpreter + service executable; env_file must be supplied with Infisical's server address and access token . On Railway Make sure the corresponding Infisical project has secret named RAILWAY_DOCKERFILE_PATH set to /projects/{project}/Dockerfile , given that the name of the Dockerfile is not modified. Create a new project on Railway. Set source repo to https://github.com/subjective-agency/wapatools Select development branch or whatever may be the case. In the build section, set Watch Paths to projects/{project_name} . In the deploy section, set Custom Start Command to python {primary_component_name}/core.py Set Region to anywhere in Europe, Runtime to V2 , and Replicas to 1. :::warning At this point Railway would likely attempt to deploy, but fail for lack of secrets. ::: Set up integration with Infisical, then re-deploy. Development Flow Suppose, in the course of development, you finished an important update to the app, and now want to deploy a new version. Here are the steps for this: Commit changes with a meaningful message, as usual. Go to projects/{project_name}/pyproject.toml and bump up the version. Run builder shell script for the application. Commit artifacts created by this, with message {project_name}-{version} , for example, ave-media-0.2.1.8 . Push these 2 commits to the remote origin. At this point, Railway would detect updates in the project directory, and initiate a new build & deploy. Status Active Consequences Setting up a consistent flow around apps dockerization opens a straight path to running apps in our infrastructure, or on Railway.","title":"\ud83d\uddf8 ADR-24. Polylith Projects & Dockerization"},{"location":"adr/24/#context","text":"Primary project's repository, wapatools , is a shared monorepo that follows the polylith structure. This structure is not directly consistent with the Docker approach, and requires certain additional steps.","title":"Context"},{"location":"adr/24/#decision","text":"Services in the W ecosystem can be run either on ==Railway==, or on our own infrastructure. Railway's upside is simplicity and reliability; its downside is only the cost.","title":"Decision"},{"location":"adr/24/#general-build-procedure","text":"A shell-script can be used to perform all actions necessary. Approximate procedure includes the following steps: Define paths to the files required for the project (healthcheck script, log config etc.); Drop old wheel file if it exists; Copy files to the project's directory ( projects/{project_name} ); Run poetry build-project command from the project directory. This would create dist folder in the projects' directory with files that can be used to install the project, including a new wheel file; Get the name of the new wheel file; Modify Dockerfile with the new wheel 's name. This solution relies on that the name of the wheel file is located on a specific line in the Dockerfile; :::warning With services that are supposed to run on Railway , the process stops here. Railway can build their own docker images, thank you very much. ::: :::info The rest of the steps are only required for services that are supposed to run on our own infrastructure. ::: 7. Build Docker image; 8. Tag new image; 9. Push new image to Docker Hub or private repository.","title":"General Build Procedure"},{"location":"adr/24/#dockerfile","text":":::info Dockerfile is almost the same for both deployment cases. The 2 main differences are: You should install Infisical CLI in non-Railway Dockerfile; When copying wheel file, in Railway one should start in the repository's root, and in self-hosted case - in the project's root. ::: Project's Dockerfile must be composed with these things in mind: Index of the line on which the name of the wheel is defined must be aligned with the builder. If using a template, no changes are necessary; Final image must include Infisical CLI for any application that requires supply of secrets; When using multistage builds, watch out for what components should be copied to the final image. This depends on what exactly was installed; As a rule, last line in the Dockerfile switches work directory to where the main executable is located; As a rule, Dockerfile doesn't include a CMD or entrypoint command, mainly because it would be overridden in the compose file anyway.","title":"Dockerfile"},{"location":"adr/24/#deployment","text":"","title":"Deployment"},{"location":"adr/24/#self-hosted-with-docker-compose","text":"compose.yml or docker-compose.yml file is used to launch the service which may be a single container or multiple. Main container's command must start with infisical run -- followed by the path to Python's interpreter + service executable; env_file must be supplied with Infisical's server address and access token .","title":"Self-Hosted with docker compose"},{"location":"adr/24/#on-railway","text":"Make sure the corresponding Infisical project has secret named RAILWAY_DOCKERFILE_PATH set to /projects/{project}/Dockerfile , given that the name of the Dockerfile is not modified. Create a new project on Railway. Set source repo to https://github.com/subjective-agency/wapatools Select development branch or whatever may be the case. In the build section, set Watch Paths to projects/{project_name} . In the deploy section, set Custom Start Command to python {primary_component_name}/core.py Set Region to anywhere in Europe, Runtime to V2 , and Replicas to 1. :::warning At this point Railway would likely attempt to deploy, but fail for lack of secrets. ::: Set up integration with Infisical, then re-deploy.","title":"On Railway"},{"location":"adr/24/#development-flow","text":"Suppose, in the course of development, you finished an important update to the app, and now want to deploy a new version. Here are the steps for this: Commit changes with a meaningful message, as usual. Go to projects/{project_name}/pyproject.toml and bump up the version. Run builder shell script for the application. Commit artifacts created by this, with message {project_name}-{version} , for example, ave-media-0.2.1.8 . Push these 2 commits to the remote origin. At this point, Railway would detect updates in the project directory, and initiate a new build & deploy.","title":"Development Flow"},{"location":"adr/24/#status","text":"Active","title":"Status"},{"location":"adr/24/#consequences","text":"Setting up a consistent flow around apps dockerization opens a straight path to running apps in our infrastructure, or on Railway.","title":"Consequences"},{"location":"adr/25/","text":"Context This document succeeds and replaces ADR-2. It describes the integration of self-hosted Infisical instance as the primary tool for communicating sensitive data to the applications. Decision Infisical is a feature-rich open-source secrets management platform with well-detailed documentation. Wapaganda instance is hosted on a Hetzner server with docker compose . It is accessible on port 50789 at principal 's IP address; its HTTPS host is https://asdfghjkl.subjective.agency (reverse proxied with Caddy). Normally, Infisical's docker compose stack includes 4 services: infisical itself, redis , postgres and db-migration , where db-migration starts with all the rest, and exits once done migrating. In W setup, postgres service is disabled; instead, a dedicated infisical database is set up on the production instance. Setup Each application that requires secrets injection has a project in Infisical. Every project can have multiple environments, but the simplest setup is to have a single dev environment with all secrets required for an app. Railway integration Infisical has native support for Railway. To set it up, Go to Railway, and obtain a new API key. Name it the same as the application. Go to Infisical -> a project you're setting up -> Integrations . Find Railway , click on it, and submit the new API key. In the pop-up, select appropriate environments on both sides. Make sure environments are aligned: you want to connect Prod to Prod, and Dev to Dev. Once the integration is saved, you should see a bunch of secrets in Railway, in the service's Shared Secrets section. Railway integration means, you don't need to add infisical run -- to the start command. Docker apps With non-Railway docker-based applications, integration is enabled with the following procedure: Dockerfile for the application must include instruction for installation of Infisical CLI. For example, here's the corresponding line from the backup service's file (that also installs PostgreSQL and a bunch of other stuff): dockerfile RUN apt-get update && apt-get install -y bash curl tar coreutils postgresql-client && curl -1sLf \\ 'https://dl.cloudsmith.io/public/infisical/infisical-cli/setup.deb.sh' | bash \\ && apt-get update && apt-get install -y infisical 2. Application's start command, whether in Dockerfile or in docker-compose.yml must be modified to start with infisical run -- like so: infisical run -- python backup/core.py 3. In order to the application to connect with Infisical, it needs 2 things: address of the host (since we're self-hosting) and a token. You can generate token for a project in the ==Access Control== tab. These 2 values must be saved in .env file in the same folder where you have app's primary docker compose file. Local apps If a locally-running application needs to be integrated with Infisical, you must have the CLI installed in the corresponding system; and launch the app in the previously described manner, by prefixing the start command with infisical run -- . Instead of tokens, you authorize the application by explicitly logging in (run infisical login and follow the flow). This login is persisted for some period of time (about a week), after which you'd have to re-login. :::danger It has been noticed that if a Redis container exits for whatever reason, Infisical instance won't be operational, but instead of failing, it would be urging you to re-login. ::: Status Active Consequences Some refactoring was necessary, otherwise, it works better than Doppler.","title":"\ud83d\uddf8 ADR-25. Secrets Management with Infisical"},{"location":"adr/25/#context","text":"This document succeeds and replaces ADR-2. It describes the integration of self-hosted Infisical instance as the primary tool for communicating sensitive data to the applications.","title":"Context"},{"location":"adr/25/#decision","text":"Infisical is a feature-rich open-source secrets management platform with well-detailed documentation. Wapaganda instance is hosted on a Hetzner server with docker compose . It is accessible on port 50789 at principal 's IP address; its HTTPS host is https://asdfghjkl.subjective.agency (reverse proxied with Caddy). Normally, Infisical's docker compose stack includes 4 services: infisical itself, redis , postgres and db-migration , where db-migration starts with all the rest, and exits once done migrating. In W setup, postgres service is disabled; instead, a dedicated infisical database is set up on the production instance.","title":"Decision"},{"location":"adr/25/#setup","text":"Each application that requires secrets injection has a project in Infisical. Every project can have multiple environments, but the simplest setup is to have a single dev environment with all secrets required for an app.","title":"Setup"},{"location":"adr/25/#railway-integration","text":"Infisical has native support for Railway. To set it up, Go to Railway, and obtain a new API key. Name it the same as the application. Go to Infisical -> a project you're setting up -> Integrations . Find Railway , click on it, and submit the new API key. In the pop-up, select appropriate environments on both sides. Make sure environments are aligned: you want to connect Prod to Prod, and Dev to Dev. Once the integration is saved, you should see a bunch of secrets in Railway, in the service's Shared Secrets section. Railway integration means, you don't need to add infisical run -- to the start command.","title":"Railway integration"},{"location":"adr/25/#docker-apps","text":"With non-Railway docker-based applications, integration is enabled with the following procedure: Dockerfile for the application must include instruction for installation of Infisical CLI. For example, here's the corresponding line from the backup service's file (that also installs PostgreSQL and a bunch of other stuff): dockerfile RUN apt-get update && apt-get install -y bash curl tar coreutils postgresql-client && curl -1sLf \\ 'https://dl.cloudsmith.io/public/infisical/infisical-cli/setup.deb.sh' | bash \\ && apt-get update && apt-get install -y infisical 2. Application's start command, whether in Dockerfile or in docker-compose.yml must be modified to start with infisical run -- like so: infisical run -- python backup/core.py 3. In order to the application to connect with Infisical, it needs 2 things: address of the host (since we're self-hosting) and a token. You can generate token for a project in the ==Access Control== tab. These 2 values must be saved in .env file in the same folder where you have app's primary docker compose file.","title":"Docker apps"},{"location":"adr/25/#local-apps","text":"If a locally-running application needs to be integrated with Infisical, you must have the CLI installed in the corresponding system; and launch the app in the previously described manner, by prefixing the start command with infisical run -- . Instead of tokens, you authorize the application by explicitly logging in (run infisical login and follow the flow). This login is persisted for some period of time (about a week), after which you'd have to re-login. :::danger It has been noticed that if a Redis container exits for whatever reason, Infisical instance won't be operational, but instead of failing, it would be urging you to re-login. :::","title":"Local apps"},{"location":"adr/25/#status","text":"Active","title":"Status"},{"location":"adr/25/#consequences","text":"Some refactoring was necessary, otherwise, it works better than Doppler.","title":"Consequences"},{"location":"adr/27/","text":"Context :::info Previous version of the system is described in ADR-12 . ::: This document describes the structure of the W project. Decision Diagram TBD Git repositories All repositories here are hosted under Subjective Agency organization on GitHub. wapatools . Primary repository, home for all applications except web . wapaganda_frontend . React-based frontend for the web application. whisperX . Fork required in order for WhisperX to run properly (support for transfactory application). Database There are 2 PostgreSQL instances set up for the project on different servers, prod and dev . Production database has tables distributed across following schemas: public; service; enums; data. There are several external tables synchronized to the database, all of them in service schema: provenance . Primary table for the data provenance mechanism. See ADR-21 for details; ~~ logs ~~. The log system that required this table was deprecated in favor of Logfire. See ADR-31 for details; memo and memo_relation are required to support the theory subsystem. See ADR-30 for details. Custom TripleLang type was created for the project. See ADR-22 for details. Dev database is maintained in sync with the prod with the help of Snaplet. See [ADR-32] for details. Internal Applications All internal applications are Docker-based. Latest version of images can be found in our private repository at registry.subjective.agency . Backup . Regularly exports project's database to JSON files. Works in 2 modes: incremental creates daily dumps of only new or updated rows; full creates weekly dumps of all relevant tables. Transfactory . Takes audiofiles as input and returns structured text (and metadata) as output. Transsuply . Pre-processing and registration of new jobs for Transfactory. TBD. Periodicals . Scans text-based media, news agencies etc for relevant updates, and collects them. TBD Ave Media . Scans audio and video-based media such as Smotrim.ru, Youtube.com etc. for relevant updates, and collects them. TBD Tgram . Scans target Telegram channels using official API, and collects data and statistics. TBD Printed . TBD Web . Provides a public view on the existing data. Entry Bot . Telegram bot that serves as input channel for new data. SyncManager . General-purpose application for misc backgrounds sync tasks, and handles requests forwarded from the Entry Bot . Supporting Applications & Integrations Infisical . Used to communicate secrets to applications. See ADR-25 for details. Authentik . Used for authentication and authorization in other tools. Memos . Notes-taking application. Used in the theory flow. HedgeDoc . Hosts documentation of the project. ~~Youtrack~~ . Used to be a ticket-tracker of the project, where also documentation was stored. Docs were moved away to HedgeDoc. Ticket-tracker has not been restored for now, for lack of need. Bytebase . Operational interface on top of the databases. Allows to explore the database via a rather convenient interface, as well as to run update and insert queries of all kinds: either in admin mode, or through a ticket-creation flow. Ofelia. TBD dbmate. TBD Network & Hosting Domain subjective.agency is hosted on domain.com and managed via Cloudflare. Mailservice is managed via namecheap.com . Secondary domain subjective.place is hosted directly on Cloudflare. Hetzner Principal Hetzner Principal server hosts: dev database, Infisical, Authentik, Hedgedoc, Bytebase, Memos. Hetzner Horsey Hetzner Horsey server hosts: production database, Caddy as reverse proxy server, Garage-operated storage (100 Gb), and all the internal applications, except Web . Railway Railway hosts: FastAPI-based backend, and React-based frontend Together they constitue ==Web== internal application. Railway is also used for test deployment of new internal applications. Status Active Consequences","title":"\u221e ADR-27. Wapaganda Structure [3]"},{"location":"adr/27/#context","text":":::info Previous version of the system is described in ADR-12 . ::: This document describes the structure of the W project.","title":"Context"},{"location":"adr/27/#decision","text":"","title":"Decision"},{"location":"adr/27/#diagram","text":"TBD","title":"Diagram"},{"location":"adr/27/#git-repositories","text":"All repositories here are hosted under Subjective Agency organization on GitHub. wapatools . Primary repository, home for all applications except web . wapaganda_frontend . React-based frontend for the web application. whisperX . Fork required in order for WhisperX to run properly (support for transfactory application).","title":"Git repositories"},{"location":"adr/27/#database","text":"There are 2 PostgreSQL instances set up for the project on different servers, prod and dev . Production database has tables distributed across following schemas: public; service; enums; data. There are several external tables synchronized to the database, all of them in service schema: provenance . Primary table for the data provenance mechanism. See ADR-21 for details; ~~ logs ~~. The log system that required this table was deprecated in favor of Logfire. See ADR-31 for details; memo and memo_relation are required to support the theory subsystem. See ADR-30 for details. Custom TripleLang type was created for the project. See ADR-22 for details. Dev database is maintained in sync with the prod with the help of Snaplet. See [ADR-32] for details.","title":"Database"},{"location":"adr/27/#internal-applications","text":"All internal applications are Docker-based. Latest version of images can be found in our private repository at registry.subjective.agency . Backup . Regularly exports project's database to JSON files. Works in 2 modes: incremental creates daily dumps of only new or updated rows; full creates weekly dumps of all relevant tables. Transfactory . Takes audiofiles as input and returns structured text (and metadata) as output. Transsuply . Pre-processing and registration of new jobs for Transfactory. TBD. Periodicals . Scans text-based media, news agencies etc for relevant updates, and collects them. TBD Ave Media . Scans audio and video-based media such as Smotrim.ru, Youtube.com etc. for relevant updates, and collects them. TBD Tgram . Scans target Telegram channels using official API, and collects data and statistics. TBD Printed . TBD Web . Provides a public view on the existing data. Entry Bot . Telegram bot that serves as input channel for new data. SyncManager . General-purpose application for misc backgrounds sync tasks, and handles requests forwarded from the Entry Bot .","title":"Internal Applications"},{"location":"adr/27/#supporting-applications-integrations","text":"Infisical . Used to communicate secrets to applications. See ADR-25 for details. Authentik . Used for authentication and authorization in other tools. Memos . Notes-taking application. Used in the theory flow. HedgeDoc . Hosts documentation of the project. ~~Youtrack~~ . Used to be a ticket-tracker of the project, where also documentation was stored. Docs were moved away to HedgeDoc. Ticket-tracker has not been restored for now, for lack of need. Bytebase . Operational interface on top of the databases. Allows to explore the database via a rather convenient interface, as well as to run update and insert queries of all kinds: either in admin mode, or through a ticket-creation flow. Ofelia. TBD dbmate. TBD","title":"Supporting Applications &amp; Integrations"},{"location":"adr/27/#network-hosting","text":"Domain subjective.agency is hosted on domain.com and managed via Cloudflare. Mailservice is managed via namecheap.com . Secondary domain subjective.place is hosted directly on Cloudflare.","title":"Network &amp; Hosting"},{"location":"adr/27/#hetzner-principal","text":"Hetzner Principal server hosts: dev database, Infisical, Authentik, Hedgedoc, Bytebase, Memos.","title":"Hetzner Principal"},{"location":"adr/27/#hetzner-horsey","text":"Hetzner Horsey server hosts: production database, Caddy as reverse proxy server, Garage-operated storage (100 Gb), and all the internal applications, except Web .","title":"Hetzner Horsey"},{"location":"adr/27/#railway","text":"Railway hosts: FastAPI-based backend, and React-based frontend Together they constitue ==Web== internal application. Railway is also used for test deployment of new internal applications.","title":"Railway"},{"location":"adr/27/#status","text":"Active","title":"Status"},{"location":"adr/27/#consequences","text":"","title":"Consequences"},{"location":"adr/28/","text":"Context The W project is build around a database, so backing up data is crucial. The core of the application was built by Yurii Cherkasov in the wapaganda_backend repository in 2022-2023, including the principal functionality for export into JSON files, resuming, and batching. This document describes how that code was modified and adapted to be run in automated fashion. Decision Application The application uses sqlmodel database models to communicate with the database and serialize data. At the start of the process, all relevant models are loaded and collected into a list. Each table is queried for number of rows to export first, which decides the export strategy (in one go, or in batches). Then data is dumped into JSON file(s). Once all the tables are exported, the data is archived and compressed; a schema file (generated by dbmate outside of the scope of this application) and database config files are also added to the archive, which is then uploaded to 2 destinations, in-company Minio server, and Backblaze storage. The outcome of the process is communicated to the application owner via Telegram. Modes Two modes are available for use: full and incremental . Full mode In full mode, the application queries the tables directly without any additional logic. Default batch size is 100K records. Incremental mode In incremental mode, the application operates via the volatility table, which serves as a temporary storage for new and updated rows. Each database table, for which backup has been enabled, has a set of triggers associated with it, including one that adds new records to volatility every time a row is added to that table, updated or removed. The removed records also have a JSON payload containing all non-null values of the removed row. The application queries the volatility table for the rows of the target table. If none are found, the target is skipped. If records are found, they get processed in the following manner: Removed records are separated, and later dumped into a file with _deleted suffix. Non-removed records are analyzed to get a unique set. This is a bottleneck: large amount of records may take a long time to parse through. Once the set of records to be exported is defined, their tables are queried using explicit lists of the record IDs. This is the reason why default batch size in incremental mode is 50K (there is a limit on the Postgres side on how many such IDs you can send in a query). Note: by default, table telegram_messages is excluded from incremental backup because it seems to hang the entire process. Arguments The application accepts 3 arguments: mode defines which backup mode to use; export_dir allows user to set the directory for exported files; exclude allows user to exclude certain tables from backup. It's a list of schema-qualified strigyfied table names, for example, [\"data.telegram_messages\",\"data.text_media\"] . These arguments can be supplied to the app directly in docker-compose.yaml file used to launch it, or in the .env file indicated there. Flow The app is built as a Docker image, and it meant to be launched with context-specific docker-compose.yaml files. Application relies on the following: a schema file has been produced by dbmate in the recent days, and saved to a particular directory; database configuration files can be located. The application is scheduled to run: in full mode: once a week, ETC nighttime; in incremental mode: once a day, ETC nighttime, except for the day when full dump is done. Storage The archive produced by the export is then uploaded to the storage: Minio storage located on the same server as the database; Backblaze cloud storage. Logging The application logs to a dedicated backup project on Logfire. Status Active Consequences Data is safe and consistent.","title":"\ud83d\uddf8 ADR-28. Backup Service"},{"location":"adr/28/#context","text":"The W project is build around a database, so backing up data is crucial. The core of the application was built by Yurii Cherkasov in the wapaganda_backend repository in 2022-2023, including the principal functionality for export into JSON files, resuming, and batching. This document describes how that code was modified and adapted to be run in automated fashion.","title":"Context"},{"location":"adr/28/#decision","text":"","title":"Decision"},{"location":"adr/28/#application","text":"The application uses sqlmodel database models to communicate with the database and serialize data. At the start of the process, all relevant models are loaded and collected into a list. Each table is queried for number of rows to export first, which decides the export strategy (in one go, or in batches). Then data is dumped into JSON file(s). Once all the tables are exported, the data is archived and compressed; a schema file (generated by dbmate outside of the scope of this application) and database config files are also added to the archive, which is then uploaded to 2 destinations, in-company Minio server, and Backblaze storage. The outcome of the process is communicated to the application owner via Telegram.","title":"Application"},{"location":"adr/28/#modes","text":"Two modes are available for use: full and incremental .","title":"Modes"},{"location":"adr/28/#full-mode","text":"In full mode, the application queries the tables directly without any additional logic. Default batch size is 100K records.","title":"Full mode"},{"location":"adr/28/#incremental-mode","text":"In incremental mode, the application operates via the volatility table, which serves as a temporary storage for new and updated rows. Each database table, for which backup has been enabled, has a set of triggers associated with it, including one that adds new records to volatility every time a row is added to that table, updated or removed. The removed records also have a JSON payload containing all non-null values of the removed row. The application queries the volatility table for the rows of the target table. If none are found, the target is skipped. If records are found, they get processed in the following manner: Removed records are separated, and later dumped into a file with _deleted suffix. Non-removed records are analyzed to get a unique set. This is a bottleneck: large amount of records may take a long time to parse through. Once the set of records to be exported is defined, their tables are queried using explicit lists of the record IDs. This is the reason why default batch size in incremental mode is 50K (there is a limit on the Postgres side on how many such IDs you can send in a query). Note: by default, table telegram_messages is excluded from incremental backup because it seems to hang the entire process.","title":"Incremental mode"},{"location":"adr/28/#arguments","text":"The application accepts 3 arguments: mode defines which backup mode to use; export_dir allows user to set the directory for exported files; exclude allows user to exclude certain tables from backup. It's a list of schema-qualified strigyfied table names, for example, [\"data.telegram_messages\",\"data.text_media\"] . These arguments can be supplied to the app directly in docker-compose.yaml file used to launch it, or in the .env file indicated there.","title":"Arguments"},{"location":"adr/28/#flow","text":"The app is built as a Docker image, and it meant to be launched with context-specific docker-compose.yaml files. Application relies on the following: a schema file has been produced by dbmate in the recent days, and saved to a particular directory; database configuration files can be located. The application is scheduled to run: in full mode: once a week, ETC nighttime; in incremental mode: once a day, ETC nighttime, except for the day when full dump is done.","title":"Flow"},{"location":"adr/28/#storage","text":"The archive produced by the export is then uploaded to the storage: Minio storage located on the same server as the database; Backblaze cloud storage.","title":"Storage"},{"location":"adr/28/#logging","text":"The application logs to a dedicated backup project on Logfire.","title":"Logging"},{"location":"adr/28/#status","text":"Active","title":"Status"},{"location":"adr/28/#consequences","text":"Data is safe and consistent.","title":"Consequences"},{"location":"adr/29/","text":"Context The project requires storage in order to exchange media files, store assets, etc. For the first period of its existense, we used Supabase, which offered 100Gb of storage in addition to the database hosting service \"for free\". When we dropped Supabase, Minio-operated storage was set up on the same Hetzner server that hosts the database. This document describes the current implementation. Decision Status Consequenses","title":"\u221e ADR-29. Storage"},{"location":"adr/29/#context","text":"The project requires storage in order to exchange media files, store assets, etc. For the first period of its existense, we used Supabase, which offered 100Gb of storage in addition to the database hosting service \"for free\". When we dropped Supabase, Minio-operated storage was set up on the same Hetzner server that hosts the database. This document describes the current implementation.","title":"Context"},{"location":"adr/29/#decision","text":"","title":"Decision"},{"location":"adr/29/#status","text":"","title":"Status"},{"location":"adr/29/#consequenses","text":"","title":"Consequenses"},{"location":"adr/3/","text":"Context As the project gradually picking up steam, it would be appropriate to introduce a stricter Git flow, to make the process of development more organized and structured. Many different approaches are possible here, the most widely used of them are git-flow and GitHub flow . Decision Given that the project is rather small, and is not expected to require significant workforce, GitHub flow seems more appropriate. The components of the approach are as follows: There is a persistent branch called master that serves as a production; Development goes on in feature branches that may or may not have YouTrack issues linked to them; Feature branch must have a feature- prefix in the name. Once a feature is deemed complete, a PR should be created. This would spin up a Railways PR environment for this feature branch. Once the PR is closed, the Railway env would be removed automatically. If PR is approved, the feature branch should be merged into master and then removed. Status In revision. The workflow would be modified towards CI/CD. [UPD] [17.02.2024] This ADR is simply cancelled for now. Consequence Git happiness.","title":"\ud83d\uddf6 ADR-3. Git Flow"},{"location":"adr/3/#context","text":"As the project gradually picking up steam, it would be appropriate to introduce a stricter Git flow, to make the process of development more organized and structured. Many different approaches are possible here, the most widely used of them are git-flow and GitHub flow .","title":"Context"},{"location":"adr/3/#decision","text":"Given that the project is rather small, and is not expected to require significant workforce, GitHub flow seems more appropriate. The components of the approach are as follows: There is a persistent branch called master that serves as a production; Development goes on in feature branches that may or may not have YouTrack issues linked to them; Feature branch must have a feature- prefix in the name. Once a feature is deemed complete, a PR should be created. This would spin up a Railways PR environment for this feature branch. Once the PR is closed, the Railway env would be removed automatically. If PR is approved, the feature branch should be merged into master and then removed.","title":"Decision"},{"location":"adr/3/#status","text":"In revision. The workflow would be modified towards CI/CD. [UPD] [17.02.2024] This ADR is simply cancelled for now.","title":"Status"},{"location":"adr/3/#consequence","text":"Git happiness.","title":"Consequence"},{"location":"adr/30/","text":"Context Website's ==Theory== section is intended to serve as a window into the supporting materials related to the purpose of the project. These are: current-time articles and blog posts written by either friends of Ukraine or relatively objective observers (subtype companions ); a special tiny collection of particularly enlightening texts written by the Russian propaganda actors (subtype copium ); my own theoretical reasonings (subtype core ); and a multitude of notes on various research subjects, from the so-called geopolitics to linguistic nuances of Russian language. The initial body of companion and copium texts was translated from Russian into English for the ==Dum Vita Est== blog, and, in April 2023, translated from Russian into Ukrainian using openai/gpt-3.5 . Decision Memos Memos is a self-hosted application for taking notes. What makes this particular app suitable for the purpose of the W project are the following nuances: Memos supports storing notes in the PostgreSQL database; App's system of tags is sufficiently intricate, and supports nested tags; There is a feature allowing to link notes with each other. As such, Memos provides a convenient interface for adding new notes. Technical Details As of mid August 2024, Memos instance is set up on a Hetzner server. HTTPS domain is https://memos.subjective.agency (reverse proxied with Caddy). Dedicated postgres database was created on Dev instance. This database is managed by the Memos application and should not be tempered with manually. Adding New Memo Most of the metadata is added to the note in the frontematter component, which must always come first. It is followed by the note's content, and then, at the end, exactly 1 line of tags. Frontmatter Frontmatter is designated with --- before and after a number of YAML-formatter key-value pairs, and must always come as the first component on the canvas. Depending on whether a note is a full-fledged article, or just a remark, there could be different amount of metadata. Generally speaking, author and urls should be definitely preserved, plus anything else that makes sense in context. With smaller notes, definitely set sequence if note is a start of a new sequence. With larger articles, following keys are requred: author subtype . This could be one of: companion , core or copium . excerpt . Short summary of / intro into the article. date_published url (if singular) or urls . In the latter case, the value must be surrounded with square brackets [] , have each URL inside wrapped in single quotes ' , and use coma , as a delimiter. translated_to_en | translated_to_uk | translated_to_ru : a JSON structure with keys i (for intelligence , could be h (human) or a ), and author . Example: {\"i\": \"h\", \"author\": \"shoomow\"} Content Must be clean markdown. Tags Tags must be put on the last line of the note, and must all be on one line. Tags must be separated with spaces. Tags can be multilevel, in which case levels must be separated with forward slash / and no spaces. Every singular tag must be preceded with # ; with multilevel tags, only the root must have # . If a tags has to be more than 1 word, words within tags must be separated with underscore _ . Example: #research/geopolitics #mackinder #book/democratic_ideals_and_reality :::info In case a newly saved memo is a part of a sequence (e.g. yet another observation taken in the course of reading a book), once it has been added, find the previous note of this sequence, and add the link to the new note to it. This is how the sequences are maintained. ::: Secondary Tables and Synchronization Flow Database Two of the original Memos tables are mirrored to the primary database via foreign tables mechanism. They can be found in the service schema, under names memo and memo_relation . Before the data can be served to the final user, it needs to be processed. For this purpose, following secondary tables were set up: public.theory2 . Primary content (notes) is stored here. service.theory_tags . Tags. Includes 1-to-many parent relationship. service.theory_tags_mapping . Mapping between notes ( theory2 ) and tags ( theory_tags ). service.theory_sequences . Sequences. service.theory_sequence_mapping Mapping between notes and sequences. Includes spot_in_sequence to be explicit about the order of notes. enums.theory_types Synchronization Synchronization is set up as 2 routes in the FastAPI web application (under theory tag): theory/sync and theory/update_sequences . Both are background processes to be run at schedule. The primary sync process collects new memos , uses python markdown libraries to properly parse it, and converts it into a DB object ( theory2 ). Frontmatter fields listed explicitly in this section would be parsed into proper columns, everything else would be merged into a JSON structure and saved in additional field, with the exception of sequence tag, which indicates first note in a new sequence, --- in this case, a new sequence object would be created. Tags would be parsed; they will be mapped to the note, and all new ones would be inserted into the theory_tags table. The secondary sync process ( update_sequences ) iterates over the known sequences (i.e. present in the theory_sequences table), finds an edge note, and checks if there is a previously non-existent relationship there. Status Active Consequences The new setup required significant refactoring of the frontend.","title":"\ud83d\uddf8 ADR-30. Theory"},{"location":"adr/30/#context","text":"Website's ==Theory== section is intended to serve as a window into the supporting materials related to the purpose of the project. These are: current-time articles and blog posts written by either friends of Ukraine or relatively objective observers (subtype companions ); a special tiny collection of particularly enlightening texts written by the Russian propaganda actors (subtype copium ); my own theoretical reasonings (subtype core ); and a multitude of notes on various research subjects, from the so-called geopolitics to linguistic nuances of Russian language. The initial body of companion and copium texts was translated from Russian into English for the ==Dum Vita Est== blog, and, in April 2023, translated from Russian into Ukrainian using openai/gpt-3.5 .","title":"Context"},{"location":"adr/30/#decision","text":"","title":"Decision"},{"location":"adr/30/#memos","text":"Memos is a self-hosted application for taking notes. What makes this particular app suitable for the purpose of the W project are the following nuances: Memos supports storing notes in the PostgreSQL database; App's system of tags is sufficiently intricate, and supports nested tags; There is a feature allowing to link notes with each other. As such, Memos provides a convenient interface for adding new notes.","title":"Memos"},{"location":"adr/30/#technical-details","text":"As of mid August 2024, Memos instance is set up on a Hetzner server. HTTPS domain is https://memos.subjective.agency (reverse proxied with Caddy). Dedicated postgres database was created on Dev instance. This database is managed by the Memos application and should not be tempered with manually.","title":"Technical Details"},{"location":"adr/30/#adding-new-memo","text":"Most of the metadata is added to the note in the frontematter component, which must always come first. It is followed by the note's content, and then, at the end, exactly 1 line of tags.","title":"Adding New Memo"},{"location":"adr/30/#frontmatter","text":"Frontmatter is designated with --- before and after a number of YAML-formatter key-value pairs, and must always come as the first component on the canvas. Depending on whether a note is a full-fledged article, or just a remark, there could be different amount of metadata. Generally speaking, author and urls should be definitely preserved, plus anything else that makes sense in context. With smaller notes, definitely set sequence if note is a start of a new sequence. With larger articles, following keys are requred: author subtype . This could be one of: companion , core or copium . excerpt . Short summary of / intro into the article. date_published url (if singular) or urls . In the latter case, the value must be surrounded with square brackets [] , have each URL inside wrapped in single quotes ' , and use coma , as a delimiter. translated_to_en | translated_to_uk | translated_to_ru : a JSON structure with keys i (for intelligence , could be h (human) or a ), and author . Example: {\"i\": \"h\", \"author\": \"shoomow\"}","title":"Frontmatter"},{"location":"adr/30/#content","text":"Must be clean markdown.","title":"Content"},{"location":"adr/30/#tags","text":"Tags must be put on the last line of the note, and must all be on one line. Tags must be separated with spaces. Tags can be multilevel, in which case levels must be separated with forward slash / and no spaces. Every singular tag must be preceded with # ; with multilevel tags, only the root must have # . If a tags has to be more than 1 word, words within tags must be separated with underscore _ . Example: #research/geopolitics #mackinder #book/democratic_ideals_and_reality :::info In case a newly saved memo is a part of a sequence (e.g. yet another observation taken in the course of reading a book), once it has been added, find the previous note of this sequence, and add the link to the new note to it. This is how the sequences are maintained. :::","title":"Tags"},{"location":"adr/30/#secondary-tables-and-synchronization-flow","text":"","title":"Secondary Tables and Synchronization Flow"},{"location":"adr/30/#database","text":"Two of the original Memos tables are mirrored to the primary database via foreign tables mechanism. They can be found in the service schema, under names memo and memo_relation . Before the data can be served to the final user, it needs to be processed. For this purpose, following secondary tables were set up: public.theory2 . Primary content (notes) is stored here. service.theory_tags . Tags. Includes 1-to-many parent relationship. service.theory_tags_mapping . Mapping between notes ( theory2 ) and tags ( theory_tags ). service.theory_sequences . Sequences. service.theory_sequence_mapping Mapping between notes and sequences. Includes spot_in_sequence to be explicit about the order of notes. enums.theory_types","title":"Database"},{"location":"adr/30/#synchronization","text":"Synchronization is set up as 2 routes in the FastAPI web application (under theory tag): theory/sync and theory/update_sequences . Both are background processes to be run at schedule. The primary sync process collects new memos , uses python markdown libraries to properly parse it, and converts it into a DB object ( theory2 ). Frontmatter fields listed explicitly in this section would be parsed into proper columns, everything else would be merged into a JSON structure and saved in additional field, with the exception of sequence tag, which indicates first note in a new sequence, --- in this case, a new sequence object would be created. Tags would be parsed; they will be mapped to the note, and all new ones would be inserted into the theory_tags table. The secondary sync process ( update_sequences ) iterates over the known sequences (i.e. present in the theory_sequences table), finds an edge note, and checks if there is a previously non-existent relationship there.","title":"Synchronization"},{"location":"adr/30/#status","text":"Active","title":"Status"},{"location":"adr/30/#consequences","text":"The new setup required significant refactoring of the frontend.","title":"Consequences"},{"location":"adr/33/","text":"Context It is more than possible to use DockerHub for exchanging Docker images, but it doesn't feel secure, since the data can be, in theory, available to random people, even if the repo is private. On the other hand, setting up private registry is as simple as running a regular docker compose application. Decision Registry was set up on the ==Horsey== server at port 50000, and secured behind HTTPS at registry.subjective.agency . :::info Without a dedicated domain for the registry, one would have to add its address to insecure-registries key in /etc/docker/daemon.json , but because the registry is accessible via https, there is no need. ::: Registry was set up as a system service, with the service file at /etc/systemd/system/docker-registry.service . Data directory is /mnt/docker/registry Example of creating data directory and applying permissions shell sudo mkdir -p /mnt/docker/registry sudo chown -R root:root /mnt/docker/registry sudo chmod -R 755 /mnt/docker/registry Authentication is set up with htpasswd . Auth directory is /mnt/docker/registry/auth . Auth config is at /mnt/docker/registry/config.yml See Also Private Docker Registry Auth Status ==Active== Consequences Only a few minor things need to change when using private registry; namely, you need to indicate the domain when pushing and pulling; and you need to authorize with the repository in order to work with it.","title":"ADR-33. Private Docker Registry"},{"location":"adr/33/#context","text":"It is more than possible to use DockerHub for exchanging Docker images, but it doesn't feel secure, since the data can be, in theory, available to random people, even if the repo is private. On the other hand, setting up private registry is as simple as running a regular docker compose application.","title":"Context"},{"location":"adr/33/#decision","text":"Registry was set up on the ==Horsey== server at port 50000, and secured behind HTTPS at registry.subjective.agency . :::info Without a dedicated domain for the registry, one would have to add its address to insecure-registries key in /etc/docker/daemon.json , but because the registry is accessible via https, there is no need. ::: Registry was set up as a system service, with the service file at /etc/systemd/system/docker-registry.service . Data directory is /mnt/docker/registry Example of creating data directory and applying permissions shell sudo mkdir -p /mnt/docker/registry sudo chown -R root:root /mnt/docker/registry sudo chmod -R 755 /mnt/docker/registry Authentication is set up with htpasswd . Auth directory is /mnt/docker/registry/auth . Auth config is at /mnt/docker/registry/config.yml","title":"Decision"},{"location":"adr/33/#see-also","text":"Private Docker Registry Auth","title":"See Also"},{"location":"adr/33/#status","text":"==Active==","title":"Status"},{"location":"adr/33/#consequences","text":"Only a few minor things need to change when using private registry; namely, you need to indicate the domain when pushing and pulling; and you need to authorize with the repository in order to work with it.","title":"Consequences"},{"location":"adr/4/","text":"Context There are two major approaches to Frontend architecture: traditional web applications that perform most of the application logic on the server, and single-page applications (SPAs) that perform most of the user interface logic in a web browser, communicating with the web server primarily using web APIs. A hybrid approach is also possible, the simplest being host one or more rich SPA-like subapplications within a larger traditional web application. Decision Traditional Web applications could be required to function in browsers without JavaScript support. As this is not our case, we opt out of such a solution. Besides, most of the modern Frontend frameworks assume SPA as an architectural choice. SPA provides users with a speeded experience because it takes the shortest time to load. What usually makes applications load for longer is if they need to reload each page's HTML every time the user is interacting with it. With single-page apps, on the other hand, there are no additional queries to download pages. SPA application expose a rich user interface with many responsive features, e.g. search with intelligent autocomplete. As a frontend solution, we choose a React-based Single Page Application (SPA) - quite a mature solution with lots of tools, libraries, and a vast community. Status Accepted Consequences Communication between SPA and service is usually performed using REST API protocol. It contains only data, with no page representation. The request usually includes either search conditions for some entity (people, organizations, media sources), or blank condition to load all the available data, when it matches the application logic. The response contains data with no connection to the data source (e.g. it contains connected organizations connected to a particular person, rather than database IDs of such an organization. SPA doesn't need to know anything about data representation and the database itself.","title":"\ud83d\uddf6\ud83d\uddf8 ADR-4. REST API"},{"location":"adr/4/#context","text":"There are two major approaches to Frontend architecture: traditional web applications that perform most of the application logic on the server, and single-page applications (SPAs) that perform most of the user interface logic in a web browser, communicating with the web server primarily using web APIs. A hybrid approach is also possible, the simplest being host one or more rich SPA-like subapplications within a larger traditional web application.","title":"Context"},{"location":"adr/4/#decision","text":"Traditional Web applications could be required to function in browsers without JavaScript support. As this is not our case, we opt out of such a solution. Besides, most of the modern Frontend frameworks assume SPA as an architectural choice. SPA provides users with a speeded experience because it takes the shortest time to load. What usually makes applications load for longer is if they need to reload each page's HTML every time the user is interacting with it. With single-page apps, on the other hand, there are no additional queries to download pages. SPA application expose a rich user interface with many responsive features, e.g. search with intelligent autocomplete. As a frontend solution, we choose a React-based Single Page Application (SPA) - quite a mature solution with lots of tools, libraries, and a vast community.","title":"Decision"},{"location":"adr/4/#status","text":"Accepted","title":"Status"},{"location":"adr/4/#consequences","text":"Communication between SPA and service is usually performed using REST API protocol. It contains only data, with no page representation. The request usually includes either search conditions for some entity (people, organizations, media sources), or blank condition to load all the available data, when it matches the application logic. The response contains data with no connection to the data source (e.g. it contains connected organizations connected to a particular person, rather than database IDs of such an organization. SPA doesn't need to know anything about data representation and the database itself.","title":"Consequences"},{"location":"adr/5/","text":"Context Project's growth is expressed not only in the evolution of the code base, but also in including new data sources. Recently it became clear that some of these sources are too abundant and not consistent enough in terms of data schema for a regular relationships-based database. Decision MongoDB provides a solution to this issue, as it is JSON-based and does not care about the schema. A serverless instances was set up with Mongo Atlas, currently holding 1 database ( wapa ) and 1 collection ( telegram-messages ). Primary search index will be by channel_id . The telegram-messages collection is intended to store JSON-like objects representing messages published in the relevant telegram channels. Each such object contains a key channel_id which value corresponds to the channel's id in the telegram_channels table on Supabase. In addition, telegram_channels table acquires a new column called history_count which should contain the number of message objects in telegram-messages collection with corresponding channel_id . Status Deprecated due to MongoDB not being used anymore. All the data previously stored in Mongo was migrated to Postgres (schema data ). Consequences Mongo does not add too much overhead to the engineering process, as its high-level API follows typical conventions and is relatively easy to fuse into the project. Some updates to the code base would be required. Quite a lot of data analysis and visualization opportunities are expected to open up.","title":"\ud83d\uddf6 ADR-5. MongoDB"},{"location":"adr/5/#context","text":"Project's growth is expressed not only in the evolution of the code base, but also in including new data sources. Recently it became clear that some of these sources are too abundant and not consistent enough in terms of data schema for a regular relationships-based database.","title":"Context"},{"location":"adr/5/#decision","text":"MongoDB provides a solution to this issue, as it is JSON-based and does not care about the schema. A serverless instances was set up with Mongo Atlas, currently holding 1 database ( wapa ) and 1 collection ( telegram-messages ). Primary search index will be by channel_id . The telegram-messages collection is intended to store JSON-like objects representing messages published in the relevant telegram channels. Each such object contains a key channel_id which value corresponds to the channel's id in the telegram_channels table on Supabase. In addition, telegram_channels table acquires a new column called history_count which should contain the number of message objects in telegram-messages collection with corresponding channel_id .","title":"Decision"},{"location":"adr/5/#status","text":"Deprecated due to MongoDB not being used anymore. All the data previously stored in Mongo was migrated to Postgres (schema data ).","title":"Status"},{"location":"adr/5/#consequences","text":"Mongo does not add too much overhead to the engineering process, as its high-level API follows typical conventions and is relatively easy to fuse into the project. Some updates to the code base would be required. Quite a lot of data analysis and visualization opportunities are expected to open up.","title":"Consequences"},{"location":"adr/6/","text":"Context One of the W's most important parts has to do with the various media segments (i.e. talk shows, streams, YouTube videos etc.). Until recently, the main focus in this area was on indicating participants, which is just barely enough to make it useful. Transcripts - text representation of the specific episodes of the relevant media segments - make it significantly more useful. With whisper , an open-source, free-to-use software doing speech recognition of high quality, it is possible to produce transcripts for every relevant media episode in (semi-)automated fashion. Whisper is a machine-learning tool that uses pre-compiled downloadable models ( tiny \u2192 small \u2192 medium \u2192 large ). The choice of model effects 2 things: processing time and result quality . The quality of the output is pretty great even with medium model (\\~95%), and large model does an almost perfect job (\\~99%). Processing time is approximately 3x and 10x on the media duration, respectively. These figures are not exact measurements, but rather a subjective estimation. Processing time is measured on a relatively advanced but still a regular PC . Whisper produces 3 text files: .txt , .str , and .vtt . The .vtt , while could contain additional metadata (such as chapters), in this particular case does not contain anything but timestamps, which makes it no different from the .srt version. .txt can be easily generated from .srt , so essentially only the srt file should be saved. Decision The following structure is implemented around the transcript manufacturing process. Transcript Flow Transcript flow consists of 4 inter-related scripts: transfactory , which produces transcripts from audio files; transsuply , which sees to it that there were always enough files for processing; transprocessor , which downloads the factory 's output and saves it to final storage ; transcleanup , which removes processed audio and text files. The flow is described in more detail in DVE-A-40. Storage The following storage units are used for temporary and permanent storage: MongoDB (cluster wapaless , collection transcripts ) that contains objects of the structure: json { \"supa_table\": str \"{segment_name}_vids\", \"supa_id\": int, \"whisper_model\": str \"l\" | \"s\" | \"m\", \"duration\": dict {\"total\": int}, # in seconds \"text\": str \"full_text\", \"srt\": dict {(\"chunk_start_time-chunk_end_time\"): \"chunk_text\"} } * Supabase storage, bucket prabyss . Used for temporary storage of audio files and resulting .srt files * Supabase storage, bucket back-backups . Used to store all the backups (not just transcripts) as pickle objects. Auxiliary database Supabase DB, schema service , table prabyss . Used to store information on the media episodes to be transcribed, as well as a data exchange point between different parts of the transcript flow. Additional technical information can be found here and here . Status Accepted Consequence This infrastructure is relatively strict and predictable, but also adjustable. In the near future, it would allow an easier implementation of full text search over the large amounts of text. In the more distant future, it would also allow NLP-based analysis and/or translation into English.","title":"\ud83d\uddf6\u227d ADR-6. Transcripts"},{"location":"adr/6/#context","text":"One of the W's most important parts has to do with the various media segments (i.e. talk shows, streams, YouTube videos etc.). Until recently, the main focus in this area was on indicating participants, which is just barely enough to make it useful. Transcripts - text representation of the specific episodes of the relevant media segments - make it significantly more useful. With whisper , an open-source, free-to-use software doing speech recognition of high quality, it is possible to produce transcripts for every relevant media episode in (semi-)automated fashion. Whisper is a machine-learning tool that uses pre-compiled downloadable models ( tiny \u2192 small \u2192 medium \u2192 large ). The choice of model effects 2 things: processing time and result quality . The quality of the output is pretty great even with medium model (\\~95%), and large model does an almost perfect job (\\~99%). Processing time is approximately 3x and 10x on the media duration, respectively. These figures are not exact measurements, but rather a subjective estimation. Processing time is measured on a relatively advanced but still a regular PC . Whisper produces 3 text files: .txt , .str , and .vtt . The .vtt , while could contain additional metadata (such as chapters), in this particular case does not contain anything but timestamps, which makes it no different from the .srt version. .txt can be easily generated from .srt , so essentially only the srt file should be saved.","title":"Context"},{"location":"adr/6/#decision","text":"The following structure is implemented around the transcript manufacturing process.","title":"Decision"},{"location":"adr/6/#transcript-flow","text":"Transcript flow consists of 4 inter-related scripts: transfactory , which produces transcripts from audio files; transsuply , which sees to it that there were always enough files for processing; transprocessor , which downloads the factory 's output and saves it to final storage ; transcleanup , which removes processed audio and text files. The flow is described in more detail in DVE-A-40.","title":"Transcript Flow"},{"location":"adr/6/#storage","text":"The following storage units are used for temporary and permanent storage: MongoDB (cluster wapaless , collection transcripts ) that contains objects of the structure: json { \"supa_table\": str \"{segment_name}_vids\", \"supa_id\": int, \"whisper_model\": str \"l\" | \"s\" | \"m\", \"duration\": dict {\"total\": int}, # in seconds \"text\": str \"full_text\", \"srt\": dict {(\"chunk_start_time-chunk_end_time\"): \"chunk_text\"} } * Supabase storage, bucket prabyss . Used for temporary storage of audio files and resulting .srt files * Supabase storage, bucket back-backups . Used to store all the backups (not just transcripts) as pickle objects.","title":"Storage"},{"location":"adr/6/#auxiliary-database","text":"Supabase DB, schema service , table prabyss . Used to store information on the media episodes to be transcribed, as well as a data exchange point between different parts of the transcript flow. Additional technical information can be found here and here .","title":"Auxiliary database"},{"location":"adr/6/#status","text":"Accepted","title":"Status"},{"location":"adr/6/#consequence","text":"This infrastructure is relatively strict and predictable, but also adjustable. In the near future, it would allow an easier implementation of full text search over the large amounts of text. In the more distant future, it would also allow NLP-based analysis and/or translation into English.","title":"Consequence"},{"location":"adr/7/","text":"Context Wapaganda DB contains data that is currently not used in the project, or access to it is limited. One such cluster is the media episodes data, including dates (limited availability), metadata (limited availability), participants (work in progress) and transcripts produced by Whisper. The project will benefit from unlocking this data and making it as accessible to the user as possible. Decision In addition to the people interface, a parallel interface for the media episodes should be developed. On the highest level, this should be a list of available media segments . A level deeper - a list of all existing episode per segment (may or may not be limited by dates - for example, to show only starting with Feb 24). For each episode in the list, a list of participants should be available on this level, along with a date, duration and a link to the episode page . Another level deeper is an episode page , which contains the same basic data plus possible additional data, and a CTA to the transcript page . transcript page should be a separate thing to enable future mark-up development . Status Cooking. Consequences With this initiative completed, there should be 2 points of entry to a transcript page \\: apart from the one described here, a user should be able to get a link to this page by visiting a person page \u2192 airtime tab \u2192 date selection \u2192 episode selection.","title":"\u221e ADR-7. Media Episodes Interface"},{"location":"adr/7/#context","text":"Wapaganda DB contains data that is currently not used in the project, or access to it is limited. One such cluster is the media episodes data, including dates (limited availability), metadata (limited availability), participants (work in progress) and transcripts produced by Whisper. The project will benefit from unlocking this data and making it as accessible to the user as possible.","title":"Context"},{"location":"adr/7/#decision","text":"In addition to the people interface, a parallel interface for the media episodes should be developed. On the highest level, this should be a list of available media segments . A level deeper - a list of all existing episode per segment (may or may not be limited by dates - for example, to show only starting with Feb 24). For each episode in the list, a list of participants should be available on this level, along with a date, duration and a link to the episode page . Another level deeper is an episode page , which contains the same basic data plus possible additional data, and a CTA to the transcript page . transcript page should be a separate thing to enable future mark-up development .","title":"Decision"},{"location":"adr/7/#status","text":"Cooking.","title":"Status"},{"location":"adr/7/#consequences","text":"With this initiative completed, there should be 2 points of entry to a transcript page \\: apart from the one described here, a user should be able to get a link to this page by visiting a person page \u2192 airtime tab \u2192 date selection \u2192 episode selection.","title":"Consequences"},{"location":"adr/8/","text":"Context Media transcripts represent significant amounts of textual data that, at some point, will be analyzed via the NLP flow (not developed). This flow will produce additional data that would require appropriate representation in the UX/UI. Decision There should be, on the one hand, a tool to 1) create custom tags, and to 2) easily mark specific parts of the text with these tags; and on the other, a special user view that would display these tags appropriately. There should be an approximate list of potentially useful tags. Status Cooking. Consequences Undetermined.","title":"\u221e ADR-8. Transcript Mark-Up"},{"location":"adr/8/#context","text":"Media transcripts represent significant amounts of textual data that, at some point, will be analyzed via the NLP flow (not developed). This flow will produce additional data that would require appropriate representation in the UX/UI.","title":"Context"},{"location":"adr/8/#decision","text":"There should be, on the one hand, a tool to 1) create custom tags, and to 2) easily mark specific parts of the text with these tags; and on the other, a special user view that would display these tags appropriately. There should be an approximate list of potentially useful tags.","title":"Decision"},{"location":"adr/8/#status","text":"Cooking.","title":"Status"},{"location":"adr/8/#consequences","text":"Undetermined.","title":"Consequences"},{"location":"adr/9/","text":"Context Given the origins and the nature of the project, 3 languages need to be available for the end user: Ukrainian (primary), English (to make it accessible to as many researchers as possible) and Russian (because most of the source material is in Russian). Localization in all 3 languages must be supported on 2 levels: Website components Data. Data Layer To support localization on the data level, most of the relevant and translatable data pieces in the DB have infrastructure for all 3 languages, which is implemented in 1 of 2 possible ways: Separate columns, such as fullname_en , fullname_uk , fullname_ru etc in the people table. Single column of JSONB type, holding a dictionary with structure {\"en\": \"eng_version\", \"ru\": \"ru_version\", \"uk\": \"uk_version\"} NOTE: While infrastructure is almost guaranteed, the data pieces themselves may be (in fact, quite likely to be) missing in various combinations. Website Components Layer See DVE-A-34 . Decision Identify localization options given that accepted frontend framework is React (DVE-83) Work out a transition plan, in order to phase out the web2py -native approach to localizing website components (DVE-84). Work out an approach to switching supported languages for both relevant layers simultaneously (DVE-42). Status Cooking. Consequences Unclear.","title":"\ud83d\uddf6\u227d ADR-9. Wapaganda Localization [1]"},{"location":"adr/9/#context","text":"Given the origins and the nature of the project, 3 languages need to be available for the end user: Ukrainian (primary), English (to make it accessible to as many researchers as possible) and Russian (because most of the source material is in Russian). Localization in all 3 languages must be supported on 2 levels: Website components Data.","title":"Context"},{"location":"adr/9/#data-layer","text":"To support localization on the data level, most of the relevant and translatable data pieces in the DB have infrastructure for all 3 languages, which is implemented in 1 of 2 possible ways: Separate columns, such as fullname_en , fullname_uk , fullname_ru etc in the people table. Single column of JSONB type, holding a dictionary with structure {\"en\": \"eng_version\", \"ru\": \"ru_version\", \"uk\": \"uk_version\"} NOTE: While infrastructure is almost guaranteed, the data pieces themselves may be (in fact, quite likely to be) missing in various combinations.","title":"Data Layer"},{"location":"adr/9/#website-components-layer","text":"See DVE-A-34 .","title":"Website Components Layer"},{"location":"adr/9/#decision","text":"Identify localization options given that accepted frontend framework is React (DVE-83) Work out a transition plan, in order to phase out the web2py -native approach to localizing website components (DVE-84). Work out an approach to switching supported languages for both relevant layers simultaneously (DVE-42).","title":"Decision"},{"location":"adr/9/#status","text":"Cooking.","title":"Status"},{"location":"adr/9/#consequences","text":"Unclear.","title":"Consequences"},{"location":"arr/1/","text":"ARR-1: Ave Media General Ave Media is an application for collecting new items from various media platforms. Process is initialized for each platform separately, which allows to run updates concurrently. Code Modules involved: bases/w/media bases/w/database bases/w/config components/w/ave_media Source-specific implementations are located at bases/w/media/platforms . File platform_service contains base classes MediaPlatform and MediaTarget , which are parents to platform-specific implementations in correspondingly named files in the same directory. Entities -- Media Platforms Platform URL Status Targets table Episodes table Other tables Youtube youtube.com Active youtube_channels youtube_vids Smotrim.ru smotrim.ru Active smotrim_brands smotrim_episodes Rutube rutube.ru Active rutube_channels rutube_episodes Soundstream soundstream.media In development soundstream_channels soundstream_episodes soundstream_playlists VK vk.com To be added TBD Komsomolka radiokp.ru To be refactored TBD Dentv dentv.ru To be refactored TBD Ntv ntv.ru To be refactored TBD Flow Diagram flowchart TD quit_on_resolve(Quit) is_target_resolved(Is target resolved?) subgraph collect_targets [Collect and init targets] direction TB fetch_channels(Fetch target-level entities from DB) fetch_channels -- for each --> init_target end subgraph init_target [Init target] direction TB assign_session2(Assign DB session) assign_channel(Assign target-level entity) set_unresolved(Set target as unresolved) fetch_latest(Fetch target's latest episode) set_empty(Set target for collecting all available updates\\nfetch_all = True) assign_session2 --- assign_channel -- if `title` is missing --> set_unresolved assign_channel --> fetch_latest -- if no latest item found --> set_empty end subgraph init_platform [Init platform] direction LR assign_session1(Assign DB session) assign_platform_specific(Assing platform-specific attributes) assign_session1 --- assign_platform_specific --- collect_targets assign_platform_specific -- if YouTube --> ytb(Youtube API client) end subgraph resolve_target [Resolve target] direction TB query_platform(Fetch target data from the platform) update_target_db(Update DB record) mark_resolved(Mark target as resolved) mark_dead(Mark target as defunct) query_platform -- if found --> mark_resolved --> update_target_db query_platform -- if not found --> mark_dead end subgraph fetch_ytb_updates [ ] direction TB ytb_get_playlist_vids(Use 'get_playlist_videos' method) ytb_limit_none(with limit = None) ytb_limit_100(with limit = 100) ytb_get_vids(Use 'videos' method for additional data) ytb_check_record_exists(Query DB to check if this record exists) ytb_add_to_updates(Add to final list) ytb_parse_vids(Parse all collected videos) ytb_persist_vids(Save videos to the database) ytb_get_playlist_vids -- if fetch_all --> ytb_limit_none --> ytb_get_vids ytb_get_playlist_vids -- if only update --> ytb_limit_100 --> ytb_get_vids ytb_get_vids -- for each vid --> ytb_check_record_exists -- if not --> ytb_add_to_updates --> ytb_parse_vids --> ytb_persist_vids end subgraph fetch_smotrim_updates [ ] direction TB sm_set_params(Define params of update) sm_fetch_all(with pages = 10) sm_update(with pages = 2) sm_fetch_audio_updates(Fetch audio updates) sm_fetch_video_updates(Fetch video updates) sm_parse_audio_updates(Parse audio updates) sm_parse_video_updates(Parse video updates) sm_deduplicate(Remove internal duplicates) sm_check_exists(Check if record already exists in DB) sm_combine_updates(Add to final list) sm_persist(Save updates to the database) sm_temp( ) sm_set_params -- if fetch_all --> sm_fetch_all --> sm_temp sm_set_params -- if update --> sm_update --> sm_temp sm_temp -- podcast & brand --> sm_fetch_audio_updates --> sm_parse_audio_updates --> sm_deduplicate sm_temp -- brand --> sm_fetch_video_updates --> sm_parse_video_updates --> sm_deduplicate sm_deduplicate -- for each record --> sm_check_exists -- if not --> sm_combine_updates -- if any updates collected --> sm_persist end subgraph fetch_rutube_updates [ ] direction TB rtb_set_params1(Set basic params) rtb_fetch_all(with pages = 1000) rtb_fetch_new(with pages = 2) rtb_set_params2(Set page params) rtb_fetch_updates(Fetch updates) rtb_parse_updates(Parse updates) rtb_deduplicate(Remove internal duplicates) rtb_check_exists(Check if record already exists in DB) rtb_persist(Save updates to the database) rtb_set_params1 -- if fetch_all --> rtb_fetch_all --> rtb_set_params2 rtb_set_params1 -- if update --> rtb_fetch_new --> rtb_set_params2 rtb_set_params2 --> rtb_fetch_updates --> rtb_parse_updates --> rtb_deduplicate -- for each record --> rtb_check_exists -- if not --> rtb_persist end subgraph update_target [ ] direction TB fetch_target_updates(Collect target's updates) is_target_resolved -- no --> resolve_target -- if resolved --> fetch_target_updates -- YouTube --> fetch_ytb_updates mark_dead --> quit_on_resolve is_target_resolved -- yes --> fetch_target_updates -- Smotrim --> fetch_smotrim_updates fetch_target_updates -- Rutube --> fetch_rutube_updates end init_platform -- for each target --> update_target History :::warning Nothing :::","title":"ARR-1: Ave Media"},{"location":"arr/1/#arr-1-ave-media","text":"","title":"ARR-1: Ave Media"},{"location":"arr/1/#general","text":"Ave Media is an application for collecting new items from various media platforms. Process is initialized for each platform separately, which allows to run updates concurrently.","title":"General"},{"location":"arr/1/#code","text":"Modules involved: bases/w/media bases/w/database bases/w/config components/w/ave_media Source-specific implementations are located at bases/w/media/platforms . File platform_service contains base classes MediaPlatform and MediaTarget , which are parents to platform-specific implementations in correspondingly named files in the same directory.","title":"Code"},{"location":"arr/1/#entities-media-platforms","text":"Platform URL Status Targets table Episodes table Other tables Youtube youtube.com Active youtube_channels youtube_vids Smotrim.ru smotrim.ru Active smotrim_brands smotrim_episodes Rutube rutube.ru Active rutube_channels rutube_episodes Soundstream soundstream.media In development soundstream_channels soundstream_episodes soundstream_playlists VK vk.com To be added TBD Komsomolka radiokp.ru To be refactored TBD Dentv dentv.ru To be refactored TBD Ntv ntv.ru To be refactored TBD","title":"Entities -- Media Platforms"},{"location":"arr/1/#flow-diagram","text":"flowchart TD quit_on_resolve(Quit) is_target_resolved(Is target resolved?) subgraph collect_targets [Collect and init targets] direction TB fetch_channels(Fetch target-level entities from DB) fetch_channels -- for each --> init_target end subgraph init_target [Init target] direction TB assign_session2(Assign DB session) assign_channel(Assign target-level entity) set_unresolved(Set target as unresolved) fetch_latest(Fetch target's latest episode) set_empty(Set target for collecting all available updates\\nfetch_all = True) assign_session2 --- assign_channel -- if `title` is missing --> set_unresolved assign_channel --> fetch_latest -- if no latest item found --> set_empty end subgraph init_platform [Init platform] direction LR assign_session1(Assign DB session) assign_platform_specific(Assing platform-specific attributes) assign_session1 --- assign_platform_specific --- collect_targets assign_platform_specific -- if YouTube --> ytb(Youtube API client) end subgraph resolve_target [Resolve target] direction TB query_platform(Fetch target data from the platform) update_target_db(Update DB record) mark_resolved(Mark target as resolved) mark_dead(Mark target as defunct) query_platform -- if found --> mark_resolved --> update_target_db query_platform -- if not found --> mark_dead end subgraph fetch_ytb_updates [ ] direction TB ytb_get_playlist_vids(Use 'get_playlist_videos' method) ytb_limit_none(with limit = None) ytb_limit_100(with limit = 100) ytb_get_vids(Use 'videos' method for additional data) ytb_check_record_exists(Query DB to check if this record exists) ytb_add_to_updates(Add to final list) ytb_parse_vids(Parse all collected videos) ytb_persist_vids(Save videos to the database) ytb_get_playlist_vids -- if fetch_all --> ytb_limit_none --> ytb_get_vids ytb_get_playlist_vids -- if only update --> ytb_limit_100 --> ytb_get_vids ytb_get_vids -- for each vid --> ytb_check_record_exists -- if not --> ytb_add_to_updates --> ytb_parse_vids --> ytb_persist_vids end subgraph fetch_smotrim_updates [ ] direction TB sm_set_params(Define params of update) sm_fetch_all(with pages = 10) sm_update(with pages = 2) sm_fetch_audio_updates(Fetch audio updates) sm_fetch_video_updates(Fetch video updates) sm_parse_audio_updates(Parse audio updates) sm_parse_video_updates(Parse video updates) sm_deduplicate(Remove internal duplicates) sm_check_exists(Check if record already exists in DB) sm_combine_updates(Add to final list) sm_persist(Save updates to the database) sm_temp( ) sm_set_params -- if fetch_all --> sm_fetch_all --> sm_temp sm_set_params -- if update --> sm_update --> sm_temp sm_temp -- podcast & brand --> sm_fetch_audio_updates --> sm_parse_audio_updates --> sm_deduplicate sm_temp -- brand --> sm_fetch_video_updates --> sm_parse_video_updates --> sm_deduplicate sm_deduplicate -- for each record --> sm_check_exists -- if not --> sm_combine_updates -- if any updates collected --> sm_persist end subgraph fetch_rutube_updates [ ] direction TB rtb_set_params1(Set basic params) rtb_fetch_all(with pages = 1000) rtb_fetch_new(with pages = 2) rtb_set_params2(Set page params) rtb_fetch_updates(Fetch updates) rtb_parse_updates(Parse updates) rtb_deduplicate(Remove internal duplicates) rtb_check_exists(Check if record already exists in DB) rtb_persist(Save updates to the database) rtb_set_params1 -- if fetch_all --> rtb_fetch_all --> rtb_set_params2 rtb_set_params1 -- if update --> rtb_fetch_new --> rtb_set_params2 rtb_set_params2 --> rtb_fetch_updates --> rtb_parse_updates --> rtb_deduplicate -- for each record --> rtb_check_exists -- if not --> rtb_persist end subgraph update_target [ ] direction TB fetch_target_updates(Collect target's updates) is_target_resolved -- no --> resolve_target -- if resolved --> fetch_target_updates -- YouTube --> fetch_ytb_updates mark_dead --> quit_on_resolve is_target_resolved -- yes --> fetch_target_updates -- Smotrim --> fetch_smotrim_updates fetch_target_updates -- Rutube --> fetch_rutube_updates end init_platform -- for each target --> update_target","title":"Flow Diagram"},{"location":"arr/1/#history","text":":::warning Nothing :::","title":"History"},{"location":"arr/2/","text":"ARR-2: Periodicals General Periodicals is a data collector application that specifically targets websites of news outlets like newspapers, magazines, etc. The application operates over Website s (represented by websites table in the DB), referred to as sources in the application. There are 2 major parts to the flow: Fetcher and Scanner. Both are implemented as base classes from which source-specific child classes derive. In this context, adding a new source would mean defining both scanner and fetcher implementations with source-specific details. Code TBD Entities -- Websites TBD Flow Diagram Scanner Scanner is responsible for collecting new items from 'category' pages. flowchart TD subgraph init_update [Init scanner] direction TB get_target(Fetch source to scan) get_latest(Fetch source's latest item) get_target --> get_latest --> create_scanner end subgraph create_scanner [Invoke factory to create the scanner] direction LR assign_source(Assign source) assign_latest(Assign latest) selector(Disassemble selector) assign_source --> assign_latest --> selector end subgraph parse_upd [Extract] get_title(Title) get_excerpt(Excerpt) get_date(Publish date) get_author(Authors data) get_url(URL) get_additional(Additional data) get_title --- get_excerpt --- get_date --- get_author --- get_url --- get_additional end subgraph persist_upd [Check and persist] check_item(Check for presence in the DB) add_item(Add to the database) check_item -- if not present --> add_item end subgraph run_upd [Run Update] direction TB fetch_via_http(Fetch new items via HTTP) extract_items(Extract items from HTTP content) fetch_via_api(Fetch new items via API) dedupe(Drop duplicates from final list) fetch_via_http --> extract_items -- each item --> parse_upd fetch_via_api -- each item --> parse_upd parse_upd --> dedupe -- for each item --> persist_upd end call_update(Trigger 'update' endpoint) --> init_update --> run_upd --> commit(Commit updates to the database) Fetcher Fetcher is responsible for getting content of relevant articles (which is missing from the 'category' pages most of the time). flowchart TD subgraph create_fetcher [Invoke factory to create the fetcher] direction LR assign_page(Assing page) end subgraph init_fetch [Init fetcher] direction LR get_page(Get page for update) end subgraph parse_page [Extract] direction TB merge_vals(Combine new and old values) get_title(Title) --> merge_vals get_img(First image) --> merge_vals get_date(Publish date) --> merge_vals get_author(Authors data) --> merge_vals get_excerpt(Excerpt) --> merge_vals get_url(URL) --> merge_vals get_additional(Additional data) --> merge_vals end subgraph resolve_page [ ] fetch_httpx(Niquests) fetch_pl(Playwright) extract_content(Extract content) fetch_httpx --> extract_content --> parse_page fetch_pl --> extract_content end call_fetch(Trigger 'fetch' endpoint) --> init_fetch --> create_fetcher --> resolve_page --> commit(Commit updates to the database)","title":"ARR-2: Periodicals"},{"location":"arr/2/#arr-2-periodicals","text":"","title":"ARR-2: Periodicals"},{"location":"arr/2/#general","text":"Periodicals is a data collector application that specifically targets websites of news outlets like newspapers, magazines, etc. The application operates over Website s (represented by websites table in the DB), referred to as sources in the application. There are 2 major parts to the flow: Fetcher and Scanner. Both are implemented as base classes from which source-specific child classes derive. In this context, adding a new source would mean defining both scanner and fetcher implementations with source-specific details.","title":"General"},{"location":"arr/2/#code","text":"TBD","title":"Code"},{"location":"arr/2/#entities-websites","text":"TBD","title":"Entities -- Websites"},{"location":"arr/2/#flow-diagram","text":"","title":"Flow Diagram"},{"location":"arr/2/#scanner","text":"Scanner is responsible for collecting new items from 'category' pages. flowchart TD subgraph init_update [Init scanner] direction TB get_target(Fetch source to scan) get_latest(Fetch source's latest item) get_target --> get_latest --> create_scanner end subgraph create_scanner [Invoke factory to create the scanner] direction LR assign_source(Assign source) assign_latest(Assign latest) selector(Disassemble selector) assign_source --> assign_latest --> selector end subgraph parse_upd [Extract] get_title(Title) get_excerpt(Excerpt) get_date(Publish date) get_author(Authors data) get_url(URL) get_additional(Additional data) get_title --- get_excerpt --- get_date --- get_author --- get_url --- get_additional end subgraph persist_upd [Check and persist] check_item(Check for presence in the DB) add_item(Add to the database) check_item -- if not present --> add_item end subgraph run_upd [Run Update] direction TB fetch_via_http(Fetch new items via HTTP) extract_items(Extract items from HTTP content) fetch_via_api(Fetch new items via API) dedupe(Drop duplicates from final list) fetch_via_http --> extract_items -- each item --> parse_upd fetch_via_api -- each item --> parse_upd parse_upd --> dedupe -- for each item --> persist_upd end call_update(Trigger 'update' endpoint) --> init_update --> run_upd --> commit(Commit updates to the database)","title":"Scanner"},{"location":"arr/2/#fetcher","text":"Fetcher is responsible for getting content of relevant articles (which is missing from the 'category' pages most of the time). flowchart TD subgraph create_fetcher [Invoke factory to create the fetcher] direction LR assign_page(Assing page) end subgraph init_fetch [Init fetcher] direction LR get_page(Get page for update) end subgraph parse_page [Extract] direction TB merge_vals(Combine new and old values) get_title(Title) --> merge_vals get_img(First image) --> merge_vals get_date(Publish date) --> merge_vals get_author(Authors data) --> merge_vals get_excerpt(Excerpt) --> merge_vals get_url(URL) --> merge_vals get_additional(Additional data) --> merge_vals end subgraph resolve_page [ ] fetch_httpx(Niquests) fetch_pl(Playwright) extract_content(Extract content) fetch_httpx --> extract_content --> parse_page fetch_pl --> extract_content end call_fetch(Trigger 'fetch' endpoint) --> init_fetch --> create_fetcher --> resolve_page --> commit(Commit updates to the database)","title":"Fetcher"},{"location":"arr/3/","text":"ARR-3: Transfactory General Transfactory is an app that transforms massive amounts of video/audio files into text using OpenAI's Whisper. It operates in conjunction with Transsuply around a special database table which serves as a queue: the supply app adds new jobs to it for the factory to pick up. The necessity for update is due to change in the principal alogrithm: where the old one used official Whisper model with no optimization, the new one uses WhisperX, a highly optimized version that offers a significant increase in processing speed, as well as improved output. Code TBD Entities TBD Flow Diagram flowchart TB classDef exit_red fill:#cd0d3b subgraph check_lines [Check for existing lines] check_transcribed(Check if transcription exists) set_need_transcription(Set 'need_transcription' to False) check_translated(Check if translation exists) set_need_translation(Set 'need_translation' to False) check_transcribed -- if exists --> set_need_transcription check_translated -- if exists --> set_need_translation end subgraph assign_transcript [Get & assign transcript] direction TB check_existence(Check if Transcript for 'path' already exists) get_transcript(Get existing transcript) create_transcript>Create new transcript] assign_transcript_id(Assing transcript ID) check_existence -- if found --> get_transcript --> assign_transcript_id check_existence -- if not found --> create_transcript --> assign_transcript_id end subgraph assign_initial [ ] assign_db_session(Assign Database session) assign_machine_id(Assign machine ID) assign_trans_settings(Assign transcription settings) assign_storage_client(Assign storage client) assign_auth_settings(Assign internal auth settings) assign_db_session --- assign_machine_id --- assign_trans_settings --- assign_storage_client --- assign_auth_settings end subgraph assign_new_job [ ] new_job(Get new job from PRABYSS table) exit_after_new_job(Exit) set_in_progress>Set job's status to 'in progress'] new_job -- if none found --> exit_after_new_job:::exit_red new_job -- if found --> set_in_progress end subgraph init_service [Init] direction TB get_machine(Fetch additional Machine's params from DB) hardware_settings(Extract settings related to hardware) decide_translation(Make decision if translation is needed) create_stats(Create FactoryJob object to hold job's stats) exit_on_lines_check(Exit) create_engine(Create WhisperX engine) assign_initial --> get_machine --> hardware_settings --> create_engine assign_initial --> assign_new_job --> decide_translation --> create_stats --> assign_transcript --> check_lines --> create_engine check_lines -- if both transcription\\nand translation exist --> exit_on_lines_check:::exit_red end subgraph perform_whisper_job [ ] direction TB load_model(Load model) load_audio(Load audio) run_task(Produce raw Whisper result) align_result(Perform alignment) diarize_result(Diarize result) cleanup_resources(Unload resources and clear the memory) collect_timestamps(Collect timestamps) return_result(Return diarized result and timestamps) load_model --> load_audio --> run_task --> align_result --> diarize_result --> cleanup_resources --> collect_timestamps --> return_result end subgraph handle_job [ ] direction TB collect_stats(Collect stats) shape_lines(Convert raw output into DB objects) persist_lines>Add converted lines to DB session] scan_anomalies(Scan output for anomalies) persist_anomalies>Add anomalies to DB session] perform_whisper_job --> collect_stats --> shape_lines --> persist_lines --> scan_anomalies -- if any found --> persist_anomalies end subgraph verify_job_success [ ] direction TB check_(Verify job success) verify_only_transcribed(Check just transcription) verify_transcribed_and_translated(Check transcription and translation) check_ -- if lang = en --> verify_only_transcribed check_ -- if lang != en --> verify_transcribed_and_translated end subgraph handle_voiceprints [Handle voiceprints] direction TB ensure_auth(Ensure auth token exists and is valid) extract_raw_speakers(Collect unique speakers from transcription output) extract_speaker_audio(Extract 60 seconds of voice material) make_voiceprint_request>Make request to voiceprint endpoint] extract_raw_speakers -- for each --> extract_speaker_audio --> make_voiceprint_request ensure_auth --- make_voiceprint_request end subgraph run [Run] direction TB handle_translation(Handle translation) handle_transcription(Handle transcription) download_audio(Download audio file) measure_file_size(Measure file size) local_cleanup(Do local cleanup) drop_from_remote(Remove audio from remote storage) set_job_completed>Add job to DB session with status 'completed'] add_stats_db>Add stats object to DB session] persist_to_db>Persist everything to the database] exit_if_no_lines(Exit) exit_after_verify(Exit) download_audio -.- measure_file_size download_audio -- if need_transcription = True --> handle_transcription -- if need_translation = True --> handle_translation --> local_cleanup --> verify_job_success -- Success --> handle_voiceprints --> set_job_completed --> add_stats_db --> persist_to_db --> drop_from_remote handle_transcription -.- handle_job handle_translation -.- handle_job handle_job -- if no lines produced --> exit_if_no_lines:::exit_red verify_job_success -- Failure --> exit_after_verify:::exit_red end init_service --> run","title":"ARR-3: Transfactory"},{"location":"arr/3/#arr-3-transfactory","text":"","title":"ARR-3: Transfactory"},{"location":"arr/3/#general","text":"Transfactory is an app that transforms massive amounts of video/audio files into text using OpenAI's Whisper. It operates in conjunction with Transsuply around a special database table which serves as a queue: the supply app adds new jobs to it for the factory to pick up. The necessity for update is due to change in the principal alogrithm: where the old one used official Whisper model with no optimization, the new one uses WhisperX, a highly optimized version that offers a significant increase in processing speed, as well as improved output.","title":"General"},{"location":"arr/3/#code","text":"TBD","title":"Code"},{"location":"arr/3/#entities","text":"TBD","title":"Entities"},{"location":"arr/3/#flow-diagram","text":"flowchart TB classDef exit_red fill:#cd0d3b subgraph check_lines [Check for existing lines] check_transcribed(Check if transcription exists) set_need_transcription(Set 'need_transcription' to False) check_translated(Check if translation exists) set_need_translation(Set 'need_translation' to False) check_transcribed -- if exists --> set_need_transcription check_translated -- if exists --> set_need_translation end subgraph assign_transcript [Get & assign transcript] direction TB check_existence(Check if Transcript for 'path' already exists) get_transcript(Get existing transcript) create_transcript>Create new transcript] assign_transcript_id(Assing transcript ID) check_existence -- if found --> get_transcript --> assign_transcript_id check_existence -- if not found --> create_transcript --> assign_transcript_id end subgraph assign_initial [ ] assign_db_session(Assign Database session) assign_machine_id(Assign machine ID) assign_trans_settings(Assign transcription settings) assign_storage_client(Assign storage client) assign_auth_settings(Assign internal auth settings) assign_db_session --- assign_machine_id --- assign_trans_settings --- assign_storage_client --- assign_auth_settings end subgraph assign_new_job [ ] new_job(Get new job from PRABYSS table) exit_after_new_job(Exit) set_in_progress>Set job's status to 'in progress'] new_job -- if none found --> exit_after_new_job:::exit_red new_job -- if found --> set_in_progress end subgraph init_service [Init] direction TB get_machine(Fetch additional Machine's params from DB) hardware_settings(Extract settings related to hardware) decide_translation(Make decision if translation is needed) create_stats(Create FactoryJob object to hold job's stats) exit_on_lines_check(Exit) create_engine(Create WhisperX engine) assign_initial --> get_machine --> hardware_settings --> create_engine assign_initial --> assign_new_job --> decide_translation --> create_stats --> assign_transcript --> check_lines --> create_engine check_lines -- if both transcription\\nand translation exist --> exit_on_lines_check:::exit_red end subgraph perform_whisper_job [ ] direction TB load_model(Load model) load_audio(Load audio) run_task(Produce raw Whisper result) align_result(Perform alignment) diarize_result(Diarize result) cleanup_resources(Unload resources and clear the memory) collect_timestamps(Collect timestamps) return_result(Return diarized result and timestamps) load_model --> load_audio --> run_task --> align_result --> diarize_result --> cleanup_resources --> collect_timestamps --> return_result end subgraph handle_job [ ] direction TB collect_stats(Collect stats) shape_lines(Convert raw output into DB objects) persist_lines>Add converted lines to DB session] scan_anomalies(Scan output for anomalies) persist_anomalies>Add anomalies to DB session] perform_whisper_job --> collect_stats --> shape_lines --> persist_lines --> scan_anomalies -- if any found --> persist_anomalies end subgraph verify_job_success [ ] direction TB check_(Verify job success) verify_only_transcribed(Check just transcription) verify_transcribed_and_translated(Check transcription and translation) check_ -- if lang = en --> verify_only_transcribed check_ -- if lang != en --> verify_transcribed_and_translated end subgraph handle_voiceprints [Handle voiceprints] direction TB ensure_auth(Ensure auth token exists and is valid) extract_raw_speakers(Collect unique speakers from transcription output) extract_speaker_audio(Extract 60 seconds of voice material) make_voiceprint_request>Make request to voiceprint endpoint] extract_raw_speakers -- for each --> extract_speaker_audio --> make_voiceprint_request ensure_auth --- make_voiceprint_request end subgraph run [Run] direction TB handle_translation(Handle translation) handle_transcription(Handle transcription) download_audio(Download audio file) measure_file_size(Measure file size) local_cleanup(Do local cleanup) drop_from_remote(Remove audio from remote storage) set_job_completed>Add job to DB session with status 'completed'] add_stats_db>Add stats object to DB session] persist_to_db>Persist everything to the database] exit_if_no_lines(Exit) exit_after_verify(Exit) download_audio -.- measure_file_size download_audio -- if need_transcription = True --> handle_transcription -- if need_translation = True --> handle_translation --> local_cleanup --> verify_job_success -- Success --> handle_voiceprints --> set_job_completed --> add_stats_db --> persist_to_db --> drop_from_remote handle_transcription -.- handle_job handle_translation -.- handle_job handle_job -- if no lines produced --> exit_if_no_lines:::exit_red verify_job_success -- Failure --> exit_after_verify:::exit_red end init_service --> run","title":"Flow Diagram"},{"location":"arr/4/","text":"ARR-4: Tgram General Tgram application periodically scans a collection of Tg channels for updates and persists them to the database. Secondary function of the app is resolution of new channels. Resolving empty channels Normally, new channels are added to the database with just handle attribute, and therefore require fetching additional data from TelegramAPI and Telemetr. :::info Telemetr is a platform that collects Telegram channels' statistics. Their API allows for a limited number of requests over a month (1000), which is more than sufficient for resolution, but not enough for consistent data collection. ::: The process of channel resolution first queries TelegramAPI for channel's details, and also collects channel's first published message (which is usually a technical message about channel creation). Then Telemetr API is queried; the data is combined into a db object and persisted with a quirk . :::warning The quirk has to do with Telemetr IDs and inconsistencies in how they are stored in the db. This is mainly a historical artifact that would be fully deprecated in one of future versions. It doesn't effect the flow. See the fetch part of the flow diagram below. ::: Scanning for updates Scanning process get the least updated channel from the db, and fetches 200 messages using Telegram API counting from the max known message id for this channel. Then messages are filtered, converted and persisted; channel params are updated correspondingly. Known issues and limitations Telegram imposes limits on the API usage, which are dynamic and not strictly predictable. In practice, you get a FLOODWAIT exception after making a certain number of requests over a certain period of time. The exception would contain the amount of seconds you need to wait before making another request - this can be used to put the app to sleep until then. Code TBD Entities TBD Flow Diagram flowchart TD subgraph floodwait [FLOODWAIT] flood0(Elevate FloodWait exception) flood1(Set 'endpoint_disabled' to TRUE) flood2(Read number of seconds from the exception meta) flood3(Run 're_enable_endpoint_after_delay') enable_endpoints(Set 'endpoint_disabled' to FALSE) wait(Wait amount of time required by Telegram API) flood0 --> flood1 --> flood2 --> flood3 --> wait --> enable_endpoints end subgraph init [Initialize service] direction LR step21(Create Pyrogram client) step22(Create database client) step23(Start the Telegram service) step21 --- step22 --- step23 end subgraph process [ ] step31(Fetch most outdated channel from DB) step32(Read ID of the latest channel message, or set to 1) step33(Define range of messages IDs to fetch) step34(Use Telegram API to fetch the messages) step35(Filter out messages with 'empty' = True) step36(Parse the messages into db objects) step37(Save the orig ID of the most recent of the new messages) step31 --> step32 --> step33 --> step34 --> step35 --> step36 --> step37 end subgraph resolve [ ] step41(Fetch unresolved channel from DB) step42(Fetch channel data from Telegram API) step43(Fetch channel's 1st message from Telegram API) step44(Query Telemetr for channel data) step45(Create instance of TgChannelResolve) step46(Set status DEFUNCT on all channels with this TelemetrID) step47(Update record with this handle and status ACTIVE with data collected) subgraph fetch_channel_data [ ] step42 --- step43 --- step44 end step41 --> fetch_channel_data --> step45 --> step46 --> step47 end subgraph persist [ ] persist1(Persist the messages) persist2(Update channel with 'last_scanned_on' and 'last_known_msg_id') persist1 --- persist2 end trigger_update(Trigger update endpoint) trigger_resolve(Trigger resolve endpoint) check_disabled(Read 'endpoint_disabled') no_updates(Update channel with 'last_scanned_on') mark_dead(Mark channel as dead) resolved(Consider channel to be resolved) trigger_update --> check_disabled -- wait if needed --> init -- Run update --> process process -- Updates not found --> no_updates process -- Updates found --> persist process -- UsernameInvalid\\nUsernameNotOccupied --> mark_dead process --> floodwait trigger_resolve --> check_disabled init -- Run resolve --> resolve --> resolved resolve -- UsernameInvalid\\nUsernameNotOccupied --> mark_dead resolve --> floodwait","title":"ARR-4: Tgram"},{"location":"arr/4/#arr-4-tgram","text":"","title":"ARR-4: Tgram"},{"location":"arr/4/#general","text":"Tgram application periodically scans a collection of Tg channels for updates and persists them to the database. Secondary function of the app is resolution of new channels.","title":"General"},{"location":"arr/4/#resolving-empty-channels","text":"Normally, new channels are added to the database with just handle attribute, and therefore require fetching additional data from TelegramAPI and Telemetr. :::info Telemetr is a platform that collects Telegram channels' statistics. Their API allows for a limited number of requests over a month (1000), which is more than sufficient for resolution, but not enough for consistent data collection. ::: The process of channel resolution first queries TelegramAPI for channel's details, and also collects channel's first published message (which is usually a technical message about channel creation). Then Telemetr API is queried; the data is combined into a db object and persisted with a quirk . :::warning The quirk has to do with Telemetr IDs and inconsistencies in how they are stored in the db. This is mainly a historical artifact that would be fully deprecated in one of future versions. It doesn't effect the flow. See the fetch part of the flow diagram below. :::","title":"Resolving empty channels"},{"location":"arr/4/#scanning-for-updates","text":"Scanning process get the least updated channel from the db, and fetches 200 messages using Telegram API counting from the max known message id for this channel. Then messages are filtered, converted and persisted; channel params are updated correspondingly.","title":"Scanning for updates"},{"location":"arr/4/#known-issues-and-limitations","text":"Telegram imposes limits on the API usage, which are dynamic and not strictly predictable. In practice, you get a FLOODWAIT exception after making a certain number of requests over a certain period of time. The exception would contain the amount of seconds you need to wait before making another request - this can be used to put the app to sleep until then.","title":"Known issues and limitations"},{"location":"arr/4/#code","text":"TBD","title":"Code"},{"location":"arr/4/#entities","text":"TBD","title":"Entities"},{"location":"arr/4/#flow-diagram","text":"flowchart TD subgraph floodwait [FLOODWAIT] flood0(Elevate FloodWait exception) flood1(Set 'endpoint_disabled' to TRUE) flood2(Read number of seconds from the exception meta) flood3(Run 're_enable_endpoint_after_delay') enable_endpoints(Set 'endpoint_disabled' to FALSE) wait(Wait amount of time required by Telegram API) flood0 --> flood1 --> flood2 --> flood3 --> wait --> enable_endpoints end subgraph init [Initialize service] direction LR step21(Create Pyrogram client) step22(Create database client) step23(Start the Telegram service) step21 --- step22 --- step23 end subgraph process [ ] step31(Fetch most outdated channel from DB) step32(Read ID of the latest channel message, or set to 1) step33(Define range of messages IDs to fetch) step34(Use Telegram API to fetch the messages) step35(Filter out messages with 'empty' = True) step36(Parse the messages into db objects) step37(Save the orig ID of the most recent of the new messages) step31 --> step32 --> step33 --> step34 --> step35 --> step36 --> step37 end subgraph resolve [ ] step41(Fetch unresolved channel from DB) step42(Fetch channel data from Telegram API) step43(Fetch channel's 1st message from Telegram API) step44(Query Telemetr for channel data) step45(Create instance of TgChannelResolve) step46(Set status DEFUNCT on all channels with this TelemetrID) step47(Update record with this handle and status ACTIVE with data collected) subgraph fetch_channel_data [ ] step42 --- step43 --- step44 end step41 --> fetch_channel_data --> step45 --> step46 --> step47 end subgraph persist [ ] persist1(Persist the messages) persist2(Update channel with 'last_scanned_on' and 'last_known_msg_id') persist1 --- persist2 end trigger_update(Trigger update endpoint) trigger_resolve(Trigger resolve endpoint) check_disabled(Read 'endpoint_disabled') no_updates(Update channel with 'last_scanned_on') mark_dead(Mark channel as dead) resolved(Consider channel to be resolved) trigger_update --> check_disabled -- wait if needed --> init -- Run update --> process process -- Updates not found --> no_updates process -- Updates found --> persist process -- UsernameInvalid\\nUsernameNotOccupied --> mark_dead process --> floodwait trigger_resolve --> check_disabled init -- Run resolve --> resolve --> resolved resolve -- UsernameInvalid\\nUsernameNotOccupied --> mark_dead resolve --> floodwait","title":"Flow Diagram"},{"location":"db/additional_schemas/","text":"All the tables actively used in the project belong to the public schema. To support storage of additional data that might be useful in the future for the current project, or a related one, two additional schemas were created: future and service . Service service schema is meant to contain additional data that is not explicitly related to the project but is required for some related purpose. Contains following tables: first_name_trans_conventions Technical table, listing conventions for translating Russian/Ukrainian names into English. This is to avoid having 'Maksim' and 'Maxim' as this could lead to duplicates. prabyss . Used in the DVE-A-40. Future future schema contains a number of tables that may become a part of the current project in the future, or may span out a separate project. As of the time of creation, the following tables were attributed to the future schema: meduza_dow_stream Contains parsed messages of the Meduza daily text streams on the ongoing events. Parsed here means that empty/technical messages are filtered out, and the data format is standardized, but the content is not verified for relevance. Most recently updated on May 2, 2023, with data up to May 1, 2023. Tables related to Ukraine's geography: This cluster of tables contains all Ukraine's administrative units as per the 2019 administrative reform. Most of the data is taken from https://zakon.rada.gov.ua/rada/show/v0290914-20#Text (the most recent version of the list dated December 16, 2021). Whenever possible locations were matched against the Nova Poshta database (it uses the old administrative system and doesn't have the occupied localities), from which lat, lon, indexes and russian versions of names were taken. Approximately 2/3 of all localities (\\~19K/29K) were resolved. ua_regions Regions, plus Crimea, Kyiv and Sevastopol. ua_regions_districts Districts make up regions. ua_district_hromadas Territorial units (\u0433\u0440\u043e\u043c\u0430\u0434\u0438) make up regional districts. ua_localities Localities are cities, villages, etc. ua_localities_districts Some localities are further divided into districts. ua_locality_types table contains types of localities, which is used in ua_localities table. modrf_briefings outdated Data on the MOD RF briefings has been collected from various sources, including Telegram, VK and YouTube. Some of the data is cleaned and JSONified, but only to a certain point. tryvoga_alerts outdated Data on the air-raid alerts in localities of Ukraine. Obtained by parsing Telegram channel. Data data schema contains data-heavy entities, namely: telegram_messages . Related to telegram_channels in public schema. Contains posts published in relevant telegram channels. Constitutes \\~90% of the DB size. telegram_channels_stats printed_content . Related to printed in public schema. Contains chapters of books, articles, blog posts. transcripts & transcribed_content . Table transcripts contains basic info on the transcribing jobs completed by transfactory (related to various tables in public such as smotrim_episodes ). Table transcribed_content contains lines from the resulting srt files processed into convenient format (related to transcripts ) Using Tables in Alternative Schemas To allow using an additional schema called schemaname , the following steps must be complete: Create schema schemaname ; Add schemaname to Exposed schemas and Extra search paths fields in Settings \u2192 API (dashboard); Run these 2 queries: sql GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA schemaname TO postgres, authenticated, service_role, dashboard_user; GRANT USAGE ON SCHEMA schemaname TO postgres, authenticated, service_role, dashboard_user; When these are done, schemaname can be used in the following manner. Moving table to another schema alter table tablename set schema schemaname; Connecting with and without schema indication This is a regular connection function, which will use public schema by default. def connect_supabase(url = os.getenv(\"SUPABASE_URL\"), key = os.getenv(\"SUPABASE_KEY\")): return supabase.create_client(url, key, options) This is a version of the same function with an additional argument schema that defaults to public . def connect_supabase(url = os.getenv(\"SUPABASE_URL\"), key = os.getenv(\"SUPABASE_KEY\"), schema = \"public\"): from supabase.lib.client_options import ClientOptions options = ClientOptions() options.schema = schema return supabase.create_client(url, key, options) Use it like so (given that credentials exist as environment variables): connect = connect_supabase(schema=\"schemaname\")","title":"Additional Schemas"},{"location":"db/additional_schemas/#service","text":"service schema is meant to contain additional data that is not explicitly related to the project but is required for some related purpose. Contains following tables: first_name_trans_conventions Technical table, listing conventions for translating Russian/Ukrainian names into English. This is to avoid having 'Maksim' and 'Maxim' as this could lead to duplicates. prabyss . Used in the DVE-A-40.","title":"Service"},{"location":"db/additional_schemas/#future","text":"future schema contains a number of tables that may become a part of the current project in the future, or may span out a separate project. As of the time of creation, the following tables were attributed to the future schema: meduza_dow_stream Contains parsed messages of the Meduza daily text streams on the ongoing events. Parsed here means that empty/technical messages are filtered out, and the data format is standardized, but the content is not verified for relevance. Most recently updated on May 2, 2023, with data up to May 1, 2023. Tables related to Ukraine's geography: This cluster of tables contains all Ukraine's administrative units as per the 2019 administrative reform. Most of the data is taken from https://zakon.rada.gov.ua/rada/show/v0290914-20#Text (the most recent version of the list dated December 16, 2021). Whenever possible locations were matched against the Nova Poshta database (it uses the old administrative system and doesn't have the occupied localities), from which lat, lon, indexes and russian versions of names were taken. Approximately 2/3 of all localities (\\~19K/29K) were resolved. ua_regions Regions, plus Crimea, Kyiv and Sevastopol. ua_regions_districts Districts make up regions. ua_district_hromadas Territorial units (\u0433\u0440\u043e\u043c\u0430\u0434\u0438) make up regional districts. ua_localities Localities are cities, villages, etc. ua_localities_districts Some localities are further divided into districts. ua_locality_types table contains types of localities, which is used in ua_localities table. modrf_briefings outdated Data on the MOD RF briefings has been collected from various sources, including Telegram, VK and YouTube. Some of the data is cleaned and JSONified, but only to a certain point. tryvoga_alerts outdated Data on the air-raid alerts in localities of Ukraine. Obtained by parsing Telegram channel.","title":"Future"},{"location":"db/additional_schemas/#data","text":"data schema contains data-heavy entities, namely: telegram_messages . Related to telegram_channels in public schema. Contains posts published in relevant telegram channels. Constitutes \\~90% of the DB size. telegram_channels_stats printed_content . Related to printed in public schema. Contains chapters of books, articles, blog posts. transcripts & transcribed_content . Table transcripts contains basic info on the transcribing jobs completed by transfactory (related to various tables in public such as smotrim_episodes ). Table transcribed_content contains lines from the resulting srt files processed into convenient format (related to transcripts )","title":"Data"},{"location":"db/additional_schemas/#using-tables-in-alternative-schemas","text":"To allow using an additional schema called schemaname , the following steps must be complete: Create schema schemaname ; Add schemaname to Exposed schemas and Extra search paths fields in Settings \u2192 API (dashboard); Run these 2 queries: sql GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA schemaname TO postgres, authenticated, service_role, dashboard_user; GRANT USAGE ON SCHEMA schemaname TO postgres, authenticated, service_role, dashboard_user; When these are done, schemaname can be used in the following manner.","title":"Using Tables in Alternative Schemas"},{"location":"db/additional_schemas/#moving-table-to-another-schema","text":"alter table tablename set schema schemaname;","title":"Moving table to another schema"},{"location":"db/additional_schemas/#connecting-with-and-without-schema-indication","text":"This is a regular connection function, which will use public schema by default. def connect_supabase(url = os.getenv(\"SUPABASE_URL\"), key = os.getenv(\"SUPABASE_KEY\")): return supabase.create_client(url, key, options) This is a version of the same function with an additional argument schema that defaults to public . def connect_supabase(url = os.getenv(\"SUPABASE_URL\"), key = os.getenv(\"SUPABASE_KEY\"), schema = \"public\"): from supabase.lib.client_options import ClientOptions options = ClientOptions() options.schema = schema return supabase.create_client(url, key, options) Use it like so (given that credentials exist as environment variables): connect = connect_supabase(schema=\"schemaname\")","title":"Connecting with and without schema indication"},{"location":"db/new_table_checklist/","text":"When adding a new table to the W database, one must consider, first, whether it is a central core table, or a sattelite table. Central core table is a table that stores data directly usable in the project one way or another. Most tables in the database belong to this group. Sattelite table contains secondary data, most often related to internal processes, such as maintaining jobs queue or storing quality control data. As a general rule, for central core, both provenance and incremental backup flows should be enabled, while for satellite tables, only incremental backup is required. incremental backup flow operates on Pydantic/SQLAlchemy models, so adding such model for the table is critical. On the other hand, the Django-based web application only uses central core tables. Central Core Satellite enable provenance Y N enable incremental backup Y Y Pydantic/SQLAlchemy model add SQLAlchemy model to wapatools.bases.database.models Y Y update wapatools.bases.incbackup_base.incbackup_db.py Y Y add Pydantic model (DTO) to wapatools.bases.database.models Y Y update wapatools.bases.incbackup_base.incbackup_dto.py Y Y Django model add Django model to core.models (backend repo) Y N Enabling provenance Enabling provenance flow for a table, means creating for this table one of the following triggers: -- applicable to most tables CREATE OR REPLACE TRIGGER trg_TABLE_NAME AFTER UPDATE OR DELETE OR INSERT ON SCHEMA_NAME.TABLE_NAME FOR EACH ROW EXECUTE FUNCTION trg_change(); -- applicable to data-heavy tables where a lot of insert operation are expected to happen in automated fashion CREATE OR REPLACE TRIGGER trg_TABLE_NAME AFTER UPDATE OR DELETE ON SCHEMA_NAME.TABLE_NAME FOR EACH ROW EXECUTE FUNCTION trg_change(); Where TABLE_NAME and SCHEMA_NAME should be replaced with actual table and schema names. Enabling incremental backup CREATE OR REPLACE TRIGGER trg_backup_youtube_channels_stats AFTER UPDATE OR DELETE OR INSERT ON data.youtube_channels_stats FOR EACH ROW EXECUTE FUNCTION mark4backup(); Additional Considerations If the table is expected to have translatable or transliteretable values, consider using custom TripleLang composite type. More details here .","title":"New Table Checklist"},{"location":"db/new_table_checklist/#enabling-provenance","text":"Enabling provenance flow for a table, means creating for this table one of the following triggers: -- applicable to most tables CREATE OR REPLACE TRIGGER trg_TABLE_NAME AFTER UPDATE OR DELETE OR INSERT ON SCHEMA_NAME.TABLE_NAME FOR EACH ROW EXECUTE FUNCTION trg_change(); -- applicable to data-heavy tables where a lot of insert operation are expected to happen in automated fashion CREATE OR REPLACE TRIGGER trg_TABLE_NAME AFTER UPDATE OR DELETE ON SCHEMA_NAME.TABLE_NAME FOR EACH ROW EXECUTE FUNCTION trg_change(); Where TABLE_NAME and SCHEMA_NAME should be replaced with actual table and schema names.","title":"Enabling provenance"},{"location":"db/new_table_checklist/#enabling-incremental-backup","text":"CREATE OR REPLACE TRIGGER trg_backup_youtube_channels_stats AFTER UPDATE OR DELETE OR INSERT ON data.youtube_channels_stats FOR EACH ROW EXECUTE FUNCTION mark4backup();","title":"Enabling incremental backup"},{"location":"db/new_table_checklist/#additional-considerations","text":"If the table is expected to have translatable or transliteretable values, consider using custom TripleLang composite type. More details here .","title":"Additional Considerations"},{"location":"db/people_bundles/","text":"Bundles are custom groups of people. 4 types of bundles are defined ( as of November 27, 2022 ). Subjective Subjective bundle is a virtual group where membership is defined by purely subjective speculations. Alias: s Example: {\"en\": \"Mean Old Jerks Club\", \"ru\": \"\u041a\u043b\u0443\u0431 \u0441\u0442\u0430\u0440\u044b\u0445 \u0437\u043b\u043e\u0431\u043d\u044b\u0445 \u043c\u0443\u0434\u0430\u043a\u043e\u0432\"} ID Name Reasoning 1 Mean Old Jerks Club A person is aged, and behaves like a mean jerk 6 Likely random people A person is probably not related to propagandistic activities 58 Sociopaths A person has spoken at least once in favor of either public executions, mass executions or death squads, or have demonstrated sociopathic tendencies otherwise 63 Opportunists A person is likely to not have solid convictions of their own Objective Objective bundle is a virtual group where membership could be defined via objectively existing features. Alias: o Example: {\"en\":\"Solovyov pack\",\"ru\":\"\u0421\u043e\u043b\u043e\u0432\u044c\u0435\u0432\u0441\u043a\u0430\u044f \u0441\u0442\u0430\u044f\"} ID Name Reasoning 53 Own correspondents abroad A person works for a Russian media in the capacity of foreign correspondent 52 Religious figure A person is a priest, a shaman, a mufti or some other kind of professional god servant 47 Military correspondent A person is a correspondent (journalist) and has worked in the war zone is this capacity 45 International surveyors A person is a foreign national (towards Russia) and has participated in at least one RF electoral episode as a surveyor 42 Diplomats A person has served as a professional diplomat 25 PhD A person has a doctorate 24 Afghan veterans A person has participated in the Soviet-Afghan war 1980-1989 22 Communists A person has expressed their allegiance to the idea of communism 19 Influencers A person actively works towards gaining larger audience online 21 Religious fanatics A person has expressed extreme ideas of religious essence 20 Politicians A person has participated in formal political process 75 Soldiers A person is a part of an army, with any kind of rank 2 Solovyov pack A person has at least once appeared on air on Solovyov Live TV channel in the capacity of a host, or a reporter 54 Cultural figure A person is a professional singer, writer, poet, performer, musician etc. and acquired some fame in that capacity Nazi A person has expressed at least one of the following convictions: - that Russian nation is superior; - that Ukrainian nation is inferior; - that Ukrainian nation is not a nation (i.e. did not occur naturally but was somebody's project); - TBD Expert Expert bundle is a virtual group where membership is defined by claimed expertise in any field of activity/knowledge. Alias: e Example: {\"en\":\"Political science\",\"ru\":\"\u041f\u043e\u043b\u0438\u0442\u043e\u043b\u043e\u0433\u0438\u044f\",\"uk\":\"\u041f\u043e\u043b\u0456\u0442\u043e\u043b\u043e\u0433\u0456\u044f\"} National National bundle is a virtual group where membership is defined by either [claimed | actual] belonging to an ethnic group, or citizenship . Alias: n Example: {\"en\":\"Ukrainians\",\"ru\":\"\u0423\u043a\u0440\u0430\u0438\u043d\u0446\u044b\",\"uk\":\"\u0423\u043a\u0440\u0430\u0457\u043d\u0446\u0456\"}","title":"People Bundles"},{"location":"db/people_bundles/#subjective","text":"Subjective bundle is a virtual group where membership is defined by purely subjective speculations. Alias: s Example: {\"en\": \"Mean Old Jerks Club\", \"ru\": \"\u041a\u043b\u0443\u0431 \u0441\u0442\u0430\u0440\u044b\u0445 \u0437\u043b\u043e\u0431\u043d\u044b\u0445 \u043c\u0443\u0434\u0430\u043a\u043e\u0432\"} ID Name Reasoning 1 Mean Old Jerks Club A person is aged, and behaves like a mean jerk 6 Likely random people A person is probably not related to propagandistic activities 58 Sociopaths A person has spoken at least once in favor of either public executions, mass executions or death squads, or have demonstrated sociopathic tendencies otherwise 63 Opportunists A person is likely to not have solid convictions of their own","title":"Subjective"},{"location":"db/people_bundles/#objective","text":"Objective bundle is a virtual group where membership could be defined via objectively existing features. Alias: o Example: {\"en\":\"Solovyov pack\",\"ru\":\"\u0421\u043e\u043b\u043e\u0432\u044c\u0435\u0432\u0441\u043a\u0430\u044f \u0441\u0442\u0430\u044f\"} ID Name Reasoning 53 Own correspondents abroad A person works for a Russian media in the capacity of foreign correspondent 52 Religious figure A person is a priest, a shaman, a mufti or some other kind of professional god servant 47 Military correspondent A person is a correspondent (journalist) and has worked in the war zone is this capacity 45 International surveyors A person is a foreign national (towards Russia) and has participated in at least one RF electoral episode as a surveyor 42 Diplomats A person has served as a professional diplomat 25 PhD A person has a doctorate 24 Afghan veterans A person has participated in the Soviet-Afghan war 1980-1989 22 Communists A person has expressed their allegiance to the idea of communism 19 Influencers A person actively works towards gaining larger audience online 21 Religious fanatics A person has expressed extreme ideas of religious essence 20 Politicians A person has participated in formal political process 75 Soldiers A person is a part of an army, with any kind of rank 2 Solovyov pack A person has at least once appeared on air on Solovyov Live TV channel in the capacity of a host, or a reporter 54 Cultural figure A person is a professional singer, writer, poet, performer, musician etc. and acquired some fame in that capacity Nazi A person has expressed at least one of the following convictions: - that Russian nation is superior; - that Ukrainian nation is inferior; - that Ukrainian nation is not a nation (i.e. did not occur naturally but was somebody's project); - TBD","title":"Objective"},{"location":"db/people_bundles/#expert","text":"Expert bundle is a virtual group where membership is defined by claimed expertise in any field of activity/knowledge. Alias: e Example: {\"en\":\"Political science\",\"ru\":\"\u041f\u043e\u043b\u0438\u0442\u043e\u043b\u043e\u0433\u0438\u044f\",\"uk\":\"\u041f\u043e\u043b\u0456\u0442\u043e\u043b\u043e\u0433\u0456\u044f\"}","title":"Expert"},{"location":"db/people_bundles/#national","text":"National bundle is a virtual group where membership is defined by either [claimed | actual] belonging to an ethnic group, or citizenship . Alias: n Example: {\"en\":\"Ukrainians\",\"ru\":\"\u0423\u043a\u0440\u0430\u0438\u043d\u0446\u044b\",\"uk\":\"\u0423\u043a\u0440\u0430\u0457\u043d\u0446\u0456\"}","title":"National"},{"location":"db/personal_info_addendum/","text":"Some of the records in the people table contain additional information that might be considered sensitive by some. This document is a list of fields containing such information. raw usually refers to strings, and means that it was taken from the source as is, i.e. a string may contain spaces and other unwanted characters address List of addresses that are possibly associated with the person. Type List of strings (raw) contact Type JSONB Possible keys phones List of strings (raw) emails List of strings telegram List of strings (raw). Refers to personal account in Telegram. Maybe in different formats associates List of people known to be associated with the person. Type List of JSONB Required keys relationship . String. Must always be present . Indicates the type of relationship connecting person with the associate. Can be wife , mother , son , partner , collegue etc. id . Integer. Either this or name must be present . Refers to the ID of a person in the people table. If present, no other keys are required except relationship . name . JSON object - triple language format . Either this or id must be present . Possible keys dob . String. The format is usually YYYY-MM-DD contact . JSON object similar in structure to the contact field. additional . JSON object similar in structure to the additional field. address . List of strings (raw). Similar in structure to the address field. social . List of strings. URLs of person's profile in social networks. Similar in structure to the social field. raw . Example object [ { \"name\": {\"en\": \"Veronica\", \"ru\": \"\u0412\u0435\u0440\u043e\u043d\u0438\u043a\u0430\", \"uk\": \"\u0412\u0454\u0440\u043e\u043d\u0456\u043a\u0430\"}, \"relationship\": \"daughter\" }, { \"dob\": \"1996-08-01\", \"name\": {\"en\": \"Olga Savchenkova\", \"ru\": \"\u041e\u043b\u044c\u0433\u0430 \u0421\u0430\u0432\u0447\u0435\u043d\u043a\u043e\u0432\u0430\", \"uk\": \"\u041e\u043b\u044c\u0433\u0430 \u0421\u0430\u0432\u0447\u0435\u043d\u043a\u043e\u0432\u0430\"}, \"social\": [\"http://polynkova.tilda.ws/#rec167782553\", \"https://kinolift.com/24633\"], \"contact\": { \"emails\": [\"o.polynkova@mail.ru\"], \"phones\": [\"+79002280187\"], \"telegram\": [\"@war_criminal\"] }, \"alias\": [ {\"en\": \"Captain Nakedbottom\", \"ru\": \"\u041e\u043a\u0441\u0430\u043d\u0430 \u0412\u043f\u0441\u0438\u043d\u0443\u0433\u0440\u044b\u0437\", \"uk\": \"\u041e\u043a\u0441\u0430\u043d\u0430 \u0420\u0430\u0448\u0438\u0441\u0442\u043a\u0430\"} ], \"additional\": { \"passport\": { \"number\": \"66 10 556462\", \"issued by\": \"a russian state agency\", \"issued on\": \"some time in the past\" }, \"tin\": \"5131654614654163\", \"edrpou\": \"24563541\", }, \"relationship\": \"wife\" \"raw\": {\"biography\": \"She was born and still lives today\", \"work\": \"that company over there\"} } ] additional Type JSONB Possible keys passport . List of JSON objects. Possible keys: number . String (raw) issued by . String (raw) issued on . String (raw) edrpou . String (raw). Refers to a Ukrainian legal entity ID. tin . List of strings. Refers to a person's tax identifier. Could be either Russian or Ukrainian, which are differently structured. drivers license . String. auto . List of JSON objects. Refers to the vehicles associated with the person. Possible keys: plate . String vin . String make . String year . String urls . List of strings. Dumpster for URLs that have anything to do with the person.","title":"Personal Information Addendum"},{"location":"db/personal_info_addendum/#address","text":"List of addresses that are possibly associated with the person.","title":"address"},{"location":"db/personal_info_addendum/#type","text":"List of strings (raw)","title":"Type"},{"location":"db/personal_info_addendum/#contact","text":"","title":"contact"},{"location":"db/personal_info_addendum/#type_1","text":"JSONB","title":"Type"},{"location":"db/personal_info_addendum/#possible-keys","text":"phones List of strings (raw) emails List of strings telegram List of strings (raw). Refers to personal account in Telegram. Maybe in different formats","title":"Possible keys"},{"location":"db/personal_info_addendum/#associates","text":"List of people known to be associated with the person.","title":"associates"},{"location":"db/personal_info_addendum/#type_2","text":"List of JSONB","title":"Type"},{"location":"db/personal_info_addendum/#required-keys","text":"relationship . String. Must always be present . Indicates the type of relationship connecting person with the associate. Can be wife , mother , son , partner , collegue etc. id . Integer. Either this or name must be present . Refers to the ID of a person in the people table. If present, no other keys are required except relationship . name . JSON object - triple language format . Either this or id must be present .","title":"Required keys"},{"location":"db/personal_info_addendum/#possible-keys_1","text":"dob . String. The format is usually YYYY-MM-DD contact . JSON object similar in structure to the contact field. additional . JSON object similar in structure to the additional field. address . List of strings (raw). Similar in structure to the address field. social . List of strings. URLs of person's profile in social networks. Similar in structure to the social field. raw .","title":"Possible keys"},{"location":"db/personal_info_addendum/#example-object","text":"[ { \"name\": {\"en\": \"Veronica\", \"ru\": \"\u0412\u0435\u0440\u043e\u043d\u0438\u043a\u0430\", \"uk\": \"\u0412\u0454\u0440\u043e\u043d\u0456\u043a\u0430\"}, \"relationship\": \"daughter\" }, { \"dob\": \"1996-08-01\", \"name\": {\"en\": \"Olga Savchenkova\", \"ru\": \"\u041e\u043b\u044c\u0433\u0430 \u0421\u0430\u0432\u0447\u0435\u043d\u043a\u043e\u0432\u0430\", \"uk\": \"\u041e\u043b\u044c\u0433\u0430 \u0421\u0430\u0432\u0447\u0435\u043d\u043a\u043e\u0432\u0430\"}, \"social\": [\"http://polynkova.tilda.ws/#rec167782553\", \"https://kinolift.com/24633\"], \"contact\": { \"emails\": [\"o.polynkova@mail.ru\"], \"phones\": [\"+79002280187\"], \"telegram\": [\"@war_criminal\"] }, \"alias\": [ {\"en\": \"Captain Nakedbottom\", \"ru\": \"\u041e\u043a\u0441\u0430\u043d\u0430 \u0412\u043f\u0441\u0438\u043d\u0443\u0433\u0440\u044b\u0437\", \"uk\": \"\u041e\u043a\u0441\u0430\u043d\u0430 \u0420\u0430\u0448\u0438\u0441\u0442\u043a\u0430\"} ], \"additional\": { \"passport\": { \"number\": \"66 10 556462\", \"issued by\": \"a russian state agency\", \"issued on\": \"some time in the past\" }, \"tin\": \"5131654614654163\", \"edrpou\": \"24563541\", }, \"relationship\": \"wife\" \"raw\": {\"biography\": \"She was born and still lives today\", \"work\": \"that company over there\"} } ]","title":"Example object"},{"location":"db/personal_info_addendum/#additional","text":"","title":"additional"},{"location":"db/personal_info_addendum/#type_3","text":"JSONB","title":"Type"},{"location":"db/personal_info_addendum/#possible-keys_2","text":"passport . List of JSON objects. Possible keys: number . String (raw) issued by . String (raw) issued on . String (raw) edrpou . String (raw). Refers to a Ukrainian legal entity ID. tin . List of strings. Refers to a person's tax identifier. Could be either Russian or Ukrainian, which are differently structured. drivers license . String. auto . List of JSON objects. Refers to the vehicles associated with the person. Possible keys: plate . String vin . String make . String year . String urls . List of strings. Dumpster for URLs that have anything to do with the person.","title":"Possible keys"},{"location":"db/popular_stats/","text":"Popular Stats is a nickname for a layer of components showing current stats related to the project. The following pieces of statistics should be calculated and displayed: Number of patients (population) Average/Median age across the db Gender distribution (male v. female) Number of registered media hours Number of transcribed media hours Number of registered and relevant media segments Number of relevant organizations Number of relevant Telegram channels Number of written works on file (articles, books) TBD UI TBD","title":"Popular Stats"},{"location":"db/psycopg/","text":"psycopg is an advanced Python adapter/connector for PostgreSQL databases. The library is at v3, but goes without a version in the name, i.e. simply psycopg , as opposed to psycopg2 , which is a previous generation. The documentation can be found on a dedicated website , and is relatively full. Installation Installation is more or less straightforward: poetry add psycopg[binary] Usage Main objects are connection and cursor . You would usually create a connection , and then a cursor that you'd use to perform transactions. In the previous version of the library, cursor was the primary object you dealt with. With the new version, you don't need to create a cursor (though you still can), because connection object has the same execute method, which creates a cursor object implicitly when called. The library is rather versatile, and allows various approaches: you can use main objects as context managers; there is an async version for every function; the format of the return result can be customized with row_factories ; To get dictionaries, pass row_factory=psycopg.rows.dict_row when creating connection; you can configure your own types if you need to. Caveats Psycopg has a behaviour that may seem surprising compared to psql \\: by default, any database operation will start a new transaction. As a consequence, changes made by any cursor of the connection will not be visible until Connection.commit() is called, and will be discarded by Connection.rollback() . The following operation on the same connection will start a new transaction. If a database operation fails, the server will refuse further commands, until a rollback() is called. This can be circumvented by setting autocommit = True when creating connection, or using transaction context . To access table in a schema other than public , use construct sql.Identifier(\"schema_name\", \"table_name\") . Types Adaptation With boolean s, numeric s, string s, uuid s or binary types are converted automatically and predictably; With date s datetime s etc, it should be datetime objects on Python side; With json , standard serializers are used by default, but it can be customized; SQL Query Composition Simple queries can be passed as regular strings, but it is advisable to default to the safe way by using the sql module of the library (mostly, to establish a habit, since more complex types require it anyway). A few examples: from psycopg import sql select_all = sql.SQL(\"SELECT * FROM {tbl};\").format(tbl=sql.Identifier(\"people\")) req = connect.execute(select_all ) one_field = sql.SQL(\"SELECT {id} FROM {tbl};\").format(tbl=sql.Identifier(\"people\"), id=sql.Identifier(\"id\")) req = connect.execute(one_field ) several_fields_w_condition = sql.SQL(\"SELECT {fields} from {tbl} WHERE {condition_field} = 'complete';\").format( tbl=sql.Identifier(\"prabyss\"), fields=sql.SQL(\",\").join([sql.Identifier(\"id\"), sql.Identifier(\"title\")]), condition_field=sql.Identifier(\"status\") ) req = connect.execute(several_fields_w_condition) select_key_of_jsonb_column = sql.SQL(\"SELECT {fld} FROM {tbl} WHERE {fld2} = %s\").format( fld=sql.SQL(\"jsonb_extract_path({content, 'en', 'markdown'})\"), tbl=sql.Identifier(\"theory\"), fld2=sql.Identifier(\"id\") ) req = connect.execute(select_key_of_jsonb_column , (record_id,)).fetchone() update_key_in_jsonb_column = sql.SQL(\"UPDATE {tbl} SET {fld} = {jset} WHERE {fld2} = %s\").format( fld=sql.Identifier(\"content\"), tbl=sql.Identifier(\"theory\"), fld2=sql.Identifier('id'), jset=sql.SQL(\"jsonb_set({fld}, '{pth}', %s)\").format(fld=sql.Identifier(\"content\"), pth = sql.SQL('{en}')) ) connect.execute(update_key_in_jsonb_column , (json.dumps({\"markdown\": new_version}), record_id)) A few things to notice: There are several different objects used in query composition, the most common of which are Identifier and SQL . Use first to point to columns and tables; second - to wrap all other portions of the query; Curly brackets are used for variable placeholders. This is similar to how f-strings are formed in Python, and this similarity may be confusing. The rule of thumb would be to assume that f-strings cannot be used at all when shaping queries, even though in pieces of SQL that do not contain variable placeholders, they are still possible. A special %s placeholder is used to point at values. At execution stage, values must be passed in an iterable (even if there's just one): (record_id,) and not simply record_id . It is possible to use named placeholders, i.e. %(title)s , in which case, the iterable must be a dictionary where keys correspond to the names of placeholders. Copy For massive inserts, copy operation is preferable . Below is an example of inserting objects from a Python iterable parsed_records , which is a list of dictionaries: from psycopg import sql from wapaganda.database.core2 import create_connection parsed_records = [...] # this list of full of dictionaries connect = create_connection() cur = connect.cursor() upload_query = sql.SQL(\"COPY {tbl} ({fields}) FROM STDIN\").format( tbl=sql.Identifier(\"schema\", \"table_in_schema\"), fields=sql.SQL(\",\").join([sql.Identifier(x) for x in parsed_records[0].keys()]) ) with cur.copy(upload_query) as copy: for rec in parsed_records: copy.write_row(list(rec.values())) Things to notice: create_connection function returns connection object with proper auth elements; copy operation is performed on the cursor object, not the connection object; list comprehension is applied to the 1st record in the list to obtain field names. Note: this requires for the parsed_records list to contain at least one record, i.e. the query has to be declared after you collected the data; when using dictionaries, pass .values() to write_row() method and don't forget to convert it into list . copy can be used for other things, such as loading data from a CSV file, or copying data between tables. Consult Postgres documentation for full functionality.","title":"psycopg In a Gist"},{"location":"db/psycopg/#installation","text":"Installation is more or less straightforward: poetry add psycopg[binary]","title":"Installation"},{"location":"db/psycopg/#usage","text":"Main objects are connection and cursor . You would usually create a connection , and then a cursor that you'd use to perform transactions. In the previous version of the library, cursor was the primary object you dealt with. With the new version, you don't need to create a cursor (though you still can), because connection object has the same execute method, which creates a cursor object implicitly when called. The library is rather versatile, and allows various approaches: you can use main objects as context managers; there is an async version for every function; the format of the return result can be customized with row_factories ; To get dictionaries, pass row_factory=psycopg.rows.dict_row when creating connection; you can configure your own types if you need to.","title":"Usage"},{"location":"db/psycopg/#caveats","text":"Psycopg has a behaviour that may seem surprising compared to psql \\: by default, any database operation will start a new transaction. As a consequence, changes made by any cursor of the connection will not be visible until Connection.commit() is called, and will be discarded by Connection.rollback() . The following operation on the same connection will start a new transaction. If a database operation fails, the server will refuse further commands, until a rollback() is called. This can be circumvented by setting autocommit = True when creating connection, or using transaction context . To access table in a schema other than public , use construct sql.Identifier(\"schema_name\", \"table_name\") .","title":"Caveats"},{"location":"db/psycopg/#types-adaptation","text":"With boolean s, numeric s, string s, uuid s or binary types are converted automatically and predictably; With date s datetime s etc, it should be datetime objects on Python side; With json , standard serializers are used by default, but it can be customized;","title":"Types Adaptation"},{"location":"db/psycopg/#sql-query-composition","text":"Simple queries can be passed as regular strings, but it is advisable to default to the safe way by using the sql module of the library (mostly, to establish a habit, since more complex types require it anyway). A few examples: from psycopg import sql select_all = sql.SQL(\"SELECT * FROM {tbl};\").format(tbl=sql.Identifier(\"people\")) req = connect.execute(select_all ) one_field = sql.SQL(\"SELECT {id} FROM {tbl};\").format(tbl=sql.Identifier(\"people\"), id=sql.Identifier(\"id\")) req = connect.execute(one_field ) several_fields_w_condition = sql.SQL(\"SELECT {fields} from {tbl} WHERE {condition_field} = 'complete';\").format( tbl=sql.Identifier(\"prabyss\"), fields=sql.SQL(\",\").join([sql.Identifier(\"id\"), sql.Identifier(\"title\")]), condition_field=sql.Identifier(\"status\") ) req = connect.execute(several_fields_w_condition) select_key_of_jsonb_column = sql.SQL(\"SELECT {fld} FROM {tbl} WHERE {fld2} = %s\").format( fld=sql.SQL(\"jsonb_extract_path({content, 'en', 'markdown'})\"), tbl=sql.Identifier(\"theory\"), fld2=sql.Identifier(\"id\") ) req = connect.execute(select_key_of_jsonb_column , (record_id,)).fetchone() update_key_in_jsonb_column = sql.SQL(\"UPDATE {tbl} SET {fld} = {jset} WHERE {fld2} = %s\").format( fld=sql.Identifier(\"content\"), tbl=sql.Identifier(\"theory\"), fld2=sql.Identifier('id'), jset=sql.SQL(\"jsonb_set({fld}, '{pth}', %s)\").format(fld=sql.Identifier(\"content\"), pth = sql.SQL('{en}')) ) connect.execute(update_key_in_jsonb_column , (json.dumps({\"markdown\": new_version}), record_id)) A few things to notice: There are several different objects used in query composition, the most common of which are Identifier and SQL . Use first to point to columns and tables; second - to wrap all other portions of the query; Curly brackets are used for variable placeholders. This is similar to how f-strings are formed in Python, and this similarity may be confusing. The rule of thumb would be to assume that f-strings cannot be used at all when shaping queries, even though in pieces of SQL that do not contain variable placeholders, they are still possible. A special %s placeholder is used to point at values. At execution stage, values must be passed in an iterable (even if there's just one): (record_id,) and not simply record_id . It is possible to use named placeholders, i.e. %(title)s , in which case, the iterable must be a dictionary where keys correspond to the names of placeholders.","title":"SQL Query Composition"},{"location":"db/psycopg/#copy","text":"For massive inserts, copy operation is preferable . Below is an example of inserting objects from a Python iterable parsed_records , which is a list of dictionaries: from psycopg import sql from wapaganda.database.core2 import create_connection parsed_records = [...] # this list of full of dictionaries connect = create_connection() cur = connect.cursor() upload_query = sql.SQL(\"COPY {tbl} ({fields}) FROM STDIN\").format( tbl=sql.Identifier(\"schema\", \"table_in_schema\"), fields=sql.SQL(\",\").join([sql.Identifier(x) for x in parsed_records[0].keys()]) ) with cur.copy(upload_query) as copy: for rec in parsed_records: copy.write_row(list(rec.values())) Things to notice: create_connection function returns connection object with proper auth elements; copy operation is performed on the cursor object, not the connection object; list comprehension is applied to the 1st record in the list to obtain field names. Note: this requires for the parsed_records list to contain at least one record, i.e. the query has to be declared after you collected the data; when using dictionaries, pass .values() to write_row() method and don't forget to convert it into list . copy can be used for other things, such as loading data from a CSV file, or copying data between tables. Consult Postgres documentation for full functionality.","title":"Copy"},{"location":"db/tg_message_pgroonga_postmortem/","text":"Table telegram_messages in data schema has been an issue due to its size (\\~11M records and \\~14Gb of disk space). For a long time, creating pgroonga index on the content column was impossible due to Supabase limitations. However, the task proved non-trivial even with those limitations removed. Attempt to create the index on the existing data failed (see DVE-264 for details) Unsuccessful solutions : Create temp table, move all the data there, create index on empty or almost empty table, move data back. Reason for failure: All steps except for the last one went just fine, however moving the data onto the indexed table turned out to be extremely slow, and because the process was initiated for the entire data set from a local client, it failed when the client stopped working (for unrelated reasons). As a result, data moved to the temp table was lost. Luckily, we have it backed up. 3. Data was restored from backup into a regular table mimicing the original structure of telegram_messages . After that, I attempted to move it in batches. The process was very-very slow, so it promted me to introduce queueing functionality so that this process can take place directly on the server in automatic fashion. 4. In the middle of one of the jobs, an unrelated request to search_in_people_skim stored function with empty input cause the database to panic and restart. This led to corruption of the pgroonga index. 5. By that point, \\~2M records were already copied over. I created a new index (took \\~15 minutes) and dropped the old one (client hang on this operation, but the index was indeed removed). 6. Copying operation resumed after this, turned out to have much faster pace: less than 24 hours later \\~80-85% of the data was already moved, while the first 15-20% took several days.","title":"Telegram Message pgroonga Index Postmortem"},{"location":"historical/clusterfuck1/","text":"On Friday, December 22, 2023 files from the data folder of the production database got removed due to negligence on the part of the project's owner. Context On Monday, December 18, 2023 I started a major refactoring work on the database (TODO: some references). The day before, on Dec 17, 2023, a backup of data was created by @atatatko on my request. The refactoring progressed quite well, although not without bumps and obstacles. Part of refactoring involved updates to the largest table we have, telegram_messages , in particular - fixing an error with missing subtype, and optimizing content field by dropping unnecessary data. Since the latter operation had to be performed on every record in the table, and the process was taking rather long due to its size, a function was developed to perform the necessary processing with PL\\Python and then scheduled to run every 30 seconds, multiple times. Event At a certain point, I had 100 of these crawlers updating telegram_messages . Then, in a moment of madness, I decided that the type of field for telegram_channel_id just has to be bigint instead of some measly integer . Launching this update led to a deadlock and inevitable corruption of the index associated with the content field, and to the database going unresponsive. I tried restarting the service, but discovered that the main drive is all out of space, which was not normal, given the size of the database at the time at around 45Gb (the drive is \\~150Gb). I lockated the directory that occupied the most space; it was Postgres' data directory. In the second moment of madness, I removed everything from it indiscriminately. This went down at about 16:00 Kyiv time. Damage Control & Discoveries The data was removed in a particularly nasty way, as it not only led to the loss of data, but made the database unaccessible via regular means. It was still possible to connect to it directly with sudo -u postgres psql This is how I discovered that only wapadb database was killed, while the secondary databases ( logs , windmill ) were pretty much intact. I exported data from all their tables from within psql . Attempt to recover the deleted files with Linux utility testdisk > photorec did produce some files, but nothing that looks like db data fiiles. Then I saw no other choice but to set up instance from scratch. This involved installing Postgres anew and restoring 1) schema; 2) data. Version 16 of the database was installed instead of v15, because why not. For database schema, we had a dump dated October 31, 2023. Using the fact that Pycharm still had the db metadata saved locally, I manually updated the dump with current definitions. Then I took multiple attempts on running the migration, fixing errors (with missing schema qualification, tables placed in the wrong place, etc.) along the way, until I finally got it right. Some missing pieces were restored from project files and documentation afterwards. The data backup, which was supposed to have been made on December 17, was missing some tables ( dentv_episodes ), and for others, the most recent records were dated Nov 3, 2023. Data from the backup was restored without much issue, however due to the gap between most recent records and the clusterfuck event, restoration of data from local files (such as .srt files produced by transfactory ) and/or updating tables via regular means, apparently, led to some mixups (specifically noticed on smotrim_episodes ). The cause of mixup is unclear. By EOD Saturday, December 23, access to the database was restored, most of the crucial data was restored, and web application was back online. Conclusions and Updates Instead of logs database, meta database was created to hold logs, provenance records and other similar stuff. Backup procedure must be established in the nearest future to avoid data loss / mixup. Id field on provenance table is now bigint , not uid . Lost data: public.dentv_episodes service.prabyss service.factory_jobs_run_details service.provenance future ? TBD","title":"The 221223 Clusterfuck Postmortem"},{"location":"historical/clusterfuck1/#context","text":"On Monday, December 18, 2023 I started a major refactoring work on the database (TODO: some references). The day before, on Dec 17, 2023, a backup of data was created by @atatatko on my request. The refactoring progressed quite well, although not without bumps and obstacles. Part of refactoring involved updates to the largest table we have, telegram_messages , in particular - fixing an error with missing subtype, and optimizing content field by dropping unnecessary data. Since the latter operation had to be performed on every record in the table, and the process was taking rather long due to its size, a function was developed to perform the necessary processing with PL\\Python and then scheduled to run every 30 seconds, multiple times.","title":"Context"},{"location":"historical/clusterfuck1/#event","text":"At a certain point, I had 100 of these crawlers updating telegram_messages . Then, in a moment of madness, I decided that the type of field for telegram_channel_id just has to be bigint instead of some measly integer . Launching this update led to a deadlock and inevitable corruption of the index associated with the content field, and to the database going unresponsive. I tried restarting the service, but discovered that the main drive is all out of space, which was not normal, given the size of the database at the time at around 45Gb (the drive is \\~150Gb). I lockated the directory that occupied the most space; it was Postgres' data directory. In the second moment of madness, I removed everything from it indiscriminately. This went down at about 16:00 Kyiv time.","title":"Event"},{"location":"historical/clusterfuck1/#damage-control-discoveries","text":"The data was removed in a particularly nasty way, as it not only led to the loss of data, but made the database unaccessible via regular means. It was still possible to connect to it directly with sudo -u postgres psql This is how I discovered that only wapadb database was killed, while the secondary databases ( logs , windmill ) were pretty much intact. I exported data from all their tables from within psql . Attempt to recover the deleted files with Linux utility testdisk > photorec did produce some files, but nothing that looks like db data fiiles. Then I saw no other choice but to set up instance from scratch. This involved installing Postgres anew and restoring 1) schema; 2) data. Version 16 of the database was installed instead of v15, because why not. For database schema, we had a dump dated October 31, 2023. Using the fact that Pycharm still had the db metadata saved locally, I manually updated the dump with current definitions. Then I took multiple attempts on running the migration, fixing errors (with missing schema qualification, tables placed in the wrong place, etc.) along the way, until I finally got it right. Some missing pieces were restored from project files and documentation afterwards. The data backup, which was supposed to have been made on December 17, was missing some tables ( dentv_episodes ), and for others, the most recent records were dated Nov 3, 2023. Data from the backup was restored without much issue, however due to the gap between most recent records and the clusterfuck event, restoration of data from local files (such as .srt files produced by transfactory ) and/or updating tables via regular means, apparently, led to some mixups (specifically noticed on smotrim_episodes ). The cause of mixup is unclear. By EOD Saturday, December 23, access to the database was restored, most of the crucial data was restored, and web application was back online.","title":"Damage Control &amp; Discoveries"},{"location":"historical/clusterfuck1/#conclusions-and-updates","text":"Instead of logs database, meta database was created to hold logs, provenance records and other similar stuff. Backup procedure must be established in the nearest future to avoid data loss / mixup. Id field on provenance table is now bigint , not uid . Lost data: public.dentv_episodes service.prabyss service.factory_jobs_run_details service.provenance future ? TBD","title":"Conclusions and Updates"},{"location":"historical/onboarding1/","text":"Architecture Web application is implemented with React on frontend and Django on backend; in addition, wapatools is a Python repo with various tools, some of which are used by the web app, while others have to do with collection, parsing and cleaning the data. More details on the web app could be found in DVE-A-52. Python Dependency Management Dependencies for the web app [???????] wapatools is handled with Poetry and follows the polylith approach with regard to repository structure. See DVE-A-59 for details of transitioning the repo to this structure. Database The project is built around a database, which is hosted on Supabase , - a service providing an alternative to Firebase (a Google product). More information on it could be found here DVE-A-11. Important: as a general rule, tables and columns have comments that aim to be as descriptive as possible. They can be retrieved with a SQL function (or a Supabase API call that launches the same function). Snaplet Snaplet is a tool that allows you to create and use database snapshots. We need it so that we don't have to develop the codebase against the prod DB thus risking all kinds of stuff. See Snaplet 101 for more details. Service The application is hosted on GitHub (repository) and Railway. Railway is a platform for hosting applications of all sorts. Secrets are handled by Doppler. See DVE-A-5 for setup instructions.","title":"Quick Onboarding. General Information and Setting Up"},{"location":"historical/onboarding1/#architecture","text":"Web application is implemented with React on frontend and Django on backend; in addition, wapatools is a Python repo with various tools, some of which are used by the web app, while others have to do with collection, parsing and cleaning the data. More details on the web app could be found in DVE-A-52.","title":"Architecture"},{"location":"historical/onboarding1/#python-dependency-management","text":"Dependencies for the web app [???????] wapatools is handled with Poetry and follows the polylith approach with regard to repository structure. See DVE-A-59 for details of transitioning the repo to this structure.","title":"Python Dependency Management"},{"location":"historical/onboarding1/#database","text":"The project is built around a database, which is hosted on Supabase , - a service providing an alternative to Firebase (a Google product). More information on it could be found here DVE-A-11. Important: as a general rule, tables and columns have comments that aim to be as descriptive as possible. They can be retrieved with a SQL function (or a Supabase API call that launches the same function).","title":"Database"},{"location":"historical/onboarding1/#snaplet","text":"Snaplet is a tool that allows you to create and use database snapshots. We need it so that we don't have to develop the codebase against the prod DB thus risking all kinds of stuff. See Snaplet 101 for more details.","title":"Snaplet"},{"location":"historical/onboarding1/#service","text":"The application is hosted on GitHub (repository) and Railway. Railway is a platform for hosting applications of all sorts. Secrets are handled by Doppler. See DVE-A-5 for setup instructions.","title":"Service"},{"location":"historical/original_setup/","text":"This document describes the configuration of the Wapaganda project as set up for the first time. Versions are current as of the moment of writing this (October 5, 2022). Hardware Processor Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz, 2592 Mhz Software Operating system Windows 10 (Version 10.0.19044 Build 19044) x64 Python 3.10.5 (3.10 is the minimum) IDE PyCharm 2022.2.2 (CE) Build #PC-222.4167.33, built on September 15, 2022 Runtime version 17.0.4+7-b469.53 amd64 VM OpenJDK 64-Bit Server VM by JetBrains s.r.o. Other Scoop Doppler CLI (installed through scoop ) doppler-env (Python dependency; installed through pip ) Railway CLI (installed through scoop ) WSL + Ubuntu 20.04.5 LTS Canonical (v. 2004.5.11.0) curl (installed through WSL ) Snaplet CLI (installed through WSL -> curl ) Browsers (to access the interface locally): Firefox 105.0.1 (64-bit) Chrome Version 106.0.5249.91 (Official Build) (64-bit)","title":"Original Setup"},{"location":"historical/original_setup/#hardware","text":"Processor Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz, 2592 Mhz","title":"Hardware"},{"location":"historical/original_setup/#software","text":"","title":"Software"},{"location":"historical/original_setup/#operating-system","text":"Windows 10 (Version 10.0.19044 Build 19044) x64","title":"Operating system"},{"location":"historical/original_setup/#python","text":"3.10.5 (3.10 is the minimum)","title":"Python"},{"location":"historical/original_setup/#ide","text":"PyCharm 2022.2.2 (CE) Build #PC-222.4167.33, built on September 15, 2022","title":"IDE"},{"location":"historical/original_setup/#runtime-version","text":"17.0.4+7-b469.53 amd64","title":"Runtime version"},{"location":"historical/original_setup/#vm","text":"OpenJDK 64-Bit Server VM by JetBrains s.r.o.","title":"VM"},{"location":"historical/original_setup/#other","text":"Scoop Doppler CLI (installed through scoop ) doppler-env (Python dependency; installed through pip ) Railway CLI (installed through scoop ) WSL + Ubuntu 20.04.5 LTS Canonical (v. 2004.5.11.0) curl (installed through WSL ) Snaplet CLI (installed through WSL -> curl ) Browsers (to access the interface locally): Firefox 105.0.1 (64-bit) Chrome Version 106.0.5249.91 (Official Build) (64-bit)","title":"Other"},{"location":"historical/snaplet/","text":"Snaplet is a tool for creating and using database snapshots for development purposes. Setting Up Prerequisites Two Postgres instances, one of which is the production database, and the other is development database (empty); A read-only role in the production Postgres project; Superuser access to the development DB Postgres project; ALTER USER postgres WITH superuser; Connection strings from both DBs (use read-only role for the production one). Snaplet requires PSQL to be installed Windows: choco install psql MacOS: brew install libpq Linux: apt install postgresql-client PSQL bin directory should be accessible through the PATH Windows: set PATH=%PATH%;\"%LOCALAPPDATA%\\Programs\\pgAdmin 4\\v6\\runtime\" MacOS: export PATH=\"/usr/local/opt/libpq/bin:$PATH\" Process Go to https://www.snaplet.dev/ , log into the account; set up the team and project. Use CONNECTION_STRING for the production database and establish a connection. In the 2nd step exclude schemas that you don't want to save; then follow through and make sure the process finishes without issues. Install Snaplet CLI with curl -sL https://app.snaplet.dev/get-cli/ | bash To use on Windows, you have two options: install WSL and an appropriate Linux distribution (for example, canonical Ubuntu 20.04.5) or use Bash which comes with any Windows Git distribution. In the terminal, navigate to your project's local directory and run bash snaplet config setup Enter the CONNECTION_STRING for the development database connection string when prompted (it also can be found in the Supabase Dev panel: Project Settings -> Database Settings -> Connections String -> URI ). This command creates a .snaplet directory and the configuration files used by the CLI Later, during making changes to the development database, you may want to switch between the development and production URL. The safest option to do that is through pre-configured Doppler secrets. Use SNAPLET_SOURCE_DATABASE_URL for creating the snapshot, and SNAPLET_DATABASE_URL for restoring one. We'll be using exactly this method below. {width=70%} Using Snaplet To create a snapshot Documentation We assume, all requirements are met, including the SNAPLET_SOURCE_DATABASE_URL secret in Doppler. To access the secret from the production environment, you should use the Snaplet prod Service token. You generate the token once, and keep it in a password manager. More about Sevice Tokens see in a related article or a Doppler official documentation # Prevent command with Service Token being recorded in bash history export HISTIGNORE='doppler run*' doppler run --token='dp.st.prd.xxxx' -- snaplet snapshot capture Snapshots are automatically stored in $HOME/.snaplet/snapshots , but you can specify a path: doppler run --token='dp.st.prd.xxxx' -- snaplet snapshot capture /path/to/stored-snapshot To restore a snapshot You restore the snapshot with another simple command snaplet snapshot restore For snapshot restoring, you should write some permissions to a target database. Make sure you don't write into a production database! Even though, multiple measures were taken to prevent it: Secret SNAPLET_DATABASE_URL in a production Doppler configuration deliberately left empty. There's a zero chance we will use Snaplet to overwrite the Production database. If we restore the snapshot through the CI script tools/snapshot_tool.py , it prevents restoring the database if the URL looks like the Production # Prevent command with Service Token being recorded in bash history export HISTIGNORE='doppler run*' doppler run --token='dp.st.dev.xxxx' -- snaplet snapshot restore Change schema after restoring a snapshot After restoring the snapshot, it usually overwrites all tables except Database Schema. This prevents accessing the restored database through PostgREST or API with HTTP Error 401. In order to fix it, we must run the PSQL script tools/schema_permission.sql in a newly created database. This should fix the access rights and restore the schema. Other commands To list existing snapshots snaplet snapshot list To list existing configs snaplet config list To save config locally snaplet config pull More CLI commands are here .","title":"Snaplet 101"},{"location":"historical/snaplet/#setting-up","text":"","title":"Setting Up"},{"location":"historical/snaplet/#prerequisites","text":"Two Postgres instances, one of which is the production database, and the other is development database (empty); A read-only role in the production Postgres project; Superuser access to the development DB Postgres project; ALTER USER postgres WITH superuser; Connection strings from both DBs (use read-only role for the production one). Snaplet requires PSQL to be installed Windows: choco install psql MacOS: brew install libpq Linux: apt install postgresql-client PSQL bin directory should be accessible through the PATH Windows: set PATH=%PATH%;\"%LOCALAPPDATA%\\Programs\\pgAdmin 4\\v6\\runtime\" MacOS: export PATH=\"/usr/local/opt/libpq/bin:$PATH\"","title":"Prerequisites"},{"location":"historical/snaplet/#process","text":"Go to https://www.snaplet.dev/ , log into the account; set up the team and project. Use CONNECTION_STRING for the production database and establish a connection. In the 2nd step exclude schemas that you don't want to save; then follow through and make sure the process finishes without issues. Install Snaplet CLI with curl -sL https://app.snaplet.dev/get-cli/ | bash To use on Windows, you have two options: install WSL and an appropriate Linux distribution (for example, canonical Ubuntu 20.04.5) or use Bash which comes with any Windows Git distribution. In the terminal, navigate to your project's local directory and run bash snaplet config setup Enter the CONNECTION_STRING for the development database connection string when prompted (it also can be found in the Supabase Dev panel: Project Settings -> Database Settings -> Connections String -> URI ). This command creates a .snaplet directory and the configuration files used by the CLI Later, during making changes to the development database, you may want to switch between the development and production URL. The safest option to do that is through pre-configured Doppler secrets. Use SNAPLET_SOURCE_DATABASE_URL for creating the snapshot, and SNAPLET_DATABASE_URL for restoring one. We'll be using exactly this method below. {width=70%}","title":"Process"},{"location":"historical/snaplet/#using-snaplet","text":"","title":"Using Snaplet"},{"location":"historical/snaplet/#to-create-a-snapshot","text":"Documentation We assume, all requirements are met, including the SNAPLET_SOURCE_DATABASE_URL secret in Doppler. To access the secret from the production environment, you should use the Snaplet prod Service token. You generate the token once, and keep it in a password manager. More about Sevice Tokens see in a related article or a Doppler official documentation # Prevent command with Service Token being recorded in bash history export HISTIGNORE='doppler run*' doppler run --token='dp.st.prd.xxxx' -- snaplet snapshot capture Snapshots are automatically stored in $HOME/.snaplet/snapshots , but you can specify a path: doppler run --token='dp.st.prd.xxxx' -- snaplet snapshot capture /path/to/stored-snapshot","title":"To create a snapshot"},{"location":"historical/snaplet/#to-restore-a-snapshot","text":"You restore the snapshot with another simple command snaplet snapshot restore For snapshot restoring, you should write some permissions to a target database. Make sure you don't write into a production database! Even though, multiple measures were taken to prevent it: Secret SNAPLET_DATABASE_URL in a production Doppler configuration deliberately left empty. There's a zero chance we will use Snaplet to overwrite the Production database. If we restore the snapshot through the CI script tools/snapshot_tool.py , it prevents restoring the database if the URL looks like the Production # Prevent command with Service Token being recorded in bash history export HISTIGNORE='doppler run*' doppler run --token='dp.st.dev.xxxx' -- snaplet snapshot restore","title":"To restore a snapshot"},{"location":"historical/snaplet/#change-schema-after-restoring-a-snapshot","text":"After restoring the snapshot, it usually overwrites all tables except Database Schema. This prevents accessing the restored database through PostgREST or API with HTTP Error 401. In order to fix it, we must run the PSQL script tools/schema_permission.sql in a newly created database. This should fix the access rights and restore the schema.","title":"Change schema after restoring a snapshot"},{"location":"historical/snaplet/#other-commands","text":"To list existing snapshots snaplet snapshot list To list existing configs snaplet config list To save config locally snaplet config pull More CLI commands are here .","title":"Other commands"},{"location":"infra/install_postgres_server/","text":"VPS Server Setup Connect Ubuntu repo with Postgres Add the PostgreSQL repository to your Ubuntu server's sources list. Update version as appropriate echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" | sudo tee /etc/apt/sources.list.d/pgdg.list wget -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - sudo apt -y update && sudo apt -y upgrade sudo apt -y install postgresql-16 postgresql-contrib-16 postgresql-client-16 postgresql-common sudo systemctl status postgresql sudo systemctl start postgresql Give access to Postgres Open firewall port Enable the firewall, unless already done sudo ufw enable Open the Postgres port for incoming connections sudo ufw allow 5432/tcp sudo ufw allow 5432/udp Check the status of the firewall sudo ufw status Restore current snapshot of the database Using Snaplet create a snapshot of the Production database snaplet snapshot capture Restore snapshot on the 2nd server snaplet snapshot restore New1 This document is the collection of steps required to install Postgres server on Ubuntu machine. https://www.postgresql.org/download/linux/ubuntu/ sudo apt install postgresql sudo apt install -y postgresql-common sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh sudo sh -c 'echo \"deb [signed-by=/usr/share/postgresql-common/pgdg/apt.postgresql.org.pgp] https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.lis t.d/pgdg.list' * sudo install -d /usr/share/postgresql-common/pgdg echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" | sudo tee /etc/apt/sources.list.d/pgdg.list wget -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - sudo apt -y install postgresql-16 postgresql-contrib-16 postgresql-client-16 postgresql-common * sudo systemctl start postgresql pg_ctl -D /usr/local/pgsql/data initdb psql -U postgres pg_ctl","title":"Installing Postgres Server"},{"location":"infra/install_postgres_server/#vps-server-setup","text":"","title":"VPS Server Setup"},{"location":"infra/install_postgres_server/#connect-ubuntu-repo-with-postgres","text":"Add the PostgreSQL repository to your Ubuntu server's sources list. Update version as appropriate echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" | sudo tee /etc/apt/sources.list.d/pgdg.list wget -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - sudo apt -y update && sudo apt -y upgrade sudo apt -y install postgresql-16 postgresql-contrib-16 postgresql-client-16 postgresql-common sudo systemctl status postgresql sudo systemctl start postgresql","title":"Connect Ubuntu repo with Postgres"},{"location":"infra/install_postgres_server/#give-access-to-postgres","text":"","title":"Give access to Postgres"},{"location":"infra/install_postgres_server/#open-firewall-port","text":"Enable the firewall, unless already done sudo ufw enable Open the Postgres port for incoming connections sudo ufw allow 5432/tcp sudo ufw allow 5432/udp Check the status of the firewall sudo ufw status","title":"Open firewall port"},{"location":"infra/install_postgres_server/#restore-current-snapshot-of-the-database","text":"Using Snaplet create a snapshot of the Production database snaplet snapshot capture Restore snapshot on the 2nd server snaplet snapshot restore","title":"Restore current snapshot of the database"},{"location":"infra/install_postgres_server/#new1","text":"This document is the collection of steps required to install Postgres server on Ubuntu machine. https://www.postgresql.org/download/linux/ubuntu/ sudo apt install postgresql sudo apt install -y postgresql-common sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh sudo sh -c 'echo \"deb [signed-by=/usr/share/postgresql-common/pgdg/apt.postgresql.org.pgp] https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.lis t.d/pgdg.list' * sudo install -d /usr/share/postgresql-common/pgdg echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" | sudo tee /etc/apt/sources.list.d/pgdg.list wget -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - sudo apt -y install postgresql-16 postgresql-contrib-16 postgresql-client-16 postgresql-common * sudo systemctl start postgresql pg_ctl -D /usr/local/pgsql/data initdb psql -U postgres pg_ctl","title":"New1"},{"location":"infra/polylith_structure/","text":"This document is to describe the process of switching wapaganda_tools project to the Polylith structure. Useful links: https://davidvujic.blogspot.com/2022/08/a-simple-scalable-python-project.html https://davidvujic.github.io/python-polylith-docs/ https://github.com/DavidVujic/python-polylith https://github.com/DavidVujic/python-polylith-example https://github.com/ttamg/python-polylith-microservices-example https://davidvujic.github.io/python-polylith-docs/migrating/ https://python-poetry.org/docs/plugins/ https://github.com/subjective-agency/wapatools Steps taken These Poetry plugins need to be installed: poetry self add poetry-multiproject-plugin poetry self add poetry-polylith-plugin Create a new project with Poetry as dependency manager. Initialize git and Poetry git init poetry init Create a workspace, with a basic Polylith folder structure poetry poly create workspace --name wapaganda --theme loose This operation creates file workspace.toml with workspace settings. If section test is enabled, a folder with the same name would be created in the root mirroring the repo structure. Create a virtual environment powershell poetry install Create new project powershell poetry poly create project --name wapatools Create bases and components for each module in the old repository: powershell poetry poly create base --name database ... poetry poly create component --name transfactory ... The concept of bases / components / projects does not align perfectly with the structure by which the old repo was organized. For started, I decided to migrate contents of the scripts folder as bases and contents of the src folder as components , all assigned to the single project . However, it would probably make sense to split functionality down into different projects , which would require moving things around. The operation above boils down to creating the respective element's folder structure, as well as _init_ file and core.py file (which is a conventional (but not mandatory) way of code organization). Once it's in place, files could be copied over from the old repo. {width=253px} Organize pyproject.toml files. One of the nuances of this approach, is that pyproject file is required not only on the root level, covering the whole package, but also on the project level. This means, that every project folder must contain its own pyproject.toml with appropriate dependencies. Repo-level file should contain following elements: ``` [tool.poetry] packages = [ {include = \"wapaganda/api\", from = \"bases\"}, ... {include = \"wapaganda/umisc\", from = \"components\"}, ] [tool.poetry.dependencies] {copy over all the dependencies from the old repo} ``` project -level file should contain following elements: ``` [tool.poetry] packages = [ {include = \"wapaganda/core\", from = \"../../bases\"}, ... {include = \"wapaganda/upload\", from = \"../../components\"} ] {note the path in the 'from' element} {ideally, only part of the repo dependencies should be listed for a project} [tool.poetry.plugins.\"poetry.application.plugin\"] poetry-polylith-plugin = \"polylith.poetry_plugin:PolylithPlugin\" [tool.poetry.dependencies] {copy over all the dependencies relevant for the project} ``` __init__.py This file is required for each component and each base . By default, the structure of __init__.py looks like this: ``` from wapaganda.base_or_component_name import core all = [\"core\"] ``` This structure assumes the existence of file core.py and imports it as a whole. Alternatively, certain elements could be imported like so: ``` from wapaganda.database.connection import connect_supabase all = [\"connect_supabase\"] ``` It is not strictly necessary to put this in order for all the components right away. This whole thing won't work until you run the following commands: Since the dependencies were not introduced in the natural way: poetry lock This would write or update poetry.lock file. The same should be done to the project -level file: poetry lock --directory projects/wapatools poetry install This would install the dependencies, and it would install the internal module ( wapatools in this case), which will finally unlock the usage. Something like this should be the final output line: Installing the current project: wapatools (0.1.0) Using polylith plugin Once the above steps are complete, the plugin could be used like so: poetry poly info Returns workspace summary that looks like this: * poetry poly libs Returns summary of libraries in use that looks like this: {width=70%} * poetry poly diff Returns the difference between current state and latest stable point (not sure if this is git-based or what) * poetry poly check Checks the validity of the structure (I think) - basically, a boolean derivation of the poetry poly libs command. Final notes I tested imports of internal modules on the connection func, and it worked just fine. Other modules should have __init__ files properly structured. All in all the experience is surprisingly positive. Of course, I had some confusion, like with all things newly learned, but it was quite easy to figure things out after all. Some time was surely spent on this, but it was comparatively small. In the future the structure should be further refined: some scripts would probably have to be moved from bases to components and vice versa. It would definitely make sense to split this whole thing into multiple projects, not just one. I have a feeling that this structure suits this repo extremely well.","title":"Polylith Project Structure"},{"location":"infra/polylith_structure/#steps-taken","text":"These Poetry plugins need to be installed: poetry self add poetry-multiproject-plugin poetry self add poetry-polylith-plugin Create a new project with Poetry as dependency manager. Initialize git and Poetry git init poetry init Create a workspace, with a basic Polylith folder structure poetry poly create workspace --name wapaganda --theme loose This operation creates file workspace.toml with workspace settings. If section test is enabled, a folder with the same name would be created in the root mirroring the repo structure. Create a virtual environment powershell poetry install Create new project powershell poetry poly create project --name wapatools Create bases and components for each module in the old repository: powershell poetry poly create base --name database ... poetry poly create component --name transfactory ... The concept of bases / components / projects does not align perfectly with the structure by which the old repo was organized. For started, I decided to migrate contents of the scripts folder as bases and contents of the src folder as components , all assigned to the single project . However, it would probably make sense to split functionality down into different projects , which would require moving things around. The operation above boils down to creating the respective element's folder structure, as well as _init_ file and core.py file (which is a conventional (but not mandatory) way of code organization). Once it's in place, files could be copied over from the old repo. {width=253px} Organize pyproject.toml files. One of the nuances of this approach, is that pyproject file is required not only on the root level, covering the whole package, but also on the project level. This means, that every project folder must contain its own pyproject.toml with appropriate dependencies. Repo-level file should contain following elements: ``` [tool.poetry] packages = [ {include = \"wapaganda/api\", from = \"bases\"}, ... {include = \"wapaganda/umisc\", from = \"components\"}, ] [tool.poetry.dependencies] {copy over all the dependencies from the old repo} ``` project -level file should contain following elements: ``` [tool.poetry] packages = [ {include = \"wapaganda/core\", from = \"../../bases\"}, ... {include = \"wapaganda/upload\", from = \"../../components\"} ] {note the path in the 'from' element} {ideally, only part of the repo dependencies should be listed for a project} [tool.poetry.plugins.\"poetry.application.plugin\"] poetry-polylith-plugin = \"polylith.poetry_plugin:PolylithPlugin\" [tool.poetry.dependencies] {copy over all the dependencies relevant for the project} ``` __init__.py This file is required for each component and each base . By default, the structure of __init__.py looks like this: ``` from wapaganda.base_or_component_name import core all = [\"core\"] ``` This structure assumes the existence of file core.py and imports it as a whole. Alternatively, certain elements could be imported like so: ``` from wapaganda.database.connection import connect_supabase all = [\"connect_supabase\"] ``` It is not strictly necessary to put this in order for all the components right away. This whole thing won't work until you run the following commands: Since the dependencies were not introduced in the natural way: poetry lock This would write or update poetry.lock file. The same should be done to the project -level file: poetry lock --directory projects/wapatools poetry install This would install the dependencies, and it would install the internal module ( wapatools in this case), which will finally unlock the usage. Something like this should be the final output line: Installing the current project: wapatools (0.1.0)","title":"Steps taken"},{"location":"infra/polylith_structure/#using-polylith-plugin","text":"Once the above steps are complete, the plugin could be used like so: poetry poly info Returns workspace summary that looks like this: * poetry poly libs Returns summary of libraries in use that looks like this: {width=70%} * poetry poly diff Returns the difference between current state and latest stable point (not sure if this is git-based or what) * poetry poly check Checks the validity of the structure (I think) - basically, a boolean derivation of the poetry poly libs command.","title":"Using polylith plugin"},{"location":"infra/polylith_structure/#final-notes","text":"I tested imports of internal modules on the connection func, and it worked just fine. Other modules should have __init__ files properly structured. All in all the experience is surprisingly positive. Of course, I had some confusion, like with all things newly learned, but it was quite easy to figure things out after all. Some time was surely spent on this, but it was comparatively small. In the future the structure should be further refined: some scripts would probably have to be moved from bases to components and vice versa. It would definitely make sense to split this whole thing into multiple projects, not just one. I have a feeling that this structure suits this repo extremely well.","title":"Final notes"},{"location":"infra/private_docker_registry_auth/","text":"Private Docker Registry Auth Make sure htpasswd is installed: shell sudo apt-get update && sudo apt-get install apache2-utils Create htpasswd file and add the first user shell sudo mkdir -p /mnt/docker/registry/auth htpasswd -Bc /mnt/docker/registry/auth/htpasswd <username> When prompted, enter password Create config file at /mnt/docker/registry/config.yml with following content: yaml version: 0.1 log: fields: service: registry environment: development http: addr: :5000 headers: X-Content-Type-Options: [nosniff] auth: htpasswd: realm: basic-realm path: /auth/htpasswd When starting the registry's container, make sure to mount paths to config and to the auth directory. For example, when defining a system service: ```ini Description=Private Docker Registry for the Wapaganda Project After=docker.service Requires=docker.service [Service] Restart=always ExecStart=/usr/bin/docker run --rm --name wapa-registry -p 50000:5000 -v /mnt/docker/registry:/var/lib/registry -v /mnt/docker/registry/auth:/auth -v /mnt/docker/registry/config.yml:/etc/docker/registry/config.yml registry:latest ExecStop=/usr/bin/docker stop registry [Install] WantedBy=multi-user.target ``` Add new users htpasswd -B /mnt/docker/registry/auth/htpasswd <new_username> Authenticate docker login registry.subjective.agency List Repositories curl -u <username>:<password> -X GET https://registry.subjective.agency/v2/_catalog List tags curl -u <username>:<password> -X GET https://registry.subjective.agency/v2/wapaganda/tags/list","title":"Private Docker Registry Auth"},{"location":"infra/private_docker_registry_auth/#private-docker-registry-auth","text":"Make sure htpasswd is installed: shell sudo apt-get update && sudo apt-get install apache2-utils Create htpasswd file and add the first user shell sudo mkdir -p /mnt/docker/registry/auth htpasswd -Bc /mnt/docker/registry/auth/htpasswd <username> When prompted, enter password Create config file at /mnt/docker/registry/config.yml with following content: yaml version: 0.1 log: fields: service: registry environment: development http: addr: :5000 headers: X-Content-Type-Options: [nosniff] auth: htpasswd: realm: basic-realm path: /auth/htpasswd When starting the registry's container, make sure to mount paths to config and to the auth directory. For example, when defining a system service: ```ini Description=Private Docker Registry for the Wapaganda Project After=docker.service Requires=docker.service [Service] Restart=always ExecStart=/usr/bin/docker run --rm --name wapa-registry -p 50000:5000 -v /mnt/docker/registry:/var/lib/registry -v /mnt/docker/registry/auth:/auth -v /mnt/docker/registry/config.yml:/etc/docker/registry/config.yml registry:latest ExecStop=/usr/bin/docker stop registry [Install] WantedBy=multi-user.target ```","title":"Private Docker Registry Auth"},{"location":"infra/private_docker_registry_auth/#add-new-users","text":"htpasswd -B /mnt/docker/registry/auth/htpasswd <new_username>","title":"Add new users"},{"location":"infra/private_docker_registry_auth/#authenticate","text":"docker login registry.subjective.agency","title":"Authenticate"},{"location":"infra/private_docker_registry_auth/#list-repositories","text":"curl -u <username>:<password> -X GET https://registry.subjective.agency/v2/_catalog","title":"List Repositories"},{"location":"infra/private_docker_registry_auth/#list-tags","text":"curl -u <username>:<password> -X GET https://registry.subjective.agency/v2/wapaganda/tags/list","title":"List tags"},{"location":"infra/ssl_lets_encrypt/","text":"For info about database SSL see here . It's possible to get SSL certificate for free using Let's Encrypt CA. Limitations: Paid certificates offer enhanced encryption levels over free options such as Let\u2019s Encrypt and provide additional features such as extended validation (EV), wildcard support, and site seals to prove authenticity. Install certbot if not yet installed apt install letsencrypt Generate certificate for server.subjective.agency certbot certonly --standalone -d server.subjective.agency Check the generated certs certbot certificates Saving debug log to /var/log/letsencrypt/letsencrypt.log - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Found the following certs: Certificate Name: server.subjective.agency Serial Number: 494d1e4555cb861b59d48512b2b81394bbc Key Type: RSA Domains: server.subjective.agency Expiry Date: 2024-04-12 19:35:08+00:00 (VALID: 87 days) Certificate Path: /etc/letsencrypt/live/server.subjective.agency/fullchain.pem Private Key Path: /etc/letsencrypt/live/server.subjective.agency/privkey.pem You can copy / add your certificates anywhere they're needed. But we have to automate it, right? Let's encrypt infrastructure comes with renewal hooks allowing certs and keys be deployed anywhere. Setup renewal hook vim /etc/letsencrypt/renewal-hooks/deploy/minio # Copy or symlink the renewed certificates to Minio accessible directory cp /etc/letsencrypt/live/server.subjective.agency/fullchain.pem ~minio-user/.minio/certs/public.crt cp /etc/letsencrypt/live/server.subjective.agency/privkey.pem ~minio-user/.minio/certs/private.key chown -R minio-user:minio-user ~minio-user/.minio/certs chmod 400 ~minio-user/.minio/certs/private.key And now after renewal minio will get the new certificates automatically. Check the certs are in-place! cd /etc/letsencrypt/renewal-hooks/deploy/minio ls -al ~minio-user/.minio/certs total 36 drwxrwxr-x 3 minio-user minio-user 4096 Jan 15 22:15 . drwxrwxr-x 4 minio-user minio-user 4096 Oct 15 21:40 .. drwx------ 2 minio-user minio-user 4096 Oct 12 09:58 CAs -r-------- 1 minio-user minio-user 1704 Jan 15 22:15 private.key -rw-r--r-- 1 minio-user minio-user 5538 Jan 15 22:15 public.crt Automate with cron We cannot keep in mind all the certs dates, so we force-update our cert every 2 months let's say on day 15: crontab -e # m h dom mon dow command 0 0 15 */2 * certbot renew --force-renew","title":"SSL with Let's Encrypt!"},{"location":"infra/ssl_lets_encrypt/#install-certbot-if-not-yet-installed","text":"apt install letsencrypt","title":"Install certbot if not yet installed"},{"location":"infra/ssl_lets_encrypt/#generate-certificate-for-serversubjectiveagency","text":"certbot certonly --standalone -d server.subjective.agency","title":"Generate certificate for server.subjective.agency"},{"location":"infra/ssl_lets_encrypt/#check-the-generated-certs","text":"certbot certificates Saving debug log to /var/log/letsencrypt/letsencrypt.log - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Found the following certs: Certificate Name: server.subjective.agency Serial Number: 494d1e4555cb861b59d48512b2b81394bbc Key Type: RSA Domains: server.subjective.agency Expiry Date: 2024-04-12 19:35:08+00:00 (VALID: 87 days) Certificate Path: /etc/letsencrypt/live/server.subjective.agency/fullchain.pem Private Key Path: /etc/letsencrypt/live/server.subjective.agency/privkey.pem You can copy / add your certificates anywhere they're needed. But we have to automate it, right? Let's encrypt infrastructure comes with renewal hooks allowing certs and keys be deployed anywhere.","title":"Check the generated certs"},{"location":"infra/ssl_lets_encrypt/#setup-renewal-hook","text":"vim /etc/letsencrypt/renewal-hooks/deploy/minio # Copy or symlink the renewed certificates to Minio accessible directory cp /etc/letsencrypt/live/server.subjective.agency/fullchain.pem ~minio-user/.minio/certs/public.crt cp /etc/letsencrypt/live/server.subjective.agency/privkey.pem ~minio-user/.minio/certs/private.key chown -R minio-user:minio-user ~minio-user/.minio/certs chmod 400 ~minio-user/.minio/certs/private.key And now after renewal minio will get the new certificates automatically.","title":"Setup renewal hook"},{"location":"infra/ssl_lets_encrypt/#check-the-certs-are-in-place","text":"cd /etc/letsencrypt/renewal-hooks/deploy/minio ls -al ~minio-user/.minio/certs total 36 drwxrwxr-x 3 minio-user minio-user 4096 Jan 15 22:15 . drwxrwxr-x 4 minio-user minio-user 4096 Oct 15 21:40 .. drwx------ 2 minio-user minio-user 4096 Oct 12 09:58 CAs -r-------- 1 minio-user minio-user 1704 Jan 15 22:15 private.key -rw-r--r-- 1 minio-user minio-user 5538 Jan 15 22:15 public.crt","title":"Check the certs are in-place!"},{"location":"infra/ssl_lets_encrypt/#automate-with-cron","text":"We cannot keep in mind all the certs dates, so we force-update our cert every 2 months let's say on day 15: crontab -e # m h dom mon dow command 0 0 15 */2 * certbot renew --force-renew","title":"Automate with cron"},{"location":"misc/begging_text/","text":"Wapaganda is a non-commercial project, dedicated to collecting information on Russian and pro-Russian propagandists of war. Essentially, it's a catalog of the personalities involved in the state-funded propaganda machine, the most efficient and deceptive since the times of Joseph Goebbels. The goal of the project is to collect all information about them, and make it freely available, with the following purposes in mind: Serving justice . These people must not escape just evaluation of their deeds. They are guilty of multiple crimes against humanity, from propaganda of hate, to calls for total genocide. Dehumanizing treatment of civilians on occupied territories, mass murder, torture, rape, looting are the direct consequences of the stream of hate, produced by Russian state-supported media. Enabling further research. Willing researchers must be given necessary information on the subject, including raw data and ways to analyze it, so that it would be possible not only to better understand it, but also to come up with methods of efficient resistance to propaganda and provide adequate answers to the burning questions. Revealing the true scale of lies produced by Russia, and the role this effort played in the hostilities. We plan to offer open data about the scale of involvement of specific individuals, organizations, media outlets, etc.; forms and patterns of propaganda, as well as objective measures of involvement, such as air time, and many more. While propaganda is a murky thing to study, we believe it's a very important task, as its impact on society goes somewhat underappreciated, especially against the background of the news about the war. However, given the amount of harm it causes, its importance seems to be even higher, as the war and all the war crimes are direct consequences of propaganda. We are a non-commercial project, with no kind of organized funding (governmental or otherwise). With this fundraising, we are looking to cover the cost of maintaining the web application we're building, and to remove impediments from the process of further development. The maintenance cost is not very high (under $100 per month), but the cost of qualified labor has become a rather serious expense recently, as we believe that wages must not be skimmed. Also: many people in Ukraine lost their jobs as a direct consequence of the war, and while we can't help everyone, but we can employ at least a few. In the future, we may want to cover expenses for such things as SEO or design, although it's not currently on the roadmap. We aren't impartial. But we know how biases work, and we are working hard to remain objective no matter what. Victory will be Ukraine's; justice and truth will prevail. Thank you for reading this far.","title":"Begging text"},{"location":"misc/slides_revealjs/","text":"I'm using reveal.js for creating presentations. Follow these steps to spin up a presentation at https://slides.subjective.agency : Create the deck. It could be created either via code, or using the UI at https://slides.com , which is a commercial solution on top of the technology, with the free plan allowing to create up to 5 decks. A created deck can be exported in HTML format. Add the deck to the forked repository at https://github.com/subjective-agency/reveal.js - either into decks directory, or as an index.html file. If replacing index.html file, DO NOT REMOVE the one that already exists, instead rename it into something like _index_.html . A cron is set up on the principal server that checks for updates every minute and rebuilds the docker image if any are detected If it doesn't, manual steps are: Log into the principal 's shell, and CD to /home/warp/reveal . This directory contains tart_reveal.sh script. Run sudo bash start_reveal.sh This would rebuild the docker image, and then spin up the container at port 50988, which is reverse-proxied to slides subdomain. Additional deck placed into decks directory with name apresent.html can be accessed at https://slides.subjective.agency/decks/apresent.html","title":"Slides With Reveal.JS"},{"location":"rfc/1/","text":"Abstract TBD Context With over 1,000 propagandists on file (and counting), it is natural to consider a scale along which they all could be placed in accordance with the measure of their respective influence. However, in order to have such scale, there must be a reliable way to measure said influence. This is a challenging task, to put it mildly. Sources of influence For now, I distinguish 3 major sources of influence: airtime (appearances on TV and in streams / videos), personal influence (which refers, for the most part, to the social media following) and published works (books & articles). They are different in essence and require different approaches, but at the end of the day, the former two result in accumulation or loss of p-points per day. The 3rd also produces p-points, but over longer stretches of time. With airtime , the amount of daily p-points depends on the following factors: Place. Is it a TV channel? If so, is it federal or local? Is it a Youtube channel? What was the audience of that channel at the time of streaming? What was the audience of that particular stream? Role. Is it a host? Is it a guest? Are we talking self-hosted video? etc. Time. For how long was this person present in the shot? [what about when it's only the voice? do we measure exact time (per shot; possible with machine learning; takes a hell of a long time), or is it better to use approximations based on internal patterns of each media segment?] Perhaps, influence already accumulated as of the air time, can be considered a factor, as well. With personal influence p-points appear from the published posts, with following things playing a part: [per media] number of posts, number of views/reads for each post; [per media] average daily views / average post views; number of social media maintained; [cumulative] TBD With published works TBD Gaps & Issues The main issue with this whole thing is that quite a lot of information is unavailable and/or unobtainable, which makes any possible calculations contingent . That is unavoidable, but it doesn't mean that they can't be consistent, too: in order to achieve consistency, a baseline personality should be selected to serve as an exemplary case, against which everybody else would be measured. Further issues: should this be a single person who feeds from all known sources of influence (not too rare; but rarely a person would be equally successful all around), or should there be separate measures for each source (which would effectively frankenstein a perfect influencer)? Another issue is that airtime and personal influence are quite complex, although differently. With airtime , time measure (seconds) and quantitative measure (viewers) must be equated and normalized, and various coefficients must be accounted for. With personal influence there are multiple social media, each with its own architecture, meaning each would require a dedicated connector.","title":"RFC-1. P-Rating"},{"location":"rfc/1/#abstract","text":"TBD","title":"Abstract"},{"location":"rfc/1/#context","text":"With over 1,000 propagandists on file (and counting), it is natural to consider a scale along which they all could be placed in accordance with the measure of their respective influence. However, in order to have such scale, there must be a reliable way to measure said influence. This is a challenging task, to put it mildly.","title":"Context"},{"location":"rfc/1/#sources-of-influence","text":"For now, I distinguish 3 major sources of influence: airtime (appearances on TV and in streams / videos), personal influence (which refers, for the most part, to the social media following) and published works (books & articles). They are different in essence and require different approaches, but at the end of the day, the former two result in accumulation or loss of p-points per day. The 3rd also produces p-points, but over longer stretches of time. With airtime , the amount of daily p-points depends on the following factors: Place. Is it a TV channel? If so, is it federal or local? Is it a Youtube channel? What was the audience of that channel at the time of streaming? What was the audience of that particular stream? Role. Is it a host? Is it a guest? Are we talking self-hosted video? etc. Time. For how long was this person present in the shot? [what about when it's only the voice? do we measure exact time (per shot; possible with machine learning; takes a hell of a long time), or is it better to use approximations based on internal patterns of each media segment?] Perhaps, influence already accumulated as of the air time, can be considered a factor, as well. With personal influence p-points appear from the published posts, with following things playing a part: [per media] number of posts, number of views/reads for each post; [per media] average daily views / average post views; number of social media maintained; [cumulative] TBD With published works TBD","title":"Sources of influence"},{"location":"rfc/1/#gaps-issues","text":"The main issue with this whole thing is that quite a lot of information is unavailable and/or unobtainable, which makes any possible calculations contingent . That is unavoidable, but it doesn't mean that they can't be consistent, too: in order to achieve consistency, a baseline personality should be selected to serve as an exemplary case, against which everybody else would be measured. Further issues: should this be a single person who feeds from all known sources of influence (not too rare; but rarely a person would be equally successful all around), or should there be separate measures for each source (which would effectively frankenstein a perfect influencer)? Another issue is that airtime and personal influence are quite complex, although differently. With airtime , time measure (seconds) and quantitative measure (viewers) must be equated and normalized, and various coefficients must be accounted for. With personal influence there are multiple social media, each with its own architecture, meaning each would require a dedicated connector.","title":"Gaps &amp; Issues"},{"location":"rfc/2/","text":"In the context of the project, transcript is a textual representation of a media episode. Media episodes are different in terms of length and structure, which means that storing their transcripts (essentially, large amounts of text) becomes a bit of a challenge. There are three general usecases here: Segment's internal structure is simple and does not change; and/or it starts and finishes with the same cast. Segment's internal structure is complex and/or involves multiple cast that can unpredictably change over the course of an episode. It is impossible to fully separate specific speakers. Segment's internal structure is relatively complex but predictable and separable. In cases 1 and 2, it would make sense to store the full episode transcript in the _vids table. Case 3 is different. These episodes consist (for the most part) of sequential chunks and usually follow one of several specific patterns (they are not 100% reliable, though). Generally, there are 2 main types of chunks that can be pinned down as monologs and dialogs for lack of better terminology. The most important thing, however, is that they can be separated - this offers a benefit of more targeted text selection down the line (i.e. reduction in processing overhead). This requires, on the one hand, a more intricate and time-consuming mark-up process, and on the other, a more complex storage structure. Specifically, instead of having transcript field in the _vids table, it would be a part of the people_on_ table, where each guest record would have a corresponding dialog , and the host record would contain all the monologs . ?? what if > 1 host ?? ?? Should the order be preserved ?? ?? if so - how ?? Type 1 Type 2 Type 3 besogon_vids 60_minutes_vids solovyovlive_vids daytv_vids kto_protiv_vids komsomolskayapravda_vids komsomolskayapravda_vids solovyovvecher_vids metametrica_vids vesti_vids mkp_vids typychny_vyshinsky_vids youtube_vids","title":"RFC-2. Transcripts"},{"location":"rfc/3/","text":"Welcome! Thank you for your curiosity. Origins Wapaganda the Project was conceived in the wake of Russo-Ukrainian war around the end of February 2022. It took some time for it to acquire shape (until the end of summer 2022, give or take), and it wasn't until January 2023 that it was deemed ready enough for public scrutiny. There is a good reason for such a lengthy timeframe: our team is tiny (2 people currently, just 1 in the beginning), and our resources were and still are quite limited. //\ufeffBy the way, if you want to support us (and we would appreciate it very much!), there are several ways how you can do that. Check out this page for more info.// Essence So, what is Wapaganda, exactly? On the surface you will find here a catalog of personalities, each of whom is guilty of actively supporting Vladimir Putin's criminal regime by propagating false narratives and tropes, and with goals of easing the negative effects of the war on the Russian population and pumping up support for the regime. Why we believe that propaganda is such an important machinery for this process is explained over here. Underneath the flimsy list of obscure individuals, there is an immense amount of data and hundreds of tiny relationships connecting all the separate pieces into a vast and fascinating system - complex, but not unfathomable. We are working to accumulate all possible data on these people, including their written works (actual books and articles, as well as blog posts, and social media footprint), their appearance in the media (including transcripts of what was said - currently, only a share of data is processed, and is available only in Russian - English translation is planned), anything else that's available out there in the open web, and even a little bit of what the dark web has to offer. Simultaneously, we're mapping all the little pieces together, sort of like a multidimensional jigsaw puzzle. This accumulation of data is being done with several things in mind: First and foremost, the data would serve as evidence for when these people are tried for their activities. The data would serve as a basis for working out the influence rating (P-rating) , which could be used to assign accountability with better precision. The data should become a valuable source for any researcher out there. We're following the CVARC model, ensuring data's Completeness, Verifiability, Accessibility, Reliability and Consistency. (TODO) Finally, the data may serve as a starting spark in the process of establishing international monitoring with the purpose of preventing yet another intricate propagandistic machine from developing. Criteria One important question to answer is how exactly people end up in the Wapaganda database. You might wanna check out the Copium Theory of Propaganda to understand the approach, but in plain language, each and every person on this list has willingly, and oftentimes eagerly, participated in the activities aimed at spreading the narratives of the Putin's creed (TODO). There are other projects out there maintaining lists of all kinds of Ukraine's enemies (Myrotvorets, OSINTBees, ..) - we, on the other hand, are specifically focused on the propagandists. You will find here soldiers and collaborants, too, but only if, on top of their military or administrative function, they also propagated the Putin's lies via mass media. Each and every person is handpicked, and there is always a reason for their inclusion. That being said, mistakes are possible, and if you encounter an incorrect piece of information, please do let us know (TODO). \ufeff\ufeffThe bulk of our database is day-to-day propaganda functionaries, such as the cast of the Solovyov Live, 60 minutes, Vesti etc. talk shows, as well as the numerous guests featured there. Only appearances after February 24, 2022 are taken into account, the assumption being that the start of the war was a sobering experience for some, so a person is considered complicit only if they have continued (or started) \ufeffshowing up. Other notable subgroups include high-level figures whom many people know, such as Margarita Simonyan and Dmitry Kiselyov; authors of books and articles directed against Ukraine and/or justifying Russian decrepit ways; semi-independent influencers; foreign friends (agents of influence outside of Russia); and others. Finally, a swarm of pro-Russian influencers, inhabiting YouTube, TikTok, Telegram and other media platforms, constitutes a significant share of monitoring targets. Data What kind of data is being absorbed, what can you find here? First off, there's data that's already available out there, in other sources. In Wapaganda it's organized a little better, and generally more accessible. Then, there is data that comes from the ongoing process of observation. Mostly, these are media appearances, quotes, and full transcripts of what was said. We monitor and scrape media platforms, including YouTube, Smotrim.ru, Vk.com. There is Telegram channels statistics, as well as a backup of their history (730+ channels, close to 10 million messages in total). Not all of these riches are currently fully accessible. If you want to speed up the process a bit, you can do it here (TODO).","title":"RFC-3. About Page"},{"location":"rfc/3/#origins","text":"Wapaganda the Project was conceived in the wake of Russo-Ukrainian war around the end of February 2022. It took some time for it to acquire shape (until the end of summer 2022, give or take), and it wasn't until January 2023 that it was deemed ready enough for public scrutiny. There is a good reason for such a lengthy timeframe: our team is tiny (2 people currently, just 1 in the beginning), and our resources were and still are quite limited. //\ufeffBy the way, if you want to support us (and we would appreciate it very much!), there are several ways how you can do that. Check out this page for more info.//","title":"Origins"},{"location":"rfc/3/#essence","text":"So, what is Wapaganda, exactly? On the surface you will find here a catalog of personalities, each of whom is guilty of actively supporting Vladimir Putin's criminal regime by propagating false narratives and tropes, and with goals of easing the negative effects of the war on the Russian population and pumping up support for the regime. Why we believe that propaganda is such an important machinery for this process is explained over here. Underneath the flimsy list of obscure individuals, there is an immense amount of data and hundreds of tiny relationships connecting all the separate pieces into a vast and fascinating system - complex, but not unfathomable. We are working to accumulate all possible data on these people, including their written works (actual books and articles, as well as blog posts, and social media footprint), their appearance in the media (including transcripts of what was said - currently, only a share of data is processed, and is available only in Russian - English translation is planned), anything else that's available out there in the open web, and even a little bit of what the dark web has to offer. Simultaneously, we're mapping all the little pieces together, sort of like a multidimensional jigsaw puzzle. This accumulation of data is being done with several things in mind: First and foremost, the data would serve as evidence for when these people are tried for their activities. The data would serve as a basis for working out the influence rating (P-rating) , which could be used to assign accountability with better precision. The data should become a valuable source for any researcher out there. We're following the CVARC model, ensuring data's Completeness, Verifiability, Accessibility, Reliability and Consistency. (TODO) Finally, the data may serve as a starting spark in the process of establishing international monitoring with the purpose of preventing yet another intricate propagandistic machine from developing.","title":"Essence"},{"location":"rfc/3/#criteria","text":"One important question to answer is how exactly people end up in the Wapaganda database. You might wanna check out the Copium Theory of Propaganda to understand the approach, but in plain language, each and every person on this list has willingly, and oftentimes eagerly, participated in the activities aimed at spreading the narratives of the Putin's creed (TODO). There are other projects out there maintaining lists of all kinds of Ukraine's enemies (Myrotvorets, OSINTBees, ..) - we, on the other hand, are specifically focused on the propagandists. You will find here soldiers and collaborants, too, but only if, on top of their military or administrative function, they also propagated the Putin's lies via mass media. Each and every person is handpicked, and there is always a reason for their inclusion. That being said, mistakes are possible, and if you encounter an incorrect piece of information, please do let us know (TODO). \ufeff\ufeffThe bulk of our database is day-to-day propaganda functionaries, such as the cast of the Solovyov Live, 60 minutes, Vesti etc. talk shows, as well as the numerous guests featured there. Only appearances after February 24, 2022 are taken into account, the assumption being that the start of the war was a sobering experience for some, so a person is considered complicit only if they have continued (or started) \ufeffshowing up. Other notable subgroups include high-level figures whom many people know, such as Margarita Simonyan and Dmitry Kiselyov; authors of books and articles directed against Ukraine and/or justifying Russian decrepit ways; semi-independent influencers; foreign friends (agents of influence outside of Russia); and others. Finally, a swarm of pro-Russian influencers, inhabiting YouTube, TikTok, Telegram and other media platforms, constitutes a significant share of monitoring targets.","title":"Criteria"},{"location":"rfc/3/#data","text":"What kind of data is being absorbed, what can you find here? First off, there's data that's already available out there, in other sources. In Wapaganda it's organized a little better, and generally more accessible. Then, there is data that comes from the ongoing process of observation. Mostly, these are media appearances, quotes, and full transcripts of what was said. We monitor and scrape media platforms, including YouTube, Smotrim.ru, Vk.com. There is Telegram channels statistics, as well as a backup of their history (730+ channels, close to 10 million messages in total). Not all of these riches are currently fully accessible. If you want to speed up the process a bit, you can do it here (TODO).","title":"Data"},{"location":"rfc/4/","text":"This article describes relationships among media segments, YouTube channels and subset of database tables suffixed with _vids or _episodes . A media segment is a sequence of related media episodes. Media episode is a distinct piece of media content willingly published on the Internet by its owners with the purpose of public distribution. Note: majority of media episodes are also distributed via other means, i.e. television, radio, printed media. Note: there may be media that are distributed via those other means but aren't available in the web. Media episode can be of different media type (podcast, talk show, news piece, etc.) \\~ this is not currently taken into account. Media segments can be explicit (named talk shows, in particular), authorial (a particular expert on a particular outlet, for example, \"Aleksandr Artamonov on DayTV\"), or synthetic (for example, \"Nikolay Platoshkin on YouTube\"). Note the difference between authorial and synthetic. Both are named after the main author of the segment, but where authorial is tied to a specific media outlet (DayTV in this example) and could move to a different platform with it, the synthetic type refers to free-to-use platforms like YouTube, where an author may have multiple channels and/or can be banned by the platform but has an option of registering a new account. For example, there are 3 YouTube channels that publish Artamonov-related stuff, and there are 2 known channels for Radio Aurora, one of which is banned. Media segments also differ by the primary source, which could be youtube.com, smotrim.ru or vk.com. This is mixed up a little, since segments could be present on several platforms simultaneously. The solution to this ambiguity is to simply select one or another. vk.com is not used anymore because other 2 platforms give us better structured data, but it has a benefit of storing hi-resolution videos of those segments that choose to publish there. Some media segments have their own tables in the db, while others are clustered together and use a common table. The current set up is somewhat inconsistent, for example, it may make sense to merge metametrica_vids and komsomolskayapravda_vids into youtube_vids , or merge all the smotrim -based tables together, but doing that won't improve things by that much yet it would take quite a bit of effort. Still, an optimization plan might be in order for implementation in the future.","title":"RFC-4. Media Segments, YouTube Channels & Tables"},{"location":"rfc/5/","text":"Context The database schema has evolved quite significantly since it was conceived, and while its evolution was guided by performance considerations, certain initial decisions have led to suboptimal landscape and need to be re-evaluated now. Decision A number of tables in the database contain various media episodes, and the current setup is ambiguous in the sense that there is no clear reasoning for why certain media segments would have their own dedicated tables, while others would be sharing. To remove this ambiguity, a squashing operation should be implemented. Specifically, tables 60_minutes_vids , kto_protiv_vids , mkp_vids , solovyovlive_vids (partially), solovyovvecher_vids , typychny_vyshinsky_vids , vesti_vids , vestifm_episodes , radio_episodes are to be merged into a single smotrim_episodes table; besogon_vids , daytv_vids , komsomolskayapravda_vids , metametrica_vids are to be merged into youtube_vids . This would require modifications to the schema: both [new] tables are to be based off of the current templates, except to indicate the distinction that is upheld by different tables right now. from to 60_minutes_vids smotrim_episodes kto_protiv_vids smotrim_episodes mkp_vids smotrim_episodes solovyovlive_vids smotrim_episodes people_on_solovyovlive people_on_smotrim solovyovvecher_vids smotrim_episodes ~~people_on_solovyovvecher~~ people_on_smotrim typychny_vyshinsky_vids smotrim_episodes vesti_vids smotrim_episodes vestifm_episodes smotrim_episodes radio_episodes smotrim_episodes besogon_vids youtube_vids daytv_vids youtube_vids people_on_daytv people_on_youtube komsomolskayapravda_vids youtube_vids people_on_morning_mardan people_on_youtube metametrica_vids youtube_vids people_on_metametrica people_on_youtube In addition: ~~youtube_authors~~ ~~should be renamed~~ ~~people_on_youtube~~ ~~; column to indicate role_id should be added to it.~~ decide what to do with youtube_authors table MongoDB objects in the transcripts collection should be updated with new table name / id Plan of action The 1st part of the operation, the copying, should be completed over the course of 1 day, and the dropping should be done on the next day - this is so that we could have a backup of the complete DB, just in case. As an additional precaution measure, 2 index files, one per platform, should be saved with mapping old_table + old_id \u2192 new_id . [x] Create table smotrim_episodes [x] Copy contents of the smotrim tables to smotrim_episodes (except solovyolive_vids) [x] Create table people_on_smotrim [x] Copy contents of the solovyovlive_vids [x] Copy contents of the people_on_solovyovlive also updating indices [x] Create people_on_youtube table [x] Copy contents of the youtube tables to youtube_vids also updating indices [x] Update all scripts dealing with the deprecated tables to deal with the new set-up. [x] Drop smotrim tables [x] Drop youtube tables Status In progress. Consequences This operation has a high potential for breaking things, so all the suggested changes must be listed first, with as much foresight into the specifics as possible, and should be applied after an internal discussion.","title":"RFC-5. Pre-Launch Database Optimization"},{"location":"rfc/5/#context","text":"The database schema has evolved quite significantly since it was conceived, and while its evolution was guided by performance considerations, certain initial decisions have led to suboptimal landscape and need to be re-evaluated now.","title":"Context"},{"location":"rfc/5/#decision","text":"A number of tables in the database contain various media episodes, and the current setup is ambiguous in the sense that there is no clear reasoning for why certain media segments would have their own dedicated tables, while others would be sharing. To remove this ambiguity, a squashing operation should be implemented. Specifically, tables 60_minutes_vids , kto_protiv_vids , mkp_vids , solovyovlive_vids (partially), solovyovvecher_vids , typychny_vyshinsky_vids , vesti_vids , vestifm_episodes , radio_episodes are to be merged into a single smotrim_episodes table; besogon_vids , daytv_vids , komsomolskayapravda_vids , metametrica_vids are to be merged into youtube_vids . This would require modifications to the schema: both [new] tables are to be based off of the current templates, except to indicate the distinction that is upheld by different tables right now. from to 60_minutes_vids smotrim_episodes kto_protiv_vids smotrim_episodes mkp_vids smotrim_episodes solovyovlive_vids smotrim_episodes people_on_solovyovlive people_on_smotrim solovyovvecher_vids smotrim_episodes ~~people_on_solovyovvecher~~ people_on_smotrim typychny_vyshinsky_vids smotrim_episodes vesti_vids smotrim_episodes vestifm_episodes smotrim_episodes radio_episodes smotrim_episodes besogon_vids youtube_vids daytv_vids youtube_vids people_on_daytv people_on_youtube komsomolskayapravda_vids youtube_vids people_on_morning_mardan people_on_youtube metametrica_vids youtube_vids people_on_metametrica people_on_youtube In addition: ~~youtube_authors~~ ~~should be renamed~~ ~~people_on_youtube~~ ~~; column to indicate role_id should be added to it.~~ decide what to do with youtube_authors table MongoDB objects in the transcripts collection should be updated with new table name / id","title":"Decision"},{"location":"rfc/5/#plan-of-action","text":"The 1st part of the operation, the copying, should be completed over the course of 1 day, and the dropping should be done on the next day - this is so that we could have a backup of the complete DB, just in case. As an additional precaution measure, 2 index files, one per platform, should be saved with mapping old_table + old_id \u2192 new_id . [x] Create table smotrim_episodes [x] Copy contents of the smotrim tables to smotrim_episodes (except solovyolive_vids) [x] Create table people_on_smotrim [x] Copy contents of the solovyovlive_vids [x] Copy contents of the people_on_solovyovlive also updating indices [x] Create people_on_youtube table [x] Copy contents of the youtube tables to youtube_vids also updating indices [x] Update all scripts dealing with the deprecated tables to deal with the new set-up. [x] Drop smotrim tables [x] Drop youtube tables","title":"Plan of action"},{"location":"rfc/5/#status","text":"In progress.","title":"Status"},{"location":"rfc/5/#consequences","text":"This operation has a high potential for breaking things, so all the suggested changes must be listed first, with as much foresight into the specifics as possible, and should be applied after an internal discussion.","title":"Consequences"},{"location":"rfc/7/","text":"Context Currently, DVE-A-56, DVE-A-29 and DVE-A-30 (all unfinished) describe the approach to media segment - media episode relationship. Due to the complexity of the real-life picture, though, this description ignores certain important nuances. This document offers a detalization on the media episode entity, and describes the problem of attributing a media episode to a media segment . Media Episode Structure Every media episode consits of a number of scenes (at least 1). A scene has 2 boolean attributes: irl indicates if the scene happened offline ( True ) or via some virtual means of communication; quicky indicates if the scene is supplementary (and short; example: Golovanov's Time within Polny contact and Mardan segments). Furthermore, a scene can be 1 of the following types/subtypes ( host2guest ): Type Subtype Example many2many circle wank \u0412\u0435\u0447\u0435\u0440 \u0441 \u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440\u043e\u043c \u0421\u043e\u043b\u043e\u0432\u044c\u0435\u0432\u044b\u043c, \u041a\u0442\u043e \u041f\u0440\u043e\u0442\u0438\u0432 many2many group one2zero wank-off Any lecture, any scene with just the host one2one boning one2many two2zero mutual wank \u041c\u0438\u0445\u0435\u0435\u0432 \u0413\u043e\u0432\u043e\u0440\u0438\u0442 two2one threeway Attribution The attribution problem arises from the recently introduced synthetic segment type, which is tied to a special organization type personal brand (id 35). Synthetic segment is meant to serve as a container that holds media episodes produced by sources with low consistency & recognition rate (mostly, YouTube channels with random selection of content) but that can be tied to an influencer patient, such as Andrey Fursov or Aleksandr Artamonov. In effect, this means, for example, that the numerous YouTube videos in which Andrey Fursov took part, (almost) regardless of which YouTube channel they were published on, should be identified as episodes of the Andrey Fursov (WYT) segment, which belongs to Andrey Fursov personal brand organization. The Andrey Fursov personal brand organization is also tied to Andrey Fursov (DayTV) segment, which is a collection of Fursov's appearances on specifically DayTV. DayTV is a media outlet whose content can be obtained from several sources, including YouTube, which makes this whole thing a bit more confusing. The other major type of media segment is named segment - this is BesogonTV, Metametrica, Empatiya Manuchi, Polny Kontact, etc. Confusion intensifies when you consider Andrey Fursov's appearances on such named segments. For example, he gave several interviews to Metametrica - do these episodes belong to Metametrica segment, or Andrey Fursov segment? Furthermore, Metametrica is organized and hosted by 2 people (Danyuk and Egorchenkov), both of whom are influencers in their own right - so, maybe these episodes should be attributed to their respective personal brand -related segments? One solution could be to allow an episode to have multiple segment-relationships, i.e. Fursov's Metametrica interviews would be tied to Metametrica segment, as well as personal brand -related segments of all the participants. This could get out of hand, if there many.","title":"RFC-7. Media Episodes Attribution Problem"},{"location":"rfc/7/#context","text":"Currently, DVE-A-56, DVE-A-29 and DVE-A-30 (all unfinished) describe the approach to media segment - media episode relationship. Due to the complexity of the real-life picture, though, this description ignores certain important nuances. This document offers a detalization on the media episode entity, and describes the problem of attributing a media episode to a media segment .","title":"Context"},{"location":"rfc/7/#media-episode-structure","text":"Every media episode consits of a number of scenes (at least 1). A scene has 2 boolean attributes: irl indicates if the scene happened offline ( True ) or via some virtual means of communication; quicky indicates if the scene is supplementary (and short; example: Golovanov's Time within Polny contact and Mardan segments). Furthermore, a scene can be 1 of the following types/subtypes ( host2guest ): Type Subtype Example many2many circle wank \u0412\u0435\u0447\u0435\u0440 \u0441 \u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440\u043e\u043c \u0421\u043e\u043b\u043e\u0432\u044c\u0435\u0432\u044b\u043c, \u041a\u0442\u043e \u041f\u0440\u043e\u0442\u0438\u0432 many2many group one2zero wank-off Any lecture, any scene with just the host one2one boning one2many two2zero mutual wank \u041c\u0438\u0445\u0435\u0435\u0432 \u0413\u043e\u0432\u043e\u0440\u0438\u0442 two2one threeway","title":"Media Episode Structure"},{"location":"rfc/7/#attribution","text":"The attribution problem arises from the recently introduced synthetic segment type, which is tied to a special organization type personal brand (id 35). Synthetic segment is meant to serve as a container that holds media episodes produced by sources with low consistency & recognition rate (mostly, YouTube channels with random selection of content) but that can be tied to an influencer patient, such as Andrey Fursov or Aleksandr Artamonov. In effect, this means, for example, that the numerous YouTube videos in which Andrey Fursov took part, (almost) regardless of which YouTube channel they were published on, should be identified as episodes of the Andrey Fursov (WYT) segment, which belongs to Andrey Fursov personal brand organization. The Andrey Fursov personal brand organization is also tied to Andrey Fursov (DayTV) segment, which is a collection of Fursov's appearances on specifically DayTV. DayTV is a media outlet whose content can be obtained from several sources, including YouTube, which makes this whole thing a bit more confusing. The other major type of media segment is named segment - this is BesogonTV, Metametrica, Empatiya Manuchi, Polny Kontact, etc. Confusion intensifies when you consider Andrey Fursov's appearances on such named segments. For example, he gave several interviews to Metametrica - do these episodes belong to Metametrica segment, or Andrey Fursov segment? Furthermore, Metametrica is organized and hosted by 2 people (Danyuk and Egorchenkov), both of whom are influencers in their own right - so, maybe these episodes should be attributed to their respective personal brand -related segments? One solution could be to allow an episode to have multiple segment-relationships, i.e. Fursov's Metametrica interviews would be tied to Metametrica segment, as well as personal brand -related segments of all the participants. This could get out of hand, if there many.","title":"Attribution"},{"location":"working/media/social_blade/","text":"SocialBlade is a platform collecting statistics on various media platforms, including Youtube. SocialBlade data would compliment the youtube stats data we have currently. SocialBlade data is NOT free, so related flow should be designed to draw maximum value from what we get. Attached is a standard result you'd get if you spend 1 credit on a channel. Historical data is available for additional creds. Relevant sections Section Datatype Notes Where info -> access -> expires_at Datetime Service item that should be used to manage how a channel data would be updated further data -> id JSON[TEXT] Data item that should be parsed and saved once youtube_channels data -> general -> created_at Datetime Data item that should be parsed and saved once. Only needed for unresolved channels youtube_channels data -> general -> geo JSON[TEXT] Data item that should be parsed and saved once youtube_channels data -> general -> branding -> social JSON[TEXT] Should be re-checked on every request. youtube_channels ~~statistics -> total~~ JSON[TEXT/INT] Contains 3 keys - uploads, subscribers and views with values current as of the moment of request. Values for subs and views are the same as the most recent daily values. Value for current number of uploaded vids can be procured from Youtube API for free. nowhere statistics -> growth JSON[TEXT/INT] It's not exactly clear what these numbers signify. Growth over the first year of existence? What's the deal with the negative values? TBD ranks JSON[TEXT/INT] Data is relevant as of the moment of request. Not included in daily daily JSON[TEXT/INT] With 1 credit we get number of subs and views over previous 30 days youtube_stats Reconciliation required for stats data currently stored in youtube_vids . It gets fetched via Youtube API, and contains numbers for views, comments and likes on specific videos (as of the moment of request). Stats data stored in youtube_channels should be the most current, and should be updated from youtube_stats table data. Approximate Flow Fetch a channel from the database. Get max amount of historical data on this channel. Parse and save the data to the database. We would get: expires_at date; general information that we only need once; related social network accounts; growth data; ranks data relevant as of the moment of request; complete set of daily data up to the date of request. If a channel is defunct, this is the end of it. If a channel is not defunct: Set new value for expires_at date (TBD) Fetch updates on this channel until expires_at date comes. With every response we would get: related social network accounts (possible update); growth data; (possible update?) (to be saved?) ranks data relevant as of the moment of request; (to be saved) daily data for previous 30 days (items not in db to be saved) Put channel on pause for 30 days. At this point we have: most current list of related social accounts; ranks data for the recent 30 days; growth data for the recent 30 days; entire set of daily data up until current date. After 30 days, spend 1 credit on this channel to fetch the standart amount of data. We would get: related social network accounts (possible update); growth data; (possible update?) (to be saved?) ranks data relevant as of the moment of request; (to be saved) daily data for previous 30 days (to be saved). Put channel on pause for 30 days. At this point we have: most current list of related social accounts; ranks data for 60 days in total: 30 days of the 1st iteration, and 30 more of the 2nd iteration; growth data for 60 days; (...) entire set of daily data up until current date. Rinse and repeat. Observations We would have consistent and steady set of daily data with no gaps. We would have inconsistent set of ranks data - 30 days stretches with 30 days gaps in between. Value of growth data is dubious at the moment. To do Find a place for expires_at timestamp; Create a table for storing daily data. Possibly, store ranks (and growth ?) data in the same table; Decide what to do with videos-level stats; Set up a trigger to update youtube_channels table with most current views, subs etc. UPD1 After reviewing the video https://www.youtube.com/watch?v=LihUTkSMRFw found by @saszko, it became clear to me that the actual flow should be as follows: Get a channel from database; Consider channel's creation date. If creation_date is less than 700 days ago, do nothing. If creation_date is more than 700 days ago, spend 3 credits to retrieve max amount of historical data from SocialBlade. Put the channel up for update via the compare flow (TBD) if channel is defunct - one time; if channel is not defunct - continuously on schedule; The compare flow would be based on the info from the aforementioned video, and would involve using playwright for access to https://socialblade.com/youtube/compare page (it returns 503 to requests ). Example compare URL is https://socialblade.com/youtube/compare/UCBMXxadAHZl9FOGhMeR_ptg/UCKgdkhHgBQrwWY_PVGxMH9Q, where pieces after compare/ are identifiers of Youtube channels. With this tool, you can retrieve up to 3 channels at a time. Retrieval algorythm is approximately as follows: Get 3 target channels from the database accounting for 1) if the channel is defunct; 2) when was it last updated. Shape compare URL. Use playwright to visit the URL and retrieve page's source code Parse the source code to retrieve 5 relevant raw strings corresponding to 5 sections returned by the compare module. Parse raw strings into proper data structures Persist only new items to the database. UPD2 With the compare flow, only 2 sections (raw strings) are relevant: document.getElementById('subscribersYTDYGraph') document.getElementById('videoviewsYTDYGraph') Of the other 4, dailysubscribersYTDYGraph and dailyvideoviewsYTDYGraph represent a diff between days, which we can calculate ourselves; and futuresubsYTDYGraph and futureviewsYTDYGraph are projections based on unknown algorythm, which makes them rather useless.","title":"Social Blade Addition"},{"location":"working/media/social_blade/#relevant-sections","text":"Section Datatype Notes Where info -> access -> expires_at Datetime Service item that should be used to manage how a channel data would be updated further data -> id JSON[TEXT] Data item that should be parsed and saved once youtube_channels data -> general -> created_at Datetime Data item that should be parsed and saved once. Only needed for unresolved channels youtube_channels data -> general -> geo JSON[TEXT] Data item that should be parsed and saved once youtube_channels data -> general -> branding -> social JSON[TEXT] Should be re-checked on every request. youtube_channels ~~statistics -> total~~ JSON[TEXT/INT] Contains 3 keys - uploads, subscribers and views with values current as of the moment of request. Values for subs and views are the same as the most recent daily values. Value for current number of uploaded vids can be procured from Youtube API for free. nowhere statistics -> growth JSON[TEXT/INT] It's not exactly clear what these numbers signify. Growth over the first year of existence? What's the deal with the negative values? TBD ranks JSON[TEXT/INT] Data is relevant as of the moment of request. Not included in daily daily JSON[TEXT/INT] With 1 credit we get number of subs and views over previous 30 days youtube_stats Reconciliation required for stats data currently stored in youtube_vids . It gets fetched via Youtube API, and contains numbers for views, comments and likes on specific videos (as of the moment of request). Stats data stored in youtube_channels should be the most current, and should be updated from youtube_stats table data.","title":"Relevant sections"},{"location":"working/media/social_blade/#approximate-flow","text":"Fetch a channel from the database. Get max amount of historical data on this channel. Parse and save the data to the database. We would get: expires_at date; general information that we only need once; related social network accounts; growth data; ranks data relevant as of the moment of request; complete set of daily data up to the date of request. If a channel is defunct, this is the end of it. If a channel is not defunct: Set new value for expires_at date (TBD) Fetch updates on this channel until expires_at date comes. With every response we would get: related social network accounts (possible update); growth data; (possible update?) (to be saved?) ranks data relevant as of the moment of request; (to be saved) daily data for previous 30 days (items not in db to be saved) Put channel on pause for 30 days. At this point we have: most current list of related social accounts; ranks data for the recent 30 days; growth data for the recent 30 days; entire set of daily data up until current date. After 30 days, spend 1 credit on this channel to fetch the standart amount of data. We would get: related social network accounts (possible update); growth data; (possible update?) (to be saved?) ranks data relevant as of the moment of request; (to be saved) daily data for previous 30 days (to be saved). Put channel on pause for 30 days. At this point we have: most current list of related social accounts; ranks data for 60 days in total: 30 days of the 1st iteration, and 30 more of the 2nd iteration; growth data for 60 days; (...) entire set of daily data up until current date. Rinse and repeat.","title":"Approximate Flow"},{"location":"working/media/social_blade/#observations","text":"We would have consistent and steady set of daily data with no gaps. We would have inconsistent set of ranks data - 30 days stretches with 30 days gaps in between. Value of growth data is dubious at the moment.","title":"Observations"},{"location":"working/media/social_blade/#to-do","text":"Find a place for expires_at timestamp; Create a table for storing daily data. Possibly, store ranks (and growth ?) data in the same table; Decide what to do with videos-level stats; Set up a trigger to update youtube_channels table with most current views, subs etc.","title":"To do"},{"location":"working/media/social_blade/#upd1","text":"After reviewing the video https://www.youtube.com/watch?v=LihUTkSMRFw found by @saszko, it became clear to me that the actual flow should be as follows: Get a channel from database; Consider channel's creation date. If creation_date is less than 700 days ago, do nothing. If creation_date is more than 700 days ago, spend 3 credits to retrieve max amount of historical data from SocialBlade. Put the channel up for update via the compare flow (TBD) if channel is defunct - one time; if channel is not defunct - continuously on schedule; The compare flow would be based on the info from the aforementioned video, and would involve using playwright for access to https://socialblade.com/youtube/compare page (it returns 503 to requests ). Example compare URL is https://socialblade.com/youtube/compare/UCBMXxadAHZl9FOGhMeR_ptg/UCKgdkhHgBQrwWY_PVGxMH9Q, where pieces after compare/ are identifiers of Youtube channels. With this tool, you can retrieve up to 3 channels at a time. Retrieval algorythm is approximately as follows: Get 3 target channels from the database accounting for 1) if the channel is defunct; 2) when was it last updated. Shape compare URL. Use playwright to visit the URL and retrieve page's source code Parse the source code to retrieve 5 relevant raw strings corresponding to 5 sections returned by the compare module. Parse raw strings into proper data structures Persist only new items to the database.","title":"UPD1"},{"location":"working/media/social_blade/#upd2","text":"With the compare flow, only 2 sections (raw strings) are relevant: document.getElementById('subscribersYTDYGraph') document.getElementById('videoviewsYTDYGraph') Of the other 4, dailysubscribersYTDYGraph and dailyvideoviewsYTDYGraph represent a diff between days, which we can calculate ourselves; and futuresubsYTDYGraph and futureviewsYTDYGraph are projections based on unknown algorythm, which makes them rather useless.","title":"UPD2"},{"location":"working/onto/classes/","text":"This document is supposed to serve as a basic informational resource on things related to the ontology-building effort. What is an ontology and why do we need one? In the context of computer science and information science, an ontology is a formal representation of knowledge that defines the concepts, entities, relationships, and properties within a particular domain. It provides a structured and organized way to describe and model a specific domain by defining the classes, attributes, and interrelationships of the entities or concepts within that domain. In the case of Wapaganda, the domain in question is the propagandistic activity of the current Russian state, and top-level classes are: Organization Person Group MediaSegment Attributes and relationships for organization , person and group should be adopted from FOAF, where all of these entities have the same parent - agent . Basic terms and attributes should be adopted from DC and, possibly, other existing vocabularies On the other hand, MediaSegments do not seem to have any kind of vocabulary developed, so this would be the primary direction of the ontology effort. The resulting ontology must serve as the basis for classes and attributes created in the Python code. With library owlready2 it is possible to link an ontology and a python module so that they compliment each other. Resourses FOAF (Friend-of-a-Friend) http://xmlns.com/foaf/0.1/ DC (Dublin Core) https://www.dublincore.org/specifications/dublin-core/dcmi-terms/ SIOC (Semantically Interlinked Online Communities) http://rdfs.org/sioc/spec/ https://schema.org/docs/schemas.html https://lov.linkeddata.es/dataset/lov/vocabs https://www.w3.org/TR/powder-primer/ https://www.ietf.org/rfc/rfc3987.txt https://www.w3.org/TR/rdf11-primer/ https://www.w3.org/TR/2004/REC-owl-guide-20040210/ https://www.w3.org/TR/2004/REC-owl-semantics-20040210/ https://www.w3.org/2002/03/tutorials.html https://protegeproject.github.io/protege/ https://www.w3.org/2001/sw/SW-FAQ","title":"Ontologies, Classes & Semantic Web"},{"location":"working/onto/classes/#what-is-an-ontology-and-why-do-we-need-one","text":"In the context of computer science and information science, an ontology is a formal representation of knowledge that defines the concepts, entities, relationships, and properties within a particular domain. It provides a structured and organized way to describe and model a specific domain by defining the classes, attributes, and interrelationships of the entities or concepts within that domain. In the case of Wapaganda, the domain in question is the propagandistic activity of the current Russian state, and top-level classes are: Organization Person Group MediaSegment Attributes and relationships for organization , person and group should be adopted from FOAF, where all of these entities have the same parent - agent . Basic terms and attributes should be adopted from DC and, possibly, other existing vocabularies On the other hand, MediaSegments do not seem to have any kind of vocabulary developed, so this would be the primary direction of the ontology effort. The resulting ontology must serve as the basis for classes and attributes created in the Python code. With library owlready2 it is possible to link an ontology and a python module so that they compliment each other.","title":"What is an ontology and why do we need one?"},{"location":"working/onto/classes/#resourses","text":"FOAF (Friend-of-a-Friend) http://xmlns.com/foaf/0.1/ DC (Dublin Core) https://www.dublincore.org/specifications/dublin-core/dcmi-terms/ SIOC (Semantically Interlinked Online Communities) http://rdfs.org/sioc/spec/ https://schema.org/docs/schemas.html https://lov.linkeddata.es/dataset/lov/vocabs https://www.w3.org/TR/powder-primer/ https://www.ietf.org/rfc/rfc3987.txt https://www.w3.org/TR/rdf11-primer/ https://www.w3.org/TR/2004/REC-owl-guide-20040210/ https://www.w3.org/TR/2004/REC-owl-semantics-20040210/ https://www.w3.org/2002/03/tutorials.html https://protegeproject.github.io/protege/ https://www.w3.org/2001/sw/SW-FAQ","title":"Resourses"},{"location":"working/onto/model_requirements/","text":"The entities and relationships for the Wapaganda Semantic Model (WSM) must be extracted from the data produced by the persons and organizations engaged in the informational support of the Russo-Ukrainian war on the Russian side (aka Russian propaganda). To make sure these entities are generic enough to be applicable to other similar situations, they must be fitted against at least 1 well-documented historical case of comparable nature, for example the history of propaganda in the Nazi Germany. Individuals of the model are people, organizations and various media resources engaged in the propagandistic activity. Correspondingly, the final arrangement must consist of the following parts: light-weight OWL model describing general entities and relationships, such as people and organizations, which should be applicable to any similar situation regardless of geography; light-weight SKOS model describing the system of axioms and ideas that constitute the current state-driven public consensus in Russia; application that takes both these models and adds a data layer to describe the state and history of the Russian informational space in the context of the military conflict in Ukraine. The model should be able to provide a way for answering questions that involve Level of engagement of any particular entity in the propagandistic activities; Amount of value any particular entity brings into the propaganda system; Relative and absolute impact any particular entity has on the public opinions; The specific subset of axioms/ideas any particular entity supports / spreads; Relative amount of hate speech in any particular entity's public statements; etc.","title":"Ontologies. Model Requirements"},{"location":"working/onto/ontology_flow_overview/","text":"On the high level, the flow is approximately as this: Over the day, some new data gets added to the DB. Once a day, on schedule, the reasoner is run on the updated DB. This step produces additional data that should be saved to the DB. Data such generated is displayed in the interface in some distinct way. Final HTML code contains additional elements/tags/attributes reflecting ontology-related stuff. Here's a non-specific example provided by ChatGPT: ``` Ontology HTML Page About Me John Doe Occupation: Software Engineer Birthdate: October 15, 1985 Address: 123 Main Street , Cityville , State , 12345 , United States Email: john.doe@example.com ``` Reasoners Some possible options for reasoner: HermiT: HermiT is a highly optimized OWL reasoner developed by the University of Oxford. It provides efficient reasoning services for OWL ontologies and supports various OWL profiles. Pellet: Pellet is an OWL reasoner developed by Clark & Parsia. It is known for its scalability and performance, making it suitable for large-scale ontologies. Pellet supports OWL 2 and provides reasoning support for various OWL profiles. FaCT++: FaCT++ is a highly optimized OWL reasoner developed by the University of Manchester. It is based on the FaCT system and provides reasoning services for OWL 2 ontologies. FaCT++ is known for its efficiency and scalability. RDFox: RDFox is a highly scalable semantic reasoning engine developed by Oxford Semantic Technologies. It supports OWL 2 RL and OWL 2 QL profiles and is designed for efficient reasoning over large-scale RDF datasets.","title":"Ontology Flow Overview"},{"location":"working/onto/ontology_flow_overview/#reasoners","text":"Some possible options for reasoner: HermiT: HermiT is a highly optimized OWL reasoner developed by the University of Oxford. It provides efficient reasoning services for OWL ontologies and supports various OWL profiles. Pellet: Pellet is an OWL reasoner developed by Clark & Parsia. It is known for its scalability and performance, making it suitable for large-scale ontologies. Pellet supports OWL 2 and provides reasoning support for various OWL profiles. FaCT++: FaCT++ is a highly optimized OWL reasoner developed by the University of Manchester. It is based on the FaCT system and provides reasoning services for OWL 2 ontologies. FaCT++ is known for its efficiency and scalability. RDFox: RDFox is a highly scalable semantic reasoning engine developed by Oxford Semantic Technologies. It supports OWL 2 RL and OWL 2 QL profiles and is designed for efficient reasoning over large-scale RDF datasets.","title":"Reasoners"},{"location":"working/onto/rucr_intro/","text":"Ruskymir Creed (RUCR) What you can see here is a model of concepts and ideas commonly re-used by the Russian propaganda on the daily basis. It is currently comprised of {number_of_concepts} concepts (or tropes, as we call them) touching on all kinds of topics, from abortions to moon landing, but mainly concerned with diminishing and lying about Ukraine and its people. These are the ideas you'd hear if you were to turn on any today's Russian media outlet, apart from the exiled political opposition and those that focus on entertainment exclusively - although, this last category is far from innocent, as more and more singers, poets and producers turn to suck from the perennial tit of the state. You will notice that some of them would explicitly contradict others - this is normal, because it reflects the actual state of affairs, where people with, sometimes, diametrically opposing views unite in the blind adoration of Vladimir Putin's activities, jokes and speeches. If you're wondering, how exactly is this even possible, - this entire project is dedicated to finding the answer to this question. We believe that RUCR is a step in the direction of it. How it works RUCR is based on the Simple Knowledge Organization System (SKOS), which is a standard developed by World Wide Web Consortium (also known as W3C) in >>>> mainly for representing existing systems of knowledge (such as library catalogs) in digital format. What's important to know, is that it defines 2 kinds of relations: the broader - narrower kind is hierarchical and says that a certain idea is broader or narrower in relation to another certain idea; and related kind says that two ideas are connected in a non-hierarchical way. For example, the idea that homosexuality is a sin is narrower to the idea that homosexuality is wrong ; and that latter one is related to the idea that liberal values were made up as a tool for disrupting nations from within . Arrows on the diagram represent broader / narrower relationships, with the arrowhead pointing to the broader term. Representing related relations is under development. Two large circles contain the 15 core ideas: they are considered such because themselves they cannot be broken down any further (well, kind of - see the WIP note below). One of the circles contains anti-Ukrainian collection, the other - everything else. This layout is quite arbitrary, that is - dictated by aesthetics mostly; however the defined relations are solid. WIP RUCR is not a finished work: some concepts may not have been identified yet, others are yet to arise - this terrain is complex and fluid, and capturing and analyzing it takes time and work. If you feel like you can help us move things forward, send us a letter. \u041a\u0440\u0435\u0434\u043e \u0440\u0443\u0441\u0441\u043a\u043e\u0433\u043e \u043c\u0438\u0440\u0430 (\u0420\u0423\u041a\u0420) \u0422\u043e, \u0447\u0442\u043e \u0432\u044b \u043d\u0430\u0439\u0434\u0435\u0442\u0435 \u0437\u0434\u0435\u0441\u044c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0441\u043e\u0431\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u044c \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u0439 \u0438 \u0438\u0434\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u043e\u0439 \u043f\u0440\u043e\u043f\u0430\u0433\u0430\u043d\u0434\u043e\u0439. \u041d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u043e\u043d\u0430 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 {number_of_concepts} \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u0439 (\u0438\u043b\u0438, \u043a\u0430\u043a \u043c\u044b \u0438\u0445 \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u043c, \u0442\u0440\u043e\u043f\u043e\u0432), \u0437\u0430\u0442\u0440\u0430\u0433\u0438\u0432\u0430\u044e\u0449\u0438\u0445 \u0441\u0430\u043c\u044b\u0435 \u0440\u0430\u0437\u043d\u044b\u0435 \u0442\u0435\u043c\u044b, \u043e\u0442 \u0430\u0431\u043e\u0440\u0442\u043e\u0432 \u0434\u043e \u043f\u043e\u0441\u0430\u0434\u043a\u0438 \u043d\u0430 \u041b\u0443\u043d\u0443, \u043d\u043e \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u043d\u0430 \u043f\u0440\u0438\u043d\u0438\u0436\u0435\u043d\u0438\u0435 \u0423\u043a\u0440\u0430\u0438\u043d\u044b, \u0438 \u043b\u043e\u0436\u044c \u043e \u0435\u0435 \u043d\u0430\u0440\u043e\u0434\u0435. \u042d\u0442\u043e \u0442\u0435 \u0438\u0434\u0435\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u044b \u0443\u0441\u043b\u044b\u0448\u0438\u0442\u0435, \u0435\u0441\u043b\u0438 \u0432\u043a\u043b\u044e\u0447\u0438\u0442\u0435 \u043b\u044e\u0431\u043e\u0435 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0435 \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u043e\u0435 \u0421\u041c\u0418, \u0437\u0430 \u0438\u0441\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435\u043c \u0440\u0430\u0437\u0432\u0435 \u0447\u0442\u043e \u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043e\u043f\u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u0432 \u0438\u0437\u0433\u043d\u0430\u043d\u0438\u0438, \u0438 \u0442\u0435\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u0438\u0441\u043a\u043b\u044e\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043d\u0430 \u0440\u0430\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f - \u0432\u043f\u0440\u043e\u0447\u0435\u043c, \u044d\u0442\u0443 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u044e\u044e \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044e \u0442\u043e\u0436\u0435 \u0441\u043b\u043e\u0436\u043d\u043e \u043d\u0430\u0437\u0432\u0430\u0442\u044c \u0447\u0438\u0441\u0442\u043e\u0439, \u0432\u0435\u0434\u044c \u0432\u0441\u0435 \u0431\u043e\u043b\u044c\u0448\u0435 \u0438 \u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u0435\u0432\u0446\u043e\u0432, \u043f\u043e\u044d\u0442\u043e\u0432, \u043f\u0440\u043e\u0434\u044e\u0441\u0435\u0440\u043e\u0432 \u0438 \u0442.\u043f. \u0441\u0442\u0430\u0440\u0430\u044e\u0442\u0441\u044f \u043f\u0440\u0438\u043d\u0438\u043a\u043d\u0443\u0442\u044c \u043a \u043d\u0435\u043f\u0435\u0440\u0435\u0441\u044b\u0445\u0430\u044e\u0449\u0435\u0439 \u0433\u043e\u0441\u0443\u0434\u0430\u0440\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439 \u0433\u0440\u0443\u0434\u0438. \u0412\u044b \u0437\u0430\u043c\u0435\u0442\u0438\u0442\u0435, \u0447\u0442\u043e \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u0437 \u043d\u0438\u0445 \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0440\u0435\u0447\u0430\u0442 \u0434\u0440\u0443\u0433\u0438\u043c - \u044d\u0442\u043e \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e, \u043f\u043e\u0441\u043e\u043b\u044c\u043a\u0443 \u043e\u0442\u0440\u0430\u0436\u0430\u0435\u0442 \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u0435 \u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0432\u0435\u0449\u0435\u0439, \u0433\u0434\u0435 \u043b\u044e\u0434\u0438 \u0441 \u043f\u043e\u0440\u043e\u0439 \u0434\u0438\u0430\u043c\u0435\u0442\u0440\u0430\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u043f\u043e\u043b\u043e\u0436\u043d\u044b\u043c\u0438 \u0432\u0437\u0433\u043b\u044f\u0434\u0430\u043c\u0438 \u0441\u043b\u0438\u0432\u0430\u044e\u0442\u0441\u044f \u0432 \u0441\u043b\u0435\u043f\u043e\u043c \u043e\u0431\u043e\u0436\u0430\u043d\u0438\u0438 \u0440\u0435\u0448\u0435\u043d\u0438\u0439, \u0448\u0443\u0442\u043e\u043a \u0438 \u0446\u0438\u0442\u0430\u0442 \u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440\u0430 \u041f\u0443\u0442\u0438\u043d\u0430. \u0415\u0441\u043b\u0438 \u0432\u044b \u0437\u0430\u0434\u0430\u0435\u0442\u0435\u0441\u044c \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u043c, \u043a\u0430\u043a \u044d\u0442\u043e \u0432\u043e\u043e\u0431\u0449\u0435 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e, - \u0447\u0442\u043e \u0436, \u043d\u0430\u0448 \u043f\u0440\u043e\u0435\u043a\u0442 \u043f\u043e\u0441\u0432\u044f\u0449\u0435\u043d \u043f\u043e\u0438\u0441\u043a\u0443 \u043e\u0442\u0432\u0435\u0442\u0430 \u043d\u0430 \u044d\u0442\u043e\u0442 \u0432\u043e\u043f\u0440\u043e\u0441 \u0446\u0435\u043b\u0438\u043a\u043e\u043c \u0438 \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e. \u041c\u044b \u0441\u0447\u0438\u0442\u0430\u0435\u043c, \u0447\u0442\u043e \u0420\u0423\u041a\u0420 - \u044d\u0442\u043e \u0448\u0430\u0433 \u0432 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u043c \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0438. \u041a\u0430\u043a \u044d\u0442\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0420\u0423\u041a\u0420 \u043e\u0441\u043d\u043e\u0432\u0430\u043d \u043d\u0430 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0435 \u043f\u043e\u0434 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435\u043c Simple Knowledge Organization System (SKOS), \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u044b\u043b \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u043d World Wide Web Consortium (\u0442\u0430\u043a\u0436\u0435 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u043c \u043a\u0430\u043a W3C) \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u0434\u043b\u044f \u0440\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0443\u0436\u0435 \u0441\u043b\u043e\u0436\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u0441\u0438\u0441\u0442\u0435\u043c \u0437\u043d\u0430\u043d\u0438\u0439 (\u0442\u0430\u043a\u0438\u0445 \u043a\u0430\u043a \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u0447\u043d\u044b\u0435 \u043a\u0430\u0442\u0430\u043b\u043e\u0433\u0438) \u0432 \u0446\u0438\u0444\u0440\u043e\u0432\u043e\u043c \u0444\u043e\u0440\u043c\u0430\u0442\u0435. \u0417\u0434\u0435\u0441\u044c \u0432\u0430\u0436\u043d\u043e \u0437\u043d\u0430\u0442\u044c, \u0447\u0442\u043e SKOS \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442 2 \u0442\u0438\u043f\u0430 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439: broader (\u0431\u043e\u043b\u0435\u0435 \u0448\u0438\u0440\u043e\u043a\u0438\u0439) - narrower (\u0431\u043e\u043b\u0435\u0435 \u0443\u0437\u043a\u0438\u0439) - \u044d\u0442\u043e \u0438\u0435\u0440\u0430\u0440\u0445\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0442\u0438\u043f, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442, \u0447\u0442\u043e \u043d\u0435\u043a\u0430\u044f \u0438\u0434\u0435\u044f \u0431\u043e\u043b\u0435\u0435 \u043e\u0431\u0449\u0430\u044f \u0438\u043b\u0438 \u0431\u043e\u043b\u0435\u0435 \u0441\u043f\u0435\u0446\u0438\u0444\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u043e \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044e \u043a \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0434\u0440\u0443\u0433\u043e\u0439 \u0438\u0434\u0435\u0435; \u0438 related \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442 \u043d\u0435-\u0438\u0435\u0440\u0430\u0440\u0445\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u0441\u0432\u044f\u0437\u044c \u043c\u0435\u0436\u0434\u0443 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0430\u043c\u0438. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0438\u0434\u0435\u044f, \u0447\u0442\u043e \u0433\u043e\u043c\u043e\u0441\u0435\u043a\u0441\u0443\u0430\u043b\u0438\u0437\u043c - \u044d\u0442\u043e \u0433\u0440\u0435\u0445 , \u0443\u0436\u0435 \u043f\u043e \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044e \u043a \u0438\u0434\u0435\u0435, \u0447\u0442\u043e \u0433\u043e\u043c\u043e\u0441\u0435\u043a\u0441\u0443\u0430\u043b\u0438\u0437\u043c - \u044d\u0442\u043e \u043f\u043b\u043e\u0445\u043e , \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0432 \u0441\u0432\u043e\u044e \u043e\u0447\u0435\u0440\u0435\u0434\u044c \u0441\u0432\u044f\u0437\u0430\u043d\u0430 \u0441 \u0438\u0434\u0435\u0435\u0439, \u0447\u0442\u043e \u043b\u0438\u0431\u0435\u0440\u0430\u043b\u044c\u043d\u044b\u0435 \u0446\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0431\u044b\u043b\u0438 \u0438\u0437\u043e\u0431\u0440\u0435\u0442\u0435\u043d\u044b \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043b\u044f \u0440\u0430\u0437\u0440\u0443\u0448\u0435\u043d\u0438\u044f \u043d\u0430\u0446\u0438\u0439 \u0438\u0437\u043d\u0443\u0442\u0440\u0438 . \u0421\u0432\u044f\u0437\u0438 \u043d\u0430 \u0434\u0438\u0430\u0433\u0440\u0430\u043c\u043c\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f \u0448\u0438\u0440\u0435 / \u0443\u0436\u0435 , \u043f\u0440\u0438\u0447\u0435\u043c \u0441\u0442\u0440\u0435\u043b\u043a\u0430 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043d\u0430 \u0431\u043e\u043b\u0435\u0435 \u0448\u0438\u0440\u043e\u043a\u0438\u0439 \u0442\u0435\u0440\u043c\u0438\u043d. \u0420\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0446\u0438\u044f related \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439 \u043f\u043e\u043a\u0430 \u0447\u0442\u043e \u0432 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0435. \u0414\u0432\u0430 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u043a\u0440\u0443\u0433\u0430 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 15 \u043a\u043e\u0440\u043d\u0435\u0432\u044b\u0445 \u0438\u0434\u0435\u0439: \u043c\u044b \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0438\u0445 \u0442\u0430\u043a\u043e\u0432\u044b\u043c\u0438, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0432 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 \u043d\u0438\u0445 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0430\u044f \u0434\u0435\u0442\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0437\u0430\u0442\u0440\u0443\u0434\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u0430 (\u043e\u0434\u043d\u0430\u043a\u043e \u0441\u043c. \u043f\u0440\u0438\u043c\u0435\u0447\u0430\u043d\u0438\u0435 \u043d\u0438\u0436\u0435). \u041e\u0434\u0438\u043d \u0438\u0437 \u043d\u0438\u0445 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u043a\u043e\u043b\u043b\u0435\u043a\u0446\u0438\u044e \u0430\u043d\u0442\u0438\u0443\u043a\u0440\u0430\u0438\u043d\u0441\u043a\u0438\u0445 \u0438\u0434\u0435\u0439, \u0434\u0440\u0443\u0433\u043e\u0439 - \u0432\u0441\u0435 \u043f\u0440\u043e\u0447\u0435\u0435. \u0422\u0430\u043a\u0430\u044f \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u0432\u043f\u043e\u043b\u043d\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u043b\u044c\u043d\u0430, \u0442\u043e \u0435\u0441\u0442\u044c, \u043f\u0440\u043e\u0434\u0438\u043a\u0442\u043e\u0432\u0430\u043d\u043e \u043f\u0440\u0435\u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u044d\u0441\u0442\u0435\u0442\u0438\u043a\u043e\u0439; \u043e\u0434\u043d\u0430\u043a\u043e \u0442\u0435 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u044b \u043c\u0435\u0436\u0434\u0443 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0430\u043c\u0438, - \u0432 \u043f\u043e\u0440\u044f\u0434\u043a\u0435. \u0420\u0430\u0431\u043e\u0442\u0430 \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u0435\u0442\u0441\u044f \u0420\u0423\u041a\u0420 \u043d\u0435 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u044b\u043c \u043f\u0440\u043e\u0435\u043a\u0442\u043e\u043c: \u043a\u0430\u043a\u0438\u0435-\u0442\u043e \u0438\u0434\u0435\u0438 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u0435\u0449\u0435 \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u044b, \u0434\u0440\u0443\u0433\u0438\u043c \u0435\u0449\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u043e\u0438\u0442 \u043f\u0440\u043e\u044f\u0432\u0438\u0442\u044c\u0441\u044f - \u044d\u0442\u043e \u0442\u0435\u0440\u0440\u0438\u0442\u043e\u0440\u0438\u044f \u0441\u043b\u043e\u0436\u043d\u0430\u044f \u0438 \u0438\u0437\u043c\u0435\u043d\u0447\u0438\u0432\u0430\u044f, \u0441\u0431\u043e\u0440 \u0438 \u0430\u043d\u0430\u043b\u0438\u0437 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u0442\u0440\u0435\u0431\u0443\u044e\u0442 \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0438 \u0442\u0440\u0443\u0434\u0430. \u0415\u0441\u043b\u0438 \u0432\u044b \u0434\u0443\u043c\u0430\u0435\u0442\u0435, \u0447\u0442\u043e \u043c\u043e\u0436\u0435\u0442\u0435 \u043f\u043e\u043c\u043e\u0447\u044c \u043d\u0430\u043c \u043f\u0440\u043e\u0434\u0432\u0438\u043d\u0443\u0442\u044c\u0441\u044f \u0434\u0430\u043b\u044c\u0448\u0435, \u043d\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u043d\u0430\u043c \u043f\u0438\u0441\u044c\u043c\u043e. \u041a\u0440\u0435\u0434\u043e \u0440\u0443\u0441\u044c\u043a\u043e\u0433\u043e \u043c\u0438\u0440\u0443 (\u0420\u0423\u041a\u0420) \u0422\u0435, \u0449\u043e \u0432\u0438 \u0442\u0443\u0442 \u0431\u0430\u0447\u0438\u0442\u0435, \u0454 \u043c\u043e\u0434\u0435\u043b\u043b\u044e \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0456\u0439 \u0442\u0430 \u0456\u0434\u0435\u0439, \u044f\u043a\u0456 \u0440\u043e\u0441\u0456\u0439\u0441\u044c\u043a\u0430 \u043f\u0440\u043e\u043f\u0430\u0433\u0430\u043d\u0434\u0430 \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u043e\u0432\u0443\u0454 \u043f\u043e\u0441\u0442\u0456\u0439\u043d\u043e. \u041d\u0430\u0440\u0430\u0437\u0456 \u0432\u043e\u043d\u0430 \u0441\u043a\u043b\u0430\u0434\u0430\u0454\u0442\u044c\u0441\u044f \u0437 {number_of_concepts} \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0456\u0439 (\u0430\u0431\u043e \u0442\u0440\u043e\u043f\u0456\u0432, \u044f\u043a \u043c\u0438 \u0457\u0445 \u043d\u0430\u0437\u0438\u0432\u0430\u0454\u043c\u043e), \u044f\u043a\u0456 \u0441\u0442\u043e\u0441\u0443\u044e\u0442\u044c\u0441\u044f \u0432\u0441\u0456\u043b\u044f\u043a\u0438\u0445 \u0442\u0435\u043c, \u0432\u0456\u0434 \u0430\u0431\u043e\u0440\u0442\u0456\u0432 \u0434\u043e \u043f\u0440\u0438\u0437\u0435\u043c\u043b\u0435\u043d\u043d\u044f \u043d\u0430 \u043c\u0456\u0441\u044f\u0446\u044c, \u0430\u043b\u0435 \u0433\u043e\u043b\u043e\u0432\u043d\u0438\u043c \u0447\u0438\u043d\u043e\u043c \u0441\u043f\u0440\u044f\u043c\u043e\u0432\u0430\u043d\u0456 \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u043d\u0448\u0435\u043d\u043d\u044f \u0442\u0430 \u0431\u0440\u0435\u0445\u043d\u044e \u043f\u0440\u043e \u0423\u043a\u0440\u0430\u0457\u043d\u0443 \u0442\u0430 \u0457\u0457 \u043b\u044e\u0434\u0435\u0439. \u0426\u0435 \u0442\u0456 \u0456\u0434\u0435\u0457, \u044f\u043a\u0456 \u0432\u0438 \u043f\u043e\u0447\u0443\u0454\u0442\u0435, \u044f\u043a\u0449\u043e \u0432\u0432\u0456\u043c\u043a\u043d\u0435\u0442\u0435 \u0431\u0443\u0434\u044c-\u044f\u043a\u0438\u0439 \u0441\u0443\u0447\u0430\u0441\u043d\u0438\u0439 \u0440\u043e\u0441\u0456\u0439\u0441\u044c\u043a\u0438\u0439 \u043c\u0435\u0434\u0456\u0430-\u0440\u0435\u0441\u0443\u0440\u0441, \u0437\u0430 \u0432\u0438\u043d\u044f\u0442\u043a\u043e\u043c \u043f\u043e\u043b\u0456\u0442\u0438\u0447\u043d\u043e\u0457 \u043e\u043f\u043e\u0437\u0438\u0446\u0456\u0457 \u0443 \u0432\u0438\u0433\u043d\u0430\u043d\u043d\u0456, \u0442\u0430 \u0442\u0438\u0445, \u0449\u043e \u0437\u043e\u0441\u0435\u0440\u0435\u0434\u0436\u0443\u044e\u0442\u044c\u0441\u044f \u0432\u0438\u043d\u044f\u0442\u043a\u043e\u0432\u043e \u043d\u0430 \u0440\u043e\u0437\u0432\u0430\u0433\u0430\u0445 - \u0445\u043e\u0447\u0430 \u0446\u044f \u043e\u0441\u0442\u0430\u043d\u043d\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0456\u044f \u0442\u0435\u0436 \u0434\u0430\u043b\u0435\u043a\u0430 \u043d\u0435 \u0431\u0435\u0437\u0432\u0438\u043d\u043d\u0430, \u043e\u0441\u043a\u0456\u043b\u044c\u043a\u0438 \u0432\u0441\u0435 \u0431\u0456\u043b\u044c\u0448\u0435 \u0441\u043f\u0456\u0432\u0430\u043a\u0456\u0432, \u043f\u043e\u0435\u0442\u0456\u0432 \u0442\u0430 \u043f\u0440\u043e\u0434\u044e\u0441\u0435\u0440\u0456\u0432 \u043e\u0431\u0438\u0440\u0430\u044e\u0442\u044c \u043d\u0430\u043c\u0430\u0433\u0430\u044e\u0442\u044c\u0441\u044f \u043f\u0440\u0438\u043f\u0430\u0441\u0442\u0438 \u0434\u043e \u043d\u0435\u043f\u0435\u0440\u0435\u0441\u0438\u0445\u0430\u044e\u0447\u0438\u0445 \u0434\u0435\u0440\u0436\u0430\u0432\u043d\u0438\u0445 \u0433\u0440\u0443\u0434\u0435\u0439. \u0412\u0438 \u043f\u043e\u043c\u0456\u0442\u0438\u0442\u0435, \u0449\u043e \u0434\u0435\u044f\u043a\u0456 \u0437 \u043d\u0438\u0445 \u0432\u043e\u0447\u0435\u0432\u0438\u0434\u044c \u0441\u0443\u043f\u0435\u0440\u0435\u0447\u0430\u0442\u044c \u0456\u043d\u0448\u0438\u043c - \u0446\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e, \u043e\u0441\u043a\u0456\u043b\u044c\u043a\u0438 \u0432\u0456\u0434\u043e\u0431\u0440\u0430\u0436\u0430\u0454 \u0440\u0435\u0430\u043b\u044c\u043d\u0438\u0439 \u0441\u0442\u0430\u043d \u0440\u0435\u0447\u0435\u0439, \u043a\u043e\u043b\u0438 \u043b\u044e\u0434\u0438, \u0437 \u0456\u043d\u043e\u0434\u0456 \u0434\u0456\u0430\u043c\u0435\u0442\u0440\u0430\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0442\u0438\u043b\u0435\u0436\u043d\u0438\u043c\u0438 \u043f\u043e\u0433\u043b\u044f\u0434\u0430\u043c\u0438, \u043e\u0431'\u0454\u0434\u043d\u0443\u044e\u0442\u044c\u0441\u044f \u0432 \u0441\u043b\u0456\u043f\u043e\u043c\u0443 \u0437\u0430\u0445\u043e\u043f\u043b\u0435\u043d\u043d\u0456 \u0440\u0456\u0448\u0435\u043d\u043d\u044f\u043c\u0438, \u0434\u0456\u044f\u043b\u044c\u043d\u0456\u0441\u0442\u044c\u044e, \u0436\u0430\u0440\u0442\u0430\u043c\u0438 \u0442\u0430 \u0446\u0438\u0442\u0430\u0442\u0430\u043c\u0438 \u0412\u043e\u043b\u043e\u0434\u0438\u043c\u0438\u0440\u0430 \u041f\u0443\u0442\u0456\u043d\u0430. \u042f\u043a\u0449\u043e \u0432\u0438 \u0446\u0456\u043a\u0430\u0432\u0438\u0442\u0435\u0441\u044c, \u044f\u043a \u0446\u0435 \u0432\u0437\u0430\u0433\u0430\u043b\u0456 \u043c\u043e\u0436\u043b\u0438\u0432\u043e, - \u0432\u0435\u0441\u044c \u043d\u0430\u0448 \u043f\u0440\u043e\u0435\u043a\u0442 \u043f\u0440\u0438\u0441\u0432\u044f\u0447\u0435\u043d\u043e \u043f\u043e\u0448\u0443\u043a\u0443 \u0432\u0456\u0434\u043f\u043e\u0432\u0456\u0434\u0456 \u043d\u0430 \u0446\u0435 \u043f\u0438\u0442\u0430\u043d\u043d\u044f. \u041c\u0438 \u0432\u0432\u0430\u0436\u0430\u0454\u043c\u043e, \u0449\u043e \u0420\u0423\u041a\u0420 - \u0446\u0435 \u043a\u0440\u043e\u043a \u0443 \u0446\u044c\u043e\u043c\u0443 \u043d\u0430\u043f\u0440\u044f\u043c\u043a\u0443. \u042f\u043a \u0446\u0435 \u043f\u0440\u0430\u0446\u044e\u0454 \u0420\u0423\u041a\u0420 \u0431\u0430\u0437\u0443\u0454\u0442\u044c\u0441\u044f \u043d\u0430 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0456 \u043f\u0456\u0434 \u043d\u0430\u0437\u0432\u043e\u044e Simple Knowledge Organization System (SKOS), \u044f\u043a\u0438\u0439 \u0431\u0443\u0432 \u0440\u043e\u0437\u0440\u043e\u0431\u043b\u0435\u043d World Wide Web Consortium (\u0432\u0456\u0434\u043e\u043c\u0438\u043c \u044f\u043a W3C) \u0432 >>>> \u0433\u043e\u043b\u043e\u0432\u043d\u0438\u043c \u0447\u0438\u043d\u043e\u043c \u0434\u043b\u044f \u0440\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0446\u0456\u0457 \u0456\u0441\u043d\u0443\u044e\u0447\u0438\u0445 \u0441\u0438\u0441\u0442\u0435\u043c \u0437\u043d\u0430\u043d\u044c (\u0442\u0430\u043a\u0438\u0445 \u044f\u043a \u0431\u0456\u0431\u043b\u0456\u043e\u0442\u0435\u0447\u043d\u0456 \u043a\u0430\u0442\u0430\u043b\u043e\u0433\u0438) \u0432 \u0446\u0438\u0444\u0440\u043e\u0432\u043e\u043c\u0443 \u0444\u043e\u0440\u043c\u0430\u0442\u0456. \u0412\u0430\u0436\u043b\u0438\u0432\u043e \u0437\u043d\u0430\u0442\u0438, \u0449\u043e \u0432\u043e\u043d\u0430 \u0432\u0438\u0437\u043d\u0430\u0447\u0430\u0454 2 \u0442\u0438\u043f\u0438 \u0432\u0456\u0434\u043d\u043e\u0441\u0438\u043d: broader (\u0448\u0438\u0440\u0448\u0435) - narrower (\u0432\u0443\u0436\u0447\u0435) - \u0446\u0435 \u0456\u0454\u0440\u0430\u0440\u0445\u0456\u0447\u043d\u0438\u0439 \u0442\u0438\u043f, \u044f\u043a\u0438\u0439 \u0432\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u044e\u0454, \u0449\u043e \u043f\u0435\u0432\u043d\u0430 \u0456\u0434\u0435\u044f \u0454 \u0431\u0456\u043b\u044c\u0448 \u0437\u0430\u0433\u0430\u043b\u044c\u043d\u043e\u044e \u0430\u0431\u043e \u0431\u0456\u043b\u044c\u0448 \u0441\u043f\u0435\u0446\u0438\u0444\u0456\u0447\u043d\u043e\u044e \u0432\u0456\u0434\u043d\u043e\u0441\u043d\u043e \u0456\u043d\u0448\u043e\u0457 \u043f\u0435\u0432\u043d\u043e\u0457 \u0456\u0434\u0435\u0457; \u0430 related \u0432\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u044e \u043d\u0435-\u0456\u0454\u0440\u0430\u0440\u0445\u0456\u0447\u043d\u0438\u0439 \u0437\u0432'\u044f\u0437\u043e\u043a \u043c\u0456\u0436 \u0434\u0432\u043e\u043c\u0430 \u0456\u0434\u0435\u044f\u043c\u0438. \u041d\u0430\u043f\u0440\u0438\u043a\u043b\u0430\u0434, \u0456\u0434\u0435\u044f, \u0449\u043e \u0433\u043e\u043c\u043e\u0441\u0435\u043a\u0441\u0443\u0430\u043b\u0456\u0437\u043c \u0454 \u0433\u0440\u0456\u0445\u043e\u043c , \u0454 \u0432\u0443\u0436\u0447\u043e\u044e \u0437\u0430 \u0456\u0434\u0435\u044e, \u0449\u043e \u0433\u043e\u043c\u043e\u0441\u0435\u043a\u0441\u0443\u0430\u043b\u0456\u0437\u043c \u0446\u0435 \u043f\u043e\u0433\u0430\u043d\u043e , \u0430 \u0446\u044f \u043e\u0441\u0442\u0430\u043d\u043d\u044f, \u0432 \u0441\u0432\u043e\u044e \u0447\u0435\u0440\u0433\u0443, \u043f\u043e\u0432'\u044f\u0437\u0430\u043d\u0430 \u0437 \u0456\u0434\u0435\u0454\u044e, \u0449\u043e \u043b\u0456\u0431\u0435\u0440\u0430\u043b\u044c\u043d\u0456 \u0446\u0456\u043d\u043d\u043e\u0441\u0442\u0456 \u0431\u0443\u043b\u0438 \u0432\u0438\u0433\u0430\u0434\u0430\u043d\u0456 \u044f\u043a \u0456\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043b\u044f \u0440\u0443\u0439\u043d\u0443\u0432\u0430\u043d\u043d\u044f \u043d\u0430\u0446\u0456\u0439 \u0437\u0441\u0435\u0440\u0435\u0434\u0438\u043d\u0438 . \u0421\u0442\u0440\u0456\u043b\u043a\u0438 \u043d\u0430 \u0434\u0456\u0430\u0433\u0440\u0430\u043c\u0456 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442\u044c \u0432\u0456\u0434\u043d\u043e\u0441\u0438\u043d\u0438 \u0448\u0438\u0440\u0448\u0435 / \u0432\u0443\u0436\u0447\u0435 , \u0442\u0430 \u0432\u043a\u0430\u0437\u0443\u044e\u0442\u044c \u043d\u0430 \u0448\u0438\u0440\u0448\u0438\u0439 \u0442\u0435\u0440\u043c\u0456\u043d. \u0420\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0446\u0456\u044f \u043f\u043e\u0432'\u044f\u0437\u0430\u043d\u0438\u0445 \u0432\u0456\u0434\u043d\u043e\u0441\u0438\u043d \u043d\u0430\u0440\u0430\u0437\u0456 \u0443 \u0440\u043e\u0437\u0440\u043e\u0431\u0446\u0456. \u0414\u0432\u0430 \u0432\u0435\u043b\u0438\u043a\u0438\u0445 \u043a\u043e\u043b\u0430 \u043c\u0456\u0441\u0442\u044f\u0442\u044c 15 \u043a\u043e\u0440\u0456\u043d\u043d\u0438\u0445 \u0456\u0434\u0435\u0439: \u043c\u0438 \u0432\u0432\u0430\u0436\u0430\u0454\u043c\u043e \u0457\u0445 \u0442\u0430\u043a\u0438\u043c\u0438, \u0442\u043e\u043c\u0443 \u0449\u043e \u0437 \u043d\u0438\u0445 \u0441\u043a\u043b\u0430\u0434\u043d\u043e \u0432\u0438\u0432\u0435\u0441\u0442\u0438 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0438 \u0449\u0435 \u0431\u0456\u043b\u044c\u0448 \u0437\u0430\u0433\u0430\u043b\u044c\u043d\u0456 (\u0430\u043b\u0435 \u0434\u0438\u0432. \u043d\u0438\u0436\u0447\u0435). \u041e\u0434\u043d\u0435 \u0437 \u043a\u0456\u043b \u043c\u0456\u0441\u0442\u0438\u0442\u044c \u043a\u043e\u043b\u0435\u043a\u0446\u0456\u044e \u0430\u043d\u0442\u0438\u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0438\u0445 \u0456\u0434\u0435\u0439, \u0434\u0440\u0443\u0433\u0435 - \u0432\u0441\u0435 \u0456\u043d\u0448\u0435. \u0426\u044f \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u0434\u043e\u0441\u0438\u0442\u044c \u0434\u043e\u0432\u0456\u043b\u044c\u043d\u0430, \u0442\u043e\u0431\u0442\u043e \u043f\u0440\u043e\u0434\u0438\u043a\u0442\u043e\u0432\u0430\u043d\u0430 \u043f\u0435\u0440\u0435\u0432\u0430\u0436\u043d\u043e \u0435\u0441\u0442\u0435\u0442\u0438\u043a\u043e\u044e; \u043e\u0434\u043d\u0430\u043a \u0432\u0438\u0437\u043d\u0430\u0447\u0435\u043d\u0456 \u043d\u0430\u043c\u0438 \u0432\u0456\u0434\u043d\u043e\u0441\u0438\u043d\u0438 \u0454 \u043e\u0431\u0491\u0440\u0443\u043d\u0442\u043e\u0432\u0430\u043d\u0438\u043c\u0438. \u0420\u043e\u0431\u043e\u0442\u0430 \u043f\u0440\u043e\u0434\u043e\u0432\u0436\u0443\u0454\u0442\u044c\u0441\u044f \u0420\u0423\u041a\u0420 \u043d\u0435 \u0454 \u0434\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043e\u044e \u0440\u043e\u0431\u043e\u0442\u043e\u044e: \u0434\u0435\u044f\u043a\u0456 \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0456\u0457 \u0449\u0435, \u043c\u043e\u0436\u043b\u0438\u0432\u043e, \u043d\u0435 \u0432\u0438\u044f\u0432\u043b\u0435\u043d\u0456, \u0456\u043d\u0448\u0456 \u0442\u0456\u043b\u044c\u043a\u0438 \u043c\u0430\u044e\u0442\u044c \u0437'\u044f\u0432\u0438\u0442\u0438\u0441\u044f - \u0446\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c \u0441\u043a\u043b\u0430\u0434\u043d\u0430 \u0456 \u0434\u0443\u0436\u0435 \u0448\u0432\u0438\u0434\u043a\u043e\u043f\u043b\u0438\u043d\u043d\u0430; \u0457\u0457 \u0432\u0438\u0432\u0447\u0435\u043d\u043d\u044f \u0442\u0430 \u0430\u043d\u0430\u043b\u0456\u0437 \u0437\u0430\u0439\u043c\u0430\u044e\u0442\u044c \u0447\u0430\u0441 \u0456 \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u044e\u0442\u044c \u043f\u0440\u0430\u0446\u0456. \u042f\u043a\u0449\u043e \u0432\u0438 \u0432\u0456\u0434\u0447\u0443\u0432\u0430\u0454\u0442\u0435, \u0449\u043e \u043c\u043e\u0436\u0435\u0442\u0435 \u0434\u043e\u043f\u043e\u043c\u043e\u0433\u0442\u0438 \u043d\u0430\u043c \u043f\u0440\u043e\u0441\u0443\u0432\u0430\u0442\u0438 \u0446\u0435 \u0432\u0441\u0435 \u0432\u043f\u0435\u0440\u0435\u0434, \u043d\u0430\u0434\u0456\u0448\u043b\u0456\u0442\u044c \u043d\u0430\u043c \u043b\u0438\u0441\u0442\u0430.","title":"RUCR Intro Description"},{"location":"working/onto/rucr_intro/#ruskymir-creed-rucr","text":"What you can see here is a model of concepts and ideas commonly re-used by the Russian propaganda on the daily basis. It is currently comprised of {number_of_concepts} concepts (or tropes, as we call them) touching on all kinds of topics, from abortions to moon landing, but mainly concerned with diminishing and lying about Ukraine and its people. These are the ideas you'd hear if you were to turn on any today's Russian media outlet, apart from the exiled political opposition and those that focus on entertainment exclusively - although, this last category is far from innocent, as more and more singers, poets and producers turn to suck from the perennial tit of the state. You will notice that some of them would explicitly contradict others - this is normal, because it reflects the actual state of affairs, where people with, sometimes, diametrically opposing views unite in the blind adoration of Vladimir Putin's activities, jokes and speeches. If you're wondering, how exactly is this even possible, - this entire project is dedicated to finding the answer to this question. We believe that RUCR is a step in the direction of it.","title":"Ruskymir Creed (RUCR)"},{"location":"working/onto/rucr_intro/#how-it-works","text":"RUCR is based on the Simple Knowledge Organization System (SKOS), which is a standard developed by World Wide Web Consortium (also known as W3C) in >>>> mainly for representing existing systems of knowledge (such as library catalogs) in digital format. What's important to know, is that it defines 2 kinds of relations: the broader - narrower kind is hierarchical and says that a certain idea is broader or narrower in relation to another certain idea; and related kind says that two ideas are connected in a non-hierarchical way. For example, the idea that homosexuality is a sin is narrower to the idea that homosexuality is wrong ; and that latter one is related to the idea that liberal values were made up as a tool for disrupting nations from within . Arrows on the diagram represent broader / narrower relationships, with the arrowhead pointing to the broader term. Representing related relations is under development. Two large circles contain the 15 core ideas: they are considered such because themselves they cannot be broken down any further (well, kind of - see the WIP note below). One of the circles contains anti-Ukrainian collection, the other - everything else. This layout is quite arbitrary, that is - dictated by aesthetics mostly; however the defined relations are solid.","title":"How it works"},{"location":"working/onto/rucr_intro/#wip","text":"RUCR is not a finished work: some concepts may not have been identified yet, others are yet to arise - this terrain is complex and fluid, and capturing and analyzing it takes time and work. If you feel like you can help us move things forward, send us a letter.","title":"WIP"},{"location":"working/onto/rucr_intro/#_1","text":"\u0422\u043e, \u0447\u0442\u043e \u0432\u044b \u043d\u0430\u0439\u0434\u0435\u0442\u0435 \u0437\u0434\u0435\u0441\u044c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0441\u043e\u0431\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u044c \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u0439 \u0438 \u0438\u0434\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u043e\u0439 \u043f\u0440\u043e\u043f\u0430\u0433\u0430\u043d\u0434\u043e\u0439. \u041d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u043e\u043d\u0430 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 {number_of_concepts} \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u0439 (\u0438\u043b\u0438, \u043a\u0430\u043a \u043c\u044b \u0438\u0445 \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u043c, \u0442\u0440\u043e\u043f\u043e\u0432), \u0437\u0430\u0442\u0440\u0430\u0433\u0438\u0432\u0430\u044e\u0449\u0438\u0445 \u0441\u0430\u043c\u044b\u0435 \u0440\u0430\u0437\u043d\u044b\u0435 \u0442\u0435\u043c\u044b, \u043e\u0442 \u0430\u0431\u043e\u0440\u0442\u043e\u0432 \u0434\u043e \u043f\u043e\u0441\u0430\u0434\u043a\u0438 \u043d\u0430 \u041b\u0443\u043d\u0443, \u043d\u043e \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u043d\u0430 \u043f\u0440\u0438\u043d\u0438\u0436\u0435\u043d\u0438\u0435 \u0423\u043a\u0440\u0430\u0438\u043d\u044b, \u0438 \u043b\u043e\u0436\u044c \u043e \u0435\u0435 \u043d\u0430\u0440\u043e\u0434\u0435. \u042d\u0442\u043e \u0442\u0435 \u0438\u0434\u0435\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u044b \u0443\u0441\u043b\u044b\u0448\u0438\u0442\u0435, \u0435\u0441\u043b\u0438 \u0432\u043a\u043b\u044e\u0447\u0438\u0442\u0435 \u043b\u044e\u0431\u043e\u0435 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0435 \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u043e\u0435 \u0421\u041c\u0418, \u0437\u0430 \u0438\u0441\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435\u043c \u0440\u0430\u0437\u0432\u0435 \u0447\u0442\u043e \u043f\u043e\u043b\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043e\u043f\u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u0432 \u0438\u0437\u0433\u043d\u0430\u043d\u0438\u0438, \u0438 \u0442\u0435\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u0438\u0441\u043a\u043b\u044e\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043d\u0430 \u0440\u0430\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f - \u0432\u043f\u0440\u043e\u0447\u0435\u043c, \u044d\u0442\u0443 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u044e\u044e \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044e \u0442\u043e\u0436\u0435 \u0441\u043b\u043e\u0436\u043d\u043e \u043d\u0430\u0437\u0432\u0430\u0442\u044c \u0447\u0438\u0441\u0442\u043e\u0439, \u0432\u0435\u0434\u044c \u0432\u0441\u0435 \u0431\u043e\u043b\u044c\u0448\u0435 \u0438 \u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u0435\u0432\u0446\u043e\u0432, \u043f\u043e\u044d\u0442\u043e\u0432, \u043f\u0440\u043e\u0434\u044e\u0441\u0435\u0440\u043e\u0432 \u0438 \u0442.\u043f. \u0441\u0442\u0430\u0440\u0430\u044e\u0442\u0441\u044f \u043f\u0440\u0438\u043d\u0438\u043a\u043d\u0443\u0442\u044c \u043a \u043d\u0435\u043f\u0435\u0440\u0435\u0441\u044b\u0445\u0430\u044e\u0449\u0435\u0439 \u0433\u043e\u0441\u0443\u0434\u0430\u0440\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439 \u0433\u0440\u0443\u0434\u0438. \u0412\u044b \u0437\u0430\u043c\u0435\u0442\u0438\u0442\u0435, \u0447\u0442\u043e \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u0437 \u043d\u0438\u0445 \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u043e \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0440\u0435\u0447\u0430\u0442 \u0434\u0440\u0443\u0433\u0438\u043c - \u044d\u0442\u043e \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e, \u043f\u043e\u0441\u043e\u043b\u044c\u043a\u0443 \u043e\u0442\u0440\u0430\u0436\u0430\u0435\u0442 \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u0435 \u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0432\u0435\u0449\u0435\u0439, \u0433\u0434\u0435 \u043b\u044e\u0434\u0438 \u0441 \u043f\u043e\u0440\u043e\u0439 \u0434\u0438\u0430\u043c\u0435\u0442\u0440\u0430\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u043f\u043e\u043b\u043e\u0436\u043d\u044b\u043c\u0438 \u0432\u0437\u0433\u043b\u044f\u0434\u0430\u043c\u0438 \u0441\u043b\u0438\u0432\u0430\u044e\u0442\u0441\u044f \u0432 \u0441\u043b\u0435\u043f\u043e\u043c \u043e\u0431\u043e\u0436\u0430\u043d\u0438\u0438 \u0440\u0435\u0448\u0435\u043d\u0438\u0439, \u0448\u0443\u0442\u043e\u043a \u0438 \u0446\u0438\u0442\u0430\u0442 \u0412\u043b\u0430\u0434\u0438\u043c\u0438\u0440\u0430 \u041f\u0443\u0442\u0438\u043d\u0430. \u0415\u0441\u043b\u0438 \u0432\u044b \u0437\u0430\u0434\u0430\u0435\u0442\u0435\u0441\u044c \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u043c, \u043a\u0430\u043a \u044d\u0442\u043e \u0432\u043e\u043e\u0431\u0449\u0435 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e, - \u0447\u0442\u043e \u0436, \u043d\u0430\u0448 \u043f\u0440\u043e\u0435\u043a\u0442 \u043f\u043e\u0441\u0432\u044f\u0449\u0435\u043d \u043f\u043e\u0438\u0441\u043a\u0443 \u043e\u0442\u0432\u0435\u0442\u0430 \u043d\u0430 \u044d\u0442\u043e\u0442 \u0432\u043e\u043f\u0440\u043e\u0441 \u0446\u0435\u043b\u0438\u043a\u043e\u043c \u0438 \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e. \u041c\u044b \u0441\u0447\u0438\u0442\u0430\u0435\u043c, \u0447\u0442\u043e \u0420\u0423\u041a\u0420 - \u044d\u0442\u043e \u0448\u0430\u0433 \u0432 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u043c \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0438.","title":"\u041a\u0440\u0435\u0434\u043e \u0440\u0443\u0441\u0441\u043a\u043e\u0433\u043e \u043c\u0438\u0440\u0430 (\u0420\u0423\u041a\u0420)"},{"location":"working/onto/rucr_intro/#_2","text":"\u0420\u0423\u041a\u0420 \u043e\u0441\u043d\u043e\u0432\u0430\u043d \u043d\u0430 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0435 \u043f\u043e\u0434 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435\u043c Simple Knowledge Organization System (SKOS), \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u044b\u043b \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u043d World Wide Web Consortium (\u0442\u0430\u043a\u0436\u0435 \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u043c \u043a\u0430\u043a W3C) \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u0434\u043b\u044f \u0440\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0443\u0436\u0435 \u0441\u043b\u043e\u0436\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u0441\u0438\u0441\u0442\u0435\u043c \u0437\u043d\u0430\u043d\u0438\u0439 (\u0442\u0430\u043a\u0438\u0445 \u043a\u0430\u043a \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u0447\u043d\u044b\u0435 \u043a\u0430\u0442\u0430\u043b\u043e\u0433\u0438) \u0432 \u0446\u0438\u0444\u0440\u043e\u0432\u043e\u043c \u0444\u043e\u0440\u043c\u0430\u0442\u0435. \u0417\u0434\u0435\u0441\u044c \u0432\u0430\u0436\u043d\u043e \u0437\u043d\u0430\u0442\u044c, \u0447\u0442\u043e SKOS \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442 2 \u0442\u0438\u043f\u0430 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439: broader (\u0431\u043e\u043b\u0435\u0435 \u0448\u0438\u0440\u043e\u043a\u0438\u0439) - narrower (\u0431\u043e\u043b\u0435\u0435 \u0443\u0437\u043a\u0438\u0439) - \u044d\u0442\u043e \u0438\u0435\u0440\u0430\u0440\u0445\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0442\u0438\u043f, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442, \u0447\u0442\u043e \u043d\u0435\u043a\u0430\u044f \u0438\u0434\u0435\u044f \u0431\u043e\u043b\u0435\u0435 \u043e\u0431\u0449\u0430\u044f \u0438\u043b\u0438 \u0431\u043e\u043b\u0435\u0435 \u0441\u043f\u0435\u0446\u0438\u0444\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u043e \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044e \u043a \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0434\u0440\u0443\u0433\u043e\u0439 \u0438\u0434\u0435\u0435; \u0438 related \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442 \u043d\u0435-\u0438\u0435\u0440\u0430\u0440\u0445\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u0441\u0432\u044f\u0437\u044c \u043c\u0435\u0436\u0434\u0443 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0430\u043c\u0438. \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0438\u0434\u0435\u044f, \u0447\u0442\u043e \u0433\u043e\u043c\u043e\u0441\u0435\u043a\u0441\u0443\u0430\u043b\u0438\u0437\u043c - \u044d\u0442\u043e \u0433\u0440\u0435\u0445 , \u0443\u0436\u0435 \u043f\u043e \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044e \u043a \u0438\u0434\u0435\u0435, \u0447\u0442\u043e \u0433\u043e\u043c\u043e\u0441\u0435\u043a\u0441\u0443\u0430\u043b\u0438\u0437\u043c - \u044d\u0442\u043e \u043f\u043b\u043e\u0445\u043e , \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0432 \u0441\u0432\u043e\u044e \u043e\u0447\u0435\u0440\u0435\u0434\u044c \u0441\u0432\u044f\u0437\u0430\u043d\u0430 \u0441 \u0438\u0434\u0435\u0435\u0439, \u0447\u0442\u043e \u043b\u0438\u0431\u0435\u0440\u0430\u043b\u044c\u043d\u044b\u0435 \u0446\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0431\u044b\u043b\u0438 \u0438\u0437\u043e\u0431\u0440\u0435\u0442\u0435\u043d\u044b \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043b\u044f \u0440\u0430\u0437\u0440\u0443\u0448\u0435\u043d\u0438\u044f \u043d\u0430\u0446\u0438\u0439 \u0438\u0437\u043d\u0443\u0442\u0440\u0438 . \u0421\u0432\u044f\u0437\u0438 \u043d\u0430 \u0434\u0438\u0430\u0433\u0440\u0430\u043c\u043c\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f \u0448\u0438\u0440\u0435 / \u0443\u0436\u0435 , \u043f\u0440\u0438\u0447\u0435\u043c \u0441\u0442\u0440\u0435\u043b\u043a\u0430 \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043d\u0430 \u0431\u043e\u043b\u0435\u0435 \u0448\u0438\u0440\u043e\u043a\u0438\u0439 \u0442\u0435\u0440\u043c\u0438\u043d. \u0420\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0446\u0438\u044f related \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0439 \u043f\u043e\u043a\u0430 \u0447\u0442\u043e \u0432 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0435. \u0414\u0432\u0430 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u043a\u0440\u0443\u0433\u0430 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 15 \u043a\u043e\u0440\u043d\u0435\u0432\u044b\u0445 \u0438\u0434\u0435\u0439: \u043c\u044b \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0438\u0445 \u0442\u0430\u043a\u043e\u0432\u044b\u043c\u0438, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0432 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 \u043d\u0438\u0445 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0430\u044f \u0434\u0435\u0442\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0437\u0430\u0442\u0440\u0443\u0434\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u0430 (\u043e\u0434\u043d\u0430\u043a\u043e \u0441\u043c. \u043f\u0440\u0438\u043c\u0435\u0447\u0430\u043d\u0438\u0435 \u043d\u0438\u0436\u0435). \u041e\u0434\u0438\u043d \u0438\u0437 \u043d\u0438\u0445 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u043a\u043e\u043b\u043b\u0435\u043a\u0446\u0438\u044e \u0430\u043d\u0442\u0438\u0443\u043a\u0440\u0430\u0438\u043d\u0441\u043a\u0438\u0445 \u0438\u0434\u0435\u0439, \u0434\u0440\u0443\u0433\u043e\u0439 - \u0432\u0441\u0435 \u043f\u0440\u043e\u0447\u0435\u0435. \u0422\u0430\u043a\u0430\u044f \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u0432\u043f\u043e\u043b\u043d\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u043b\u044c\u043d\u0430, \u0442\u043e \u0435\u0441\u0442\u044c, \u043f\u0440\u043e\u0434\u0438\u043a\u0442\u043e\u0432\u0430\u043d\u043e \u043f\u0440\u0435\u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u044d\u0441\u0442\u0435\u0442\u0438\u043a\u043e\u0439; \u043e\u0434\u043d\u0430\u043a\u043e \u0442\u0435 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u044b \u043c\u0435\u0436\u0434\u0443 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0430\u043c\u0438, - \u0432 \u043f\u043e\u0440\u044f\u0434\u043a\u0435.","title":"\u041a\u0430\u043a \u044d\u0442\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442"},{"location":"working/onto/rucr_intro/#_3","text":"\u0420\u0423\u041a\u0420 \u043d\u0435 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u044b\u043c \u043f\u0440\u043e\u0435\u043a\u0442\u043e\u043c: \u043a\u0430\u043a\u0438\u0435-\u0442\u043e \u0438\u0434\u0435\u0438 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u0435\u0449\u0435 \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u044b, \u0434\u0440\u0443\u0433\u0438\u043c \u0435\u0449\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u043e\u0438\u0442 \u043f\u0440\u043e\u044f\u0432\u0438\u0442\u044c\u0441\u044f - \u044d\u0442\u043e \u0442\u0435\u0440\u0440\u0438\u0442\u043e\u0440\u0438\u044f \u0441\u043b\u043e\u0436\u043d\u0430\u044f \u0438 \u0438\u0437\u043c\u0435\u043d\u0447\u0438\u0432\u0430\u044f, \u0441\u0431\u043e\u0440 \u0438 \u0430\u043d\u0430\u043b\u0438\u0437 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u0442\u0440\u0435\u0431\u0443\u044e\u0442 \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0438 \u0442\u0440\u0443\u0434\u0430. \u0415\u0441\u043b\u0438 \u0432\u044b \u0434\u0443\u043c\u0430\u0435\u0442\u0435, \u0447\u0442\u043e \u043c\u043e\u0436\u0435\u0442\u0435 \u043f\u043e\u043c\u043e\u0447\u044c \u043d\u0430\u043c \u043f\u0440\u043e\u0434\u0432\u0438\u043d\u0443\u0442\u044c\u0441\u044f \u0434\u0430\u043b\u044c\u0448\u0435, \u043d\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u043d\u0430\u043c \u043f\u0438\u0441\u044c\u043c\u043e.","title":"\u0420\u0430\u0431\u043e\u0442\u0430 \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0430\u0435\u0442\u0441\u044f"},{"location":"working/onto/rucr_intro/#_4","text":"\u0422\u0435, \u0449\u043e \u0432\u0438 \u0442\u0443\u0442 \u0431\u0430\u0447\u0438\u0442\u0435, \u0454 \u043c\u043e\u0434\u0435\u043b\u043b\u044e \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0456\u0439 \u0442\u0430 \u0456\u0434\u0435\u0439, \u044f\u043a\u0456 \u0440\u043e\u0441\u0456\u0439\u0441\u044c\u043a\u0430 \u043f\u0440\u043e\u043f\u0430\u0433\u0430\u043d\u0434\u0430 \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u043e\u0432\u0443\u0454 \u043f\u043e\u0441\u0442\u0456\u0439\u043d\u043e. \u041d\u0430\u0440\u0430\u0437\u0456 \u0432\u043e\u043d\u0430 \u0441\u043a\u043b\u0430\u0434\u0430\u0454\u0442\u044c\u0441\u044f \u0437 {number_of_concepts} \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0456\u0439 (\u0430\u0431\u043e \u0442\u0440\u043e\u043f\u0456\u0432, \u044f\u043a \u043c\u0438 \u0457\u0445 \u043d\u0430\u0437\u0438\u0432\u0430\u0454\u043c\u043e), \u044f\u043a\u0456 \u0441\u0442\u043e\u0441\u0443\u044e\u0442\u044c\u0441\u044f \u0432\u0441\u0456\u043b\u044f\u043a\u0438\u0445 \u0442\u0435\u043c, \u0432\u0456\u0434 \u0430\u0431\u043e\u0440\u0442\u0456\u0432 \u0434\u043e \u043f\u0440\u0438\u0437\u0435\u043c\u043b\u0435\u043d\u043d\u044f \u043d\u0430 \u043c\u0456\u0441\u044f\u0446\u044c, \u0430\u043b\u0435 \u0433\u043e\u043b\u043e\u0432\u043d\u0438\u043c \u0447\u0438\u043d\u043e\u043c \u0441\u043f\u0440\u044f\u043c\u043e\u0432\u0430\u043d\u0456 \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u043d\u0448\u0435\u043d\u043d\u044f \u0442\u0430 \u0431\u0440\u0435\u0445\u043d\u044e \u043f\u0440\u043e \u0423\u043a\u0440\u0430\u0457\u043d\u0443 \u0442\u0430 \u0457\u0457 \u043b\u044e\u0434\u0435\u0439. \u0426\u0435 \u0442\u0456 \u0456\u0434\u0435\u0457, \u044f\u043a\u0456 \u0432\u0438 \u043f\u043e\u0447\u0443\u0454\u0442\u0435, \u044f\u043a\u0449\u043e \u0432\u0432\u0456\u043c\u043a\u043d\u0435\u0442\u0435 \u0431\u0443\u0434\u044c-\u044f\u043a\u0438\u0439 \u0441\u0443\u0447\u0430\u0441\u043d\u0438\u0439 \u0440\u043e\u0441\u0456\u0439\u0441\u044c\u043a\u0438\u0439 \u043c\u0435\u0434\u0456\u0430-\u0440\u0435\u0441\u0443\u0440\u0441, \u0437\u0430 \u0432\u0438\u043d\u044f\u0442\u043a\u043e\u043c \u043f\u043e\u043b\u0456\u0442\u0438\u0447\u043d\u043e\u0457 \u043e\u043f\u043e\u0437\u0438\u0446\u0456\u0457 \u0443 \u0432\u0438\u0433\u043d\u0430\u043d\u043d\u0456, \u0442\u0430 \u0442\u0438\u0445, \u0449\u043e \u0437\u043e\u0441\u0435\u0440\u0435\u0434\u0436\u0443\u044e\u0442\u044c\u0441\u044f \u0432\u0438\u043d\u044f\u0442\u043a\u043e\u0432\u043e \u043d\u0430 \u0440\u043e\u0437\u0432\u0430\u0433\u0430\u0445 - \u0445\u043e\u0447\u0430 \u0446\u044f \u043e\u0441\u0442\u0430\u043d\u043d\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0456\u044f \u0442\u0435\u0436 \u0434\u0430\u043b\u0435\u043a\u0430 \u043d\u0435 \u0431\u0435\u0437\u0432\u0438\u043d\u043d\u0430, \u043e\u0441\u043a\u0456\u043b\u044c\u043a\u0438 \u0432\u0441\u0435 \u0431\u0456\u043b\u044c\u0448\u0435 \u0441\u043f\u0456\u0432\u0430\u043a\u0456\u0432, \u043f\u043e\u0435\u0442\u0456\u0432 \u0442\u0430 \u043f\u0440\u043e\u0434\u044e\u0441\u0435\u0440\u0456\u0432 \u043e\u0431\u0438\u0440\u0430\u044e\u0442\u044c \u043d\u0430\u043c\u0430\u0433\u0430\u044e\u0442\u044c\u0441\u044f \u043f\u0440\u0438\u043f\u0430\u0441\u0442\u0438 \u0434\u043e \u043d\u0435\u043f\u0435\u0440\u0435\u0441\u0438\u0445\u0430\u044e\u0447\u0438\u0445 \u0434\u0435\u0440\u0436\u0430\u0432\u043d\u0438\u0445 \u0433\u0440\u0443\u0434\u0435\u0439. \u0412\u0438 \u043f\u043e\u043c\u0456\u0442\u0438\u0442\u0435, \u0449\u043e \u0434\u0435\u044f\u043a\u0456 \u0437 \u043d\u0438\u0445 \u0432\u043e\u0447\u0435\u0432\u0438\u0434\u044c \u0441\u0443\u043f\u0435\u0440\u0435\u0447\u0430\u0442\u044c \u0456\u043d\u0448\u0438\u043c - \u0446\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e, \u043e\u0441\u043a\u0456\u043b\u044c\u043a\u0438 \u0432\u0456\u0434\u043e\u0431\u0440\u0430\u0436\u0430\u0454 \u0440\u0435\u0430\u043b\u044c\u043d\u0438\u0439 \u0441\u0442\u0430\u043d \u0440\u0435\u0447\u0435\u0439, \u043a\u043e\u043b\u0438 \u043b\u044e\u0434\u0438, \u0437 \u0456\u043d\u043e\u0434\u0456 \u0434\u0456\u0430\u043c\u0435\u0442\u0440\u0430\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0442\u0438\u043b\u0435\u0436\u043d\u0438\u043c\u0438 \u043f\u043e\u0433\u043b\u044f\u0434\u0430\u043c\u0438, \u043e\u0431'\u0454\u0434\u043d\u0443\u044e\u0442\u044c\u0441\u044f \u0432 \u0441\u043b\u0456\u043f\u043e\u043c\u0443 \u0437\u0430\u0445\u043e\u043f\u043b\u0435\u043d\u043d\u0456 \u0440\u0456\u0448\u0435\u043d\u043d\u044f\u043c\u0438, \u0434\u0456\u044f\u043b\u044c\u043d\u0456\u0441\u0442\u044c\u044e, \u0436\u0430\u0440\u0442\u0430\u043c\u0438 \u0442\u0430 \u0446\u0438\u0442\u0430\u0442\u0430\u043c\u0438 \u0412\u043e\u043b\u043e\u0434\u0438\u043c\u0438\u0440\u0430 \u041f\u0443\u0442\u0456\u043d\u0430. \u042f\u043a\u0449\u043e \u0432\u0438 \u0446\u0456\u043a\u0430\u0432\u0438\u0442\u0435\u0441\u044c, \u044f\u043a \u0446\u0435 \u0432\u0437\u0430\u0433\u0430\u043b\u0456 \u043c\u043e\u0436\u043b\u0438\u0432\u043e, - \u0432\u0435\u0441\u044c \u043d\u0430\u0448 \u043f\u0440\u043e\u0435\u043a\u0442 \u043f\u0440\u0438\u0441\u0432\u044f\u0447\u0435\u043d\u043e \u043f\u043e\u0448\u0443\u043a\u0443 \u0432\u0456\u0434\u043f\u043e\u0432\u0456\u0434\u0456 \u043d\u0430 \u0446\u0435 \u043f\u0438\u0442\u0430\u043d\u043d\u044f. \u041c\u0438 \u0432\u0432\u0430\u0436\u0430\u0454\u043c\u043e, \u0449\u043e \u0420\u0423\u041a\u0420 - \u0446\u0435 \u043a\u0440\u043e\u043a \u0443 \u0446\u044c\u043e\u043c\u0443 \u043d\u0430\u043f\u0440\u044f\u043c\u043a\u0443.","title":"\u041a\u0440\u0435\u0434\u043e \u0440\u0443\u0441\u044c\u043a\u043e\u0433\u043e \u043c\u0438\u0440\u0443 (\u0420\u0423\u041a\u0420)"},{"location":"working/onto/rucr_intro/#_5","text":"\u0420\u0423\u041a\u0420 \u0431\u0430\u0437\u0443\u0454\u0442\u044c\u0441\u044f \u043d\u0430 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0456 \u043f\u0456\u0434 \u043d\u0430\u0437\u0432\u043e\u044e Simple Knowledge Organization System (SKOS), \u044f\u043a\u0438\u0439 \u0431\u0443\u0432 \u0440\u043e\u0437\u0440\u043e\u0431\u043b\u0435\u043d World Wide Web Consortium (\u0432\u0456\u0434\u043e\u043c\u0438\u043c \u044f\u043a W3C) \u0432 >>>> \u0433\u043e\u043b\u043e\u0432\u043d\u0438\u043c \u0447\u0438\u043d\u043e\u043c \u0434\u043b\u044f \u0440\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0446\u0456\u0457 \u0456\u0441\u043d\u0443\u044e\u0447\u0438\u0445 \u0441\u0438\u0441\u0442\u0435\u043c \u0437\u043d\u0430\u043d\u044c (\u0442\u0430\u043a\u0438\u0445 \u044f\u043a \u0431\u0456\u0431\u043b\u0456\u043e\u0442\u0435\u0447\u043d\u0456 \u043a\u0430\u0442\u0430\u043b\u043e\u0433\u0438) \u0432 \u0446\u0438\u0444\u0440\u043e\u0432\u043e\u043c\u0443 \u0444\u043e\u0440\u043c\u0430\u0442\u0456. \u0412\u0430\u0436\u043b\u0438\u0432\u043e \u0437\u043d\u0430\u0442\u0438, \u0449\u043e \u0432\u043e\u043d\u0430 \u0432\u0438\u0437\u043d\u0430\u0447\u0430\u0454 2 \u0442\u0438\u043f\u0438 \u0432\u0456\u0434\u043d\u043e\u0441\u0438\u043d: broader (\u0448\u0438\u0440\u0448\u0435) - narrower (\u0432\u0443\u0436\u0447\u0435) - \u0446\u0435 \u0456\u0454\u0440\u0430\u0440\u0445\u0456\u0447\u043d\u0438\u0439 \u0442\u0438\u043f, \u044f\u043a\u0438\u0439 \u0432\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u044e\u0454, \u0449\u043e \u043f\u0435\u0432\u043d\u0430 \u0456\u0434\u0435\u044f \u0454 \u0431\u0456\u043b\u044c\u0448 \u0437\u0430\u0433\u0430\u043b\u044c\u043d\u043e\u044e \u0430\u0431\u043e \u0431\u0456\u043b\u044c\u0448 \u0441\u043f\u0435\u0446\u0438\u0444\u0456\u0447\u043d\u043e\u044e \u0432\u0456\u0434\u043d\u043e\u0441\u043d\u043e \u0456\u043d\u0448\u043e\u0457 \u043f\u0435\u0432\u043d\u043e\u0457 \u0456\u0434\u0435\u0457; \u0430 related \u0432\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u044e \u043d\u0435-\u0456\u0454\u0440\u0430\u0440\u0445\u0456\u0447\u043d\u0438\u0439 \u0437\u0432'\u044f\u0437\u043e\u043a \u043c\u0456\u0436 \u0434\u0432\u043e\u043c\u0430 \u0456\u0434\u0435\u044f\u043c\u0438. \u041d\u0430\u043f\u0440\u0438\u043a\u043b\u0430\u0434, \u0456\u0434\u0435\u044f, \u0449\u043e \u0433\u043e\u043c\u043e\u0441\u0435\u043a\u0441\u0443\u0430\u043b\u0456\u0437\u043c \u0454 \u0433\u0440\u0456\u0445\u043e\u043c , \u0454 \u0432\u0443\u0436\u0447\u043e\u044e \u0437\u0430 \u0456\u0434\u0435\u044e, \u0449\u043e \u0433\u043e\u043c\u043e\u0441\u0435\u043a\u0441\u0443\u0430\u043b\u0456\u0437\u043c \u0446\u0435 \u043f\u043e\u0433\u0430\u043d\u043e , \u0430 \u0446\u044f \u043e\u0441\u0442\u0430\u043d\u043d\u044f, \u0432 \u0441\u0432\u043e\u044e \u0447\u0435\u0440\u0433\u0443, \u043f\u043e\u0432'\u044f\u0437\u0430\u043d\u0430 \u0437 \u0456\u0434\u0435\u0454\u044e, \u0449\u043e \u043b\u0456\u0431\u0435\u0440\u0430\u043b\u044c\u043d\u0456 \u0446\u0456\u043d\u043d\u043e\u0441\u0442\u0456 \u0431\u0443\u043b\u0438 \u0432\u0438\u0433\u0430\u0434\u0430\u043d\u0456 \u044f\u043a \u0456\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043b\u044f \u0440\u0443\u0439\u043d\u0443\u0432\u0430\u043d\u043d\u044f \u043d\u0430\u0446\u0456\u0439 \u0437\u0441\u0435\u0440\u0435\u0434\u0438\u043d\u0438 . \u0421\u0442\u0440\u0456\u043b\u043a\u0438 \u043d\u0430 \u0434\u0456\u0430\u0433\u0440\u0430\u043c\u0456 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442\u044c \u0432\u0456\u0434\u043d\u043e\u0441\u0438\u043d\u0438 \u0448\u0438\u0440\u0448\u0435 / \u0432\u0443\u0436\u0447\u0435 , \u0442\u0430 \u0432\u043a\u0430\u0437\u0443\u044e\u0442\u044c \u043d\u0430 \u0448\u0438\u0440\u0448\u0438\u0439 \u0442\u0435\u0440\u043c\u0456\u043d. \u0420\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0446\u0456\u044f \u043f\u043e\u0432'\u044f\u0437\u0430\u043d\u0438\u0445 \u0432\u0456\u0434\u043d\u043e\u0441\u0438\u043d \u043d\u0430\u0440\u0430\u0437\u0456 \u0443 \u0440\u043e\u0437\u0440\u043e\u0431\u0446\u0456. \u0414\u0432\u0430 \u0432\u0435\u043b\u0438\u043a\u0438\u0445 \u043a\u043e\u043b\u0430 \u043c\u0456\u0441\u0442\u044f\u0442\u044c 15 \u043a\u043e\u0440\u0456\u043d\u043d\u0438\u0445 \u0456\u0434\u0435\u0439: \u043c\u0438 \u0432\u0432\u0430\u0436\u0430\u0454\u043c\u043e \u0457\u0445 \u0442\u0430\u043a\u0438\u043c\u0438, \u0442\u043e\u043c\u0443 \u0449\u043e \u0437 \u043d\u0438\u0445 \u0441\u043a\u043b\u0430\u0434\u043d\u043e \u0432\u0438\u0432\u0435\u0441\u0442\u0438 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0438 \u0449\u0435 \u0431\u0456\u043b\u044c\u0448 \u0437\u0430\u0433\u0430\u043b\u044c\u043d\u0456 (\u0430\u043b\u0435 \u0434\u0438\u0432. \u043d\u0438\u0436\u0447\u0435). \u041e\u0434\u043d\u0435 \u0437 \u043a\u0456\u043b \u043c\u0456\u0441\u0442\u0438\u0442\u044c \u043a\u043e\u043b\u0435\u043a\u0446\u0456\u044e \u0430\u043d\u0442\u0438\u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0438\u0445 \u0456\u0434\u0435\u0439, \u0434\u0440\u0443\u0433\u0435 - \u0432\u0441\u0435 \u0456\u043d\u0448\u0435. \u0426\u044f \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u0434\u043e\u0441\u0438\u0442\u044c \u0434\u043e\u0432\u0456\u043b\u044c\u043d\u0430, \u0442\u043e\u0431\u0442\u043e \u043f\u0440\u043e\u0434\u0438\u043a\u0442\u043e\u0432\u0430\u043d\u0430 \u043f\u0435\u0440\u0435\u0432\u0430\u0436\u043d\u043e \u0435\u0441\u0442\u0435\u0442\u0438\u043a\u043e\u044e; \u043e\u0434\u043d\u0430\u043a \u0432\u0438\u0437\u043d\u0430\u0447\u0435\u043d\u0456 \u043d\u0430\u043c\u0438 \u0432\u0456\u0434\u043d\u043e\u0441\u0438\u043d\u0438 \u0454 \u043e\u0431\u0491\u0440\u0443\u043d\u0442\u043e\u0432\u0430\u043d\u0438\u043c\u0438.","title":"\u042f\u043a \u0446\u0435 \u043f\u0440\u0430\u0446\u044e\u0454"},{"location":"working/onto/rucr_intro/#_6","text":"\u0420\u0423\u041a\u0420 \u043d\u0435 \u0454 \u0434\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043e\u044e \u0440\u043e\u0431\u043e\u0442\u043e\u044e: \u0434\u0435\u044f\u043a\u0456 \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0456\u0457 \u0449\u0435, \u043c\u043e\u0436\u043b\u0438\u0432\u043e, \u043d\u0435 \u0432\u0438\u044f\u0432\u043b\u0435\u043d\u0456, \u0456\u043d\u0448\u0456 \u0442\u0456\u043b\u044c\u043a\u0438 \u043c\u0430\u044e\u0442\u044c \u0437'\u044f\u0432\u0438\u0442\u0438\u0441\u044f - \u0446\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c \u0441\u043a\u043b\u0430\u0434\u043d\u0430 \u0456 \u0434\u0443\u0436\u0435 \u0448\u0432\u0438\u0434\u043a\u043e\u043f\u043b\u0438\u043d\u043d\u0430; \u0457\u0457 \u0432\u0438\u0432\u0447\u0435\u043d\u043d\u044f \u0442\u0430 \u0430\u043d\u0430\u043b\u0456\u0437 \u0437\u0430\u0439\u043c\u0430\u044e\u0442\u044c \u0447\u0430\u0441 \u0456 \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u044e\u0442\u044c \u043f\u0440\u0430\u0446\u0456. \u042f\u043a\u0449\u043e \u0432\u0438 \u0432\u0456\u0434\u0447\u0443\u0432\u0430\u0454\u0442\u0435, \u0449\u043e \u043c\u043e\u0436\u0435\u0442\u0435 \u0434\u043e\u043f\u043e\u043c\u043e\u0433\u0442\u0438 \u043d\u0430\u043c \u043f\u0440\u043e\u0441\u0443\u0432\u0430\u0442\u0438 \u0446\u0435 \u0432\u0441\u0435 \u0432\u043f\u0435\u0440\u0435\u0434, \u043d\u0430\u0434\u0456\u0448\u043b\u0456\u0442\u044c \u043d\u0430\u043c \u043b\u0438\u0441\u0442\u0430.","title":"\u0420\u043e\u0431\u043e\u0442\u0430 \u043f\u0440\u043e\u0434\u043e\u0432\u0436\u0443\u0454\u0442\u044c\u0441\u044f"},{"location":"working/onto/setting_the_stage/","text":"What are we building? A set of interrelated ontologies and taxonomies that, taken together, describe the domain of propaganda ; and A number of web applications, whose structure is tied to these ontologies, and whose content is focused on the Russian propaganda phenomena that stems from the Vladimir Putin's delusional view of the world, and the Russo-Ukrainian war he started on Feb 24, 2022. Why are we building it? The domain in question is highly versatile, with natural language playing a major role in it. There are hundreds of entities involved in the process, the relationships between which have to be mined and parsed. Moreover, the amount of data that needs to be processed is insurmountable without extra tricks in our sleeve. One such trick is machine learning (we already have 1 application and there will be more). But it's the semantic web technologies that would help us simplify the task of tieing all the different pieces together into a coherent web of knowledge that can be used to answer some interesting questions. How are we building it? The scope, structure, and content of the ontologies would be driven by the actual data that our software is expected to interact with: that is, entities and relationships between them would be defined against the mass of available Russian data. However, in order to introduce a sort of validation element into the process, these entities must be applied to at least 1 of the following: The propaganda phenomena of the Nazi Germany, 1923-1945. The Rwandan genocide of 1994 and its roots. The individual instances and other entities would be generated (and regularly updated) by mining a variety of structured and unstructured data sources. The mining would be made automatic to the furthest extent possible, but with a required human-based verification. Natural language processing and entity disambiguation would be an integral part of the flow. Who is building it? As of the moment of writing this, exactly 1 person is involved in the ontology development. The rest of the Wapaganda team is working mainly with the web application. People skillful in machine learning, natural language processing, etc. may still join the team at some point, but for now we are learning all these things ourselves in order to apply them. Bias awareness notice The first author of the project hasn't left Ukraine since the war started, and all the other team members have some ties to the situation in one way or another - that is to say, we all have skin in the game. This provides energy and vigor, which are crucial in achieving the goal, but also inevitably results in a certain bias. The nature of this bias is known to us, and we are taking extra measures to prevent it from effecting the quality of the models. Who cares? Since one of the underlying drivers behind the entire project is the desire to understand, to make sense of what we are observing, everybody who experiences the same lack of clarity on the subject, may care. It is a firm conviction of the authors, that achieving the goals of the project would bring us slightly closer to understanding the process of human thinking in general, and of thinking aberrations, in particular.","title":"Ontologies. Setting the stage"},{"location":"working/onto/setting_the_stage/#what-are-we-building","text":"A set of interrelated ontologies and taxonomies that, taken together, describe the domain of propaganda ; and A number of web applications, whose structure is tied to these ontologies, and whose content is focused on the Russian propaganda phenomena that stems from the Vladimir Putin's delusional view of the world, and the Russo-Ukrainian war he started on Feb 24, 2022.","title":"What are we building?"},{"location":"working/onto/setting_the_stage/#why-are-we-building-it","text":"The domain in question is highly versatile, with natural language playing a major role in it. There are hundreds of entities involved in the process, the relationships between which have to be mined and parsed. Moreover, the amount of data that needs to be processed is insurmountable without extra tricks in our sleeve. One such trick is machine learning (we already have 1 application and there will be more). But it's the semantic web technologies that would help us simplify the task of tieing all the different pieces together into a coherent web of knowledge that can be used to answer some interesting questions.","title":"Why are we building it?"},{"location":"working/onto/setting_the_stage/#how-are-we-building-it","text":"The scope, structure, and content of the ontologies would be driven by the actual data that our software is expected to interact with: that is, entities and relationships between them would be defined against the mass of available Russian data. However, in order to introduce a sort of validation element into the process, these entities must be applied to at least 1 of the following: The propaganda phenomena of the Nazi Germany, 1923-1945. The Rwandan genocide of 1994 and its roots. The individual instances and other entities would be generated (and regularly updated) by mining a variety of structured and unstructured data sources. The mining would be made automatic to the furthest extent possible, but with a required human-based verification. Natural language processing and entity disambiguation would be an integral part of the flow.","title":"How are we building it?"},{"location":"working/onto/setting_the_stage/#who-is-building-it","text":"As of the moment of writing this, exactly 1 person is involved in the ontology development. The rest of the Wapaganda team is working mainly with the web application. People skillful in machine learning, natural language processing, etc. may still join the team at some point, but for now we are learning all these things ourselves in order to apply them.","title":"Who is building it?"},{"location":"working/onto/setting_the_stage/#bias-awareness-notice","text":"The first author of the project hasn't left Ukraine since the war started, and all the other team members have some ties to the situation in one way or another - that is to say, we all have skin in the game. This provides energy and vigor, which are crucial in achieving the goal, but also inevitably results in a certain bias. The nature of this bias is known to us, and we are taking extra measures to prevent it from effecting the quality of the models.","title":"Bias awareness notice"},{"location":"working/onto/setting_the_stage/#who-cares","text":"Since one of the underlying drivers behind the entire project is the desire to understand, to make sense of what we are observing, everybody who experiences the same lack of clarity on the subject, may care. It is a firm conviction of the authors, that achieving the goals of the project would bring us slightly closer to understanding the process of human thinking in general, and of thinking aberrations, in particular.","title":"Who cares?"},{"location":"working/onto/web_resources/","text":"W3C https://www.w3.org/2001/sw/ Upper Ontologies Terms mentioned in Relevance fields provide basis to be extended by the W ontology. FOAF Name FOAF :: Friend-of-a-Friend URL http://xmlns.com/foaf/0.1/# Description FOAF provides a standardized way to describe people and their relationships, such as friends, colleagues, and acquaintances Relevance FOAF uses foundational class Agent that has a number of relevant subclasses, namely Person , Group and Organization . Dublin Core Name URL https://www.dublincore.org/specifications/dublin-core/dcmi-terms/ Description Dublin Core is a metadata standard for describing digital resources, providing a set of basic elements to enhance resource discovery and interoperability on the web. It offers 15 core metadata elements, such as title , creator , date , and description , which can be used to describe various types of resources, including web pages, images, videos, and documents. Relevance DC should be used for all the media-entities. BIO Name BIO URL https://vocab.org/bio/ Description Vocabulary for describing biographical information about people, both living and dead Relevance BIO should be used as a FOAF extension to describe relationships between people, as well events related to them SIOC Name SIOC :: Semantically-Interlinked Online Communities URL http://rdfs.org/sioc/spec/# Description SIOC is an attempt to link online community sites (weblogs, message boards, wikis, etc.) and to describe the information that communities have about their structure and contents, and to find related information and new connections between content items and other community objects Relevance SIOC could be useful as far as describing the online communities and resources SKOS Name SKOS :: Simple Knowledge Organization System URL https://www.w3.org/2009/08/skos-reference/skos.html Description SKOS is a vocabulary and data model designed for representing and organizing knowledge organization systems, such as thesauri, taxonomies, and classification schemes. SKOS enables the creation of multilingual and interoperable knowledge models. Relevance SKOS would be useful for the task of modelling the knowledge domain - the system of axioms, assertions etc. that make up the propagandistic address. QUDT Name QUDT :: Quantities, Units, Dimensions, and Data Types URL https://qudt.org/ Description QUDT is a comprehensive ontology and data model that provides a standardized representation of physical quantities, units of measurement, dimensions, and data types Relevance Might not be relevant CiTO Name CiTO :: Citation Typing Ontology URL https://sparontologies.github.io/cito/current/cito.html# Description CiTO is an ontology that enables characterization of the nature or type of citations , both factually and rhetorically. Relevance CiTO would be useful for the task of modelling the knowledge domain - the system of axioms, assertions etc. that make up the propagandistic address, in support to SKOS. POWDER Name POWDER :: Protocol for Web Description Resources URL https://www.w3.org/TR/powder-primer/ Description POWDER provides a mechanism to describe and discover Web resources, in which respect its functionality overlaps with SIOC and DC. Relevance POWDER, although a W3C recommendation, failed to gain momentum and probably ain't relevant. Other ontologies YAGO Name YAGO URL https://yago-knowledge.org/ Description YAGO is a large knowledge base with general knowledge about people, cities, countries, movies, and organizations. Relevance YAGO provides 2 useful things: a way to properly link entities to Wikidata pages, and the ontology to support some extra kinds of relations. It would be useful for resolving specific entities (people, organizations, etc.) GeoNames Name GeoNames URL https://www.geonames.org/ Description Relevance Platforms AllegroGraph Name URL https://allegrograph.com Description Relevance Ontotext GraphDB Name URL https://www.ontotext.com/products/graphdb/ Description Relevance Stardog Name URL https://www.stardog.com Description Relevance MarkLogic Name URL https://www.marklogic.com/ Description Relevance RDFox Name URL https://www.oxfordsemantic.tech Description Relevance Neon4J Name URL Description Relevance RDF4J Name URL https://rdf4j.org/ Description Relevance Apache Jena Name URL https://jena.apache.org/ Description Relevance CozoDB Name URL https://github.com/cozodb Description Relevance Standards OWL Name OWL :: Web Ontology Language URL https://www.w3.org/TR/2004/REC-owl-semantics-20040210/ Description Relevance RDF Name RDF :: Resource Description Framework URL https://www.w3.org/TR/rdf11-primer Description Relevance Tools AIDA Name URL https://github.com/codepie/aida Description Relevance Protege Name URL https://protegeproject.github.io/protege/ Description Relevance Linked Pipes Name URL https://github.com/linkedpipes Description Relevance","title":"Semantic Web Resources"},{"location":"working/onto/web_resources/#w3c","text":"https://www.w3.org/2001/sw/","title":"W3C"},{"location":"working/onto/web_resources/#upper-ontologies","text":"Terms mentioned in Relevance fields provide basis to be extended by the W ontology.","title":"Upper Ontologies"},{"location":"working/onto/web_resources/#foaf","text":"Name FOAF :: Friend-of-a-Friend URL http://xmlns.com/foaf/0.1/# Description FOAF provides a standardized way to describe people and their relationships, such as friends, colleagues, and acquaintances Relevance FOAF uses foundational class Agent that has a number of relevant subclasses, namely Person , Group and Organization .","title":"FOAF"},{"location":"working/onto/web_resources/#dublin-core","text":"Name URL https://www.dublincore.org/specifications/dublin-core/dcmi-terms/ Description Dublin Core is a metadata standard for describing digital resources, providing a set of basic elements to enhance resource discovery and interoperability on the web. It offers 15 core metadata elements, such as title , creator , date , and description , which can be used to describe various types of resources, including web pages, images, videos, and documents. Relevance DC should be used for all the media-entities.","title":"Dublin Core"},{"location":"working/onto/web_resources/#bio","text":"Name BIO URL https://vocab.org/bio/ Description Vocabulary for describing biographical information about people, both living and dead Relevance BIO should be used as a FOAF extension to describe relationships between people, as well events related to them","title":"BIO"},{"location":"working/onto/web_resources/#sioc","text":"Name SIOC :: Semantically-Interlinked Online Communities URL http://rdfs.org/sioc/spec/# Description SIOC is an attempt to link online community sites (weblogs, message boards, wikis, etc.) and to describe the information that communities have about their structure and contents, and to find related information and new connections between content items and other community objects Relevance SIOC could be useful as far as describing the online communities and resources","title":"SIOC"},{"location":"working/onto/web_resources/#skos","text":"Name SKOS :: Simple Knowledge Organization System URL https://www.w3.org/2009/08/skos-reference/skos.html Description SKOS is a vocabulary and data model designed for representing and organizing knowledge organization systems, such as thesauri, taxonomies, and classification schemes. SKOS enables the creation of multilingual and interoperable knowledge models. Relevance SKOS would be useful for the task of modelling the knowledge domain - the system of axioms, assertions etc. that make up the propagandistic address.","title":"SKOS"},{"location":"working/onto/web_resources/#qudt","text":"Name QUDT :: Quantities, Units, Dimensions, and Data Types URL https://qudt.org/ Description QUDT is a comprehensive ontology and data model that provides a standardized representation of physical quantities, units of measurement, dimensions, and data types Relevance Might not be relevant","title":"QUDT"},{"location":"working/onto/web_resources/#cito","text":"Name CiTO :: Citation Typing Ontology URL https://sparontologies.github.io/cito/current/cito.html# Description CiTO is an ontology that enables characterization of the nature or type of citations , both factually and rhetorically. Relevance CiTO would be useful for the task of modelling the knowledge domain - the system of axioms, assertions etc. that make up the propagandistic address, in support to SKOS.","title":"CiTO"},{"location":"working/onto/web_resources/#powder","text":"Name POWDER :: Protocol for Web Description Resources URL https://www.w3.org/TR/powder-primer/ Description POWDER provides a mechanism to describe and discover Web resources, in which respect its functionality overlaps with SIOC and DC. Relevance POWDER, although a W3C recommendation, failed to gain momentum and probably ain't relevant.","title":"POWDER"},{"location":"working/onto/web_resources/#other-ontologies","text":"","title":"Other ontologies"},{"location":"working/onto/web_resources/#yago","text":"Name YAGO URL https://yago-knowledge.org/ Description YAGO is a large knowledge base with general knowledge about people, cities, countries, movies, and organizations. Relevance YAGO provides 2 useful things: a way to properly link entities to Wikidata pages, and the ontology to support some extra kinds of relations. It would be useful for resolving specific entities (people, organizations, etc.)","title":"YAGO"},{"location":"working/onto/web_resources/#geonames","text":"Name GeoNames URL https://www.geonames.org/ Description Relevance","title":"GeoNames"},{"location":"working/onto/web_resources/#platforms","text":"","title":"Platforms"},{"location":"working/onto/web_resources/#allegrograph","text":"Name URL https://allegrograph.com Description Relevance","title":"AllegroGraph"},{"location":"working/onto/web_resources/#ontotext-graphdb","text":"Name URL https://www.ontotext.com/products/graphdb/ Description Relevance","title":"Ontotext GraphDB"},{"location":"working/onto/web_resources/#stardog","text":"Name URL https://www.stardog.com Description Relevance","title":"Stardog"},{"location":"working/onto/web_resources/#marklogic","text":"Name URL https://www.marklogic.com/ Description Relevance","title":"MarkLogic"},{"location":"working/onto/web_resources/#rdfox","text":"Name URL https://www.oxfordsemantic.tech Description Relevance","title":"RDFox"},{"location":"working/onto/web_resources/#neon4j","text":"Name URL Description Relevance","title":"Neon4J"},{"location":"working/onto/web_resources/#rdf4j","text":"Name URL https://rdf4j.org/ Description Relevance","title":"RDF4J"},{"location":"working/onto/web_resources/#apache-jena","text":"Name URL https://jena.apache.org/ Description Relevance","title":"Apache Jena"},{"location":"working/onto/web_resources/#cozodb","text":"Name URL https://github.com/cozodb Description Relevance","title":"CozoDB"},{"location":"working/onto/web_resources/#standards","text":"","title":"Standards"},{"location":"working/onto/web_resources/#owl","text":"Name OWL :: Web Ontology Language URL https://www.w3.org/TR/2004/REC-owl-semantics-20040210/ Description Relevance","title":"OWL"},{"location":"working/onto/web_resources/#rdf","text":"Name RDF :: Resource Description Framework URL https://www.w3.org/TR/rdf11-primer Description Relevance","title":"RDF"},{"location":"working/onto/web_resources/#tools","text":"","title":"Tools"},{"location":"working/onto/web_resources/#aida","text":"Name URL https://github.com/codepie/aida Description Relevance","title":"AIDA"},{"location":"working/onto/web_resources/#protege","text":"Name URL https://protegeproject.github.io/protege/ Description Relevance","title":"Protege"},{"location":"working/onto/web_resources/#linked-pipes","text":"Name URL https://github.com/linkedpipes Description Relevance","title":"Linked Pipes"},{"location":"working/telegram/tg_data/","text":"Telegram Data refers to the 2 initiatives involving the telegram-related data: telegram stats (using telemetr.io ) and telegram messages (using Telegram API). Progress Log July 2, 2023 This flow remained untouched until about 3 weeks ago, when the refactoring work started. The refactoring is carried on by @dx The refactoring implies fusing all the components of the flow into a single OOP-based app, where resolving of channels, collection of stats, and downloading the messages, are all implemented as methods of primary classes. FastAPI was selected as the framework for this app, with SQLAlchemy 2 as ORM. Whenever possible, methods are made async . See ARCH-A-10 November 25, 2023 The development of the new telegram app is approaching final stages. The app is intended to run in a Docker container on our stand-alone server. During development, datetime fields were updated to not contain TZ information, and coerce dates to UTC as much as possible. Telegram channel's status field was changed to be of type text instead of boolean so that the edge-cases (such as Telemetr using an already registered internal ID) can be handled better. There might be issues with integrating the new app into the wapatools repository accounting for its structure. Telegram Stats Telegram statistical data is obtained from the telemetr.io web service. Results are stored in the telegram_channels_stats table. Available data ... Primary goal Build visualization(s) with available data. Make telegram collection process async Telegram Messages Telegram messages are obtained by direct download via the official Telegram API, and then stored in the telegram_messages table in the data schema of the DB. Primary goal Perform various kinds of analyses, such as Deleted messages rate; Commonly used words; Share of reposts/forwards; NLP stuff; TBD Secondary goal Identification of important messages; (automatic) translation into English / Ukrainian","title":"Telegram Data"},{"location":"working/telegram/tg_data/#progress-log","text":"","title":"Progress Log"},{"location":"working/telegram/tg_data/#july-2-2023","text":"This flow remained untouched until about 3 weeks ago, when the refactoring work started. The refactoring is carried on by @dx The refactoring implies fusing all the components of the flow into a single OOP-based app, where resolving of channels, collection of stats, and downloading the messages, are all implemented as methods of primary classes. FastAPI was selected as the framework for this app, with SQLAlchemy 2 as ORM. Whenever possible, methods are made async . See ARCH-A-10","title":"July 2, 2023"},{"location":"working/telegram/tg_data/#november-25-2023","text":"The development of the new telegram app is approaching final stages. The app is intended to run in a Docker container on our stand-alone server. During development, datetime fields were updated to not contain TZ information, and coerce dates to UTC as much as possible. Telegram channel's status field was changed to be of type text instead of boolean so that the edge-cases (such as Telemetr using an already registered internal ID) can be handled better. There might be issues with integrating the new app into the wapatools repository accounting for its structure.","title":"November 25, 2023"},{"location":"working/telegram/tg_data/#telegram-stats","text":"Telegram statistical data is obtained from the telemetr.io web service. Results are stored in the telegram_channels_stats table.","title":"Telegram Stats"},{"location":"working/telegram/tg_data/#available-data","text":"...","title":"Available data"},{"location":"working/telegram/tg_data/#primary-goal","text":"Build visualization(s) with available data. Make telegram collection process async","title":"Primary goal"},{"location":"working/telegram/tg_data/#telegram-messages","text":"Telegram messages are obtained by direct download via the official Telegram API, and then stored in the telegram_messages table in the data schema of the DB.","title":"Telegram Messages"},{"location":"working/telegram/tg_data/#primary-goal_1","text":"Perform various kinds of analyses, such as Deleted messages rate; Commonly used words; Share of reposts/forwards; NLP stuff; TBD","title":"Primary goal"},{"location":"working/telegram/tg_data/#secondary-goal","text":"Identification of important messages; (automatic) translation into English / Ukrainian","title":"Secondary goal"},{"location":"working/theory/v1/","text":"The core of the Theory section should be composed of a small body of original texts: The Copium Theory of Propaganda https://shoomow.info/?p=2069&preview=true (unfinished) Russian Corral In the Universal Informational Golfstream https://shoomow.info/?p=1555&lang=en Geopolitical Disease & The Ruskymir Variation https://shoomow.info/?p=748&lang=en Why the Russian Evidence For Biolabs in Ukraine is Hot Garbage https://shoomow.info/?p=411&lang=en Why Russian Referendums Cannot Be Taken Seriously https://shoomow.info/?p=2046&lang=en TODO: Verify Assumption of Continuity https://shoomow.info/?p=1773&lang=en All primary original texts should be located on the root level of the section (i.e. be children of Theory ). Additionally, Companion texts subsection is required, to include various texts by other authors that illustrate or illuminate various points made in the originals. Possible companion texts are (list is not exhaustive): Childhood Songs (Leonid Kaganov on March 31, 2022) https://shoomow.info/?p=97&lang=en Putin\u2019s Reich and Its Prospects (Dima Gubin, April 4, 2022) https://shoomow.info/?p=170&lang=en I Don\u2019t Want to Believe This (Dmitry Glukhovsky, April 9, 2022) https://shoomow.info/?p=274&lang=en Russian Guilt (Vladimir Pastukhov, April 14, 2022) https://shoomow.info/?p=366&lang=en From Bandera to \u201cAzov\u201d: Answers to the Questions About Ukrainian Nationalism (Konstantin Skorkin, April 17, 2022) https://shoomow.info/?p=564&lang=en Geopolitics Is the Dehumanization Of the World (Maksim Trudolyubov, April 19, 2022) https://shoomow.info/?p=664&lang=en Putin vs. Hitler: The Difference (Dmitry Chernyshev, April 21, 2022) https://shoomow.info/?p=810&lang=en Motherland or Truth? (Dima Gubin, April 18, 2022) https://shoomow.info/?p=682&lang=en What Made the Propaganda Possible (Andrey Pertsev, April 2022) https://shoomow.info/?p=375&lang=en Enter the Gloom, Find People In There: Why Do Russians Support the War? (Shura Burtin, April 24, 2022) https://shoomow.info/?p=924&lang=en Zigzags of Journalism: How a Small Smolensk Periodical Became the Bullhorne of the War Party With a Million+ Audience (\u201cMozhem obyasnit\u201d, April 11, 2022) https://shoomow.info/?p=670&lang=en The Cult of Victory (Alexey Roschin, April 24, 2022) https://shoomow.info/?p=1102&lang=en Solovyov At The Crossroads (Alexey Roschin, April 28, 2022) https://shoomow.info/?p=1201&lang=en The Fate of Germans: No Fame, No Heroes (Alexey Roschin, May 4, 2022) https://shoomow.info/?p=1373&lang=en Russian Hysteria (Vladimir Pastukhov, May 4, 2022) https://shoomow.info/?p=1342&lang=en What is Propaganda and How it Works (Dmitry Sidorov, May 3, 2022) https://shoomow.info/?p=1336&lang=en A Little About Sergey Mardan (Alexey Roschin) https://shoomow.info/?p=1798&lang=en The Kissinger Suggestion (Vladimir Pastukhov, May 26, 2022) https://shoomow.info/?p=1815&lang=en 10 \u043f\u0440\u0438\u0447\u0438\u043d, \u043f\u043e\u0447\u0435\u043c\u0443 \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u0432 \u043e\u0431\u0449\u0435\u0441\u0442\u0432\u0435 \u2013 \u043d\u0435 \u043a\u043b\u0435\u0442\u043a\u0430 \u0432 \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u043c\u0435 https://scinquisitor.livejournal.com/208371.html Finally, another subsection (name TBD) is required to hold texts written by patients that are also relevant to the Theory . Possible such texts are: What Russia Must Do With Ukraine (T. Sergeytsev, April 3, 2022) https://shoomow.info/?p=144&lang=en The Coming of Russia and the New World (P. Akopov, February 26, 2022) https://shoomow.info/?p=172&lang=en What Russia is Fighting For in Ukraine (V. Nikiforova, April 6, 2022) https://shoomow.info/?p=281&lang=en Putin About \u201cSpecial Operation\u201d (April 12, 2022) https://shoomow.info/?p=329&lang=en The Kind of Ukraine We Don\u2019t Need (T. Sergeytsev, April 10, 2021) https://shoomow.info/?p=251&lang=en Joint Press Conference of A. Lukashenko & V. Putin on April 12, 2022 [Transcript] https://shoomow.info/?p=788&lang=en Putin\u2019s Direct Speech With Commentary https://shoomow.info/?p=1558&preview=true Russian Propaganda Snapshot: March 1, 2022 https://shoomow.info/?p=2031&lang=en Russian Propaganda Snapshot: April 2, 2022 https://shoomow.info/?p=2038&lang=en Vladimir Ovchinsky On Death Penalty In USA https://shoomow.info/?p=2095&lang=en All texts must have versions in English and Russian, or English and Ukrainian, or all three languages.","title":"Theory v1"},{"location":"working/transfactory/anomaly_cleanup_activity/","text":"September 29-30, 2023 Over the course of 29 and 30 of September, 2023, the 1st run at anomalies cleanup was undertaken, working over the anomalies_transcription field, with threshold of 300 seconds (i.e. anomalies with lesser duration were ignored). The process overall was successful, although a few bits of data got lost due to errors in the alogythm. In total, 1024 transcripts known to contain anomalies were analyzed. Of these: 529 were left intact, i.e. no lines were dropped due to threshold mismatch; 495 contained anomalies longer than the established threshold \\~ 207 868 junk lines were dropped from the data.transcribed_content table This corresponds to \\~ 383 hours worth of transcribed data For context, total number of transcribed hours is at 8472 hours, and number of records in the transcribed_content table after cleanup is slightly over 9 million , as of Sep 30, 2023.","title":"Transfactory Anomaly Cleanup Activity"},{"location":"working/transfactory/anomaly_cleanup_activity/#september-29-30-2023","text":"Over the course of 29 and 30 of September, 2023, the 1st run at anomalies cleanup was undertaken, working over the anomalies_transcription field, with threshold of 300 seconds (i.e. anomalies with lesser duration were ignored). The process overall was successful, although a few bits of data got lost due to errors in the alogythm. In total, 1024 transcripts known to contain anomalies were analyzed. Of these: 529 were left intact, i.e. no lines were dropped due to threshold mismatch; 495 contained anomalies longer than the established threshold \\~ 207 868 junk lines were dropped from the data.transcribed_content table This corresponds to \\~ 383 hours worth of transcribed data For context, total number of transcribed hours is at 8472 hours, and number of records in the transcribed_content table after cleanup is slightly over 9 million , as of Sep 30, 2023.","title":"September 29-30, 2023"},{"location":"working/transfactory/anomaly_detection_summary/","text":"Driven by the DVE-A-95, an update to the transfactory was introduced to address the issue on at least some of the relevant levels. Silence Trimming First measure taken was addition of silence trimming process to the transsuply script, i.e. removal of leading and trailing silence. Trimming is done with ffmpeg , and extends script's runtime significantly (at least, 2x); at that silence trimming is actually required in minority of cases - but as far as I can tell, ffmpeg cannot simply detect if trimming is needed, but can only process the file using an internal algorythm, so the trimming happens in any case (producing a new copy of the file), but if the difference in duration between the original and trimmed versions is less than 1 second, original version is uploaded instead of the trimmed one. Trimmed version is then removed in any case, while the original version is moved to cold storage as per the usual flow. Anomaly Detection Algorythm Anomaly detection is done over the array produced by the shape_transcribed_lines_object function, which is a part of the transprocessor flow - this flow is updated with anomaly detection functionality, so all the newly processed transcripts are analyzed right away. The algorythm consists of 2 parts, the 1st of which is covered by detect_faulty_patches function, and the 2nd - by group_patches function; umbrella function detect_anomalies joins them to produce the final anomaly status. All these can be found in the newly added transfactory_common base module, where some other functions were also moved, with corresponding code optimizations. The algorythm is applied to the standard transprocessor flow, as well as to the recover_lines flow. Anomalies data object The algo produces a JSON object that must have at least 1 key, anomalies , which is a boolean - this would be the contents of either anomalies_transcription or anomalies_translation columns. If it's False , no other keys are expected, so the object is {\"anomalies\": False} - this is saved to indicate that the corresponding text array was, in fact, analyzed, and not forgotten. If it's True , 2 other keys are expected - raw_faulty_patches , which is a list of dicts produced by detect_faulty_patches , and operational_timeframes , which is a list of tuples, each consisting of 2 floats that indicate start and end time relative to the beginning of the corresponding audiofile, produced by group_patches . Anomaly Stats as of Sep 29, 2023 Records in prabyss table # Total 7035 Analyzed transcriptions 4839 Analyzed translations 2013 With anomalies in transcription 1024 With anomalies in translation 887 With anomalies in both 542 Transcription: total anomaly duration \\<= 1 min 204/1024 Transcription: total anomaly duration btw 1 min & 5 min 262/1024 Transcription: total anomaly duration btw 5 min & 10 min 143/1024 Transcription: total anomaly duration btw 10 min & 30 min 154/1024 Transcription: total anomaly duration btw 30 min & 60 min 120/1024 Transcription: total anomaly duration btw 60 min & 120 min 110/1024 Transcription: total anomaly duration > 120 min 31/1024 Translation: total anomaly duration \\<= 1 min 273/887 Translation: total anomaly duration btw 1 min & 5 min 260/887 Translation: total anomaly duration btw 5 min & 10 min 133/887 Translation: total anomaly duration btw 10 min & 30 min 118/887 Translation: total anomaly duration btw 30 min & 60 min 60/887 Translation: total anomaly duration btw 60 min & 120 min 31/887 Translation: total anomaly duration > 120 min 12/887","title":"Transfactory Anomaly Detection Summary"},{"location":"working/transfactory/anomaly_detection_summary/#silence-trimming","text":"First measure taken was addition of silence trimming process to the transsuply script, i.e. removal of leading and trailing silence. Trimming is done with ffmpeg , and extends script's runtime significantly (at least, 2x); at that silence trimming is actually required in minority of cases - but as far as I can tell, ffmpeg cannot simply detect if trimming is needed, but can only process the file using an internal algorythm, so the trimming happens in any case (producing a new copy of the file), but if the difference in duration between the original and trimmed versions is less than 1 second, original version is uploaded instead of the trimmed one. Trimmed version is then removed in any case, while the original version is moved to cold storage as per the usual flow.","title":"Silence Trimming"},{"location":"working/transfactory/anomaly_detection_summary/#anomaly-detection-algorythm","text":"Anomaly detection is done over the array produced by the shape_transcribed_lines_object function, which is a part of the transprocessor flow - this flow is updated with anomaly detection functionality, so all the newly processed transcripts are analyzed right away. The algorythm consists of 2 parts, the 1st of which is covered by detect_faulty_patches function, and the 2nd - by group_patches function; umbrella function detect_anomalies joins them to produce the final anomaly status. All these can be found in the newly added transfactory_common base module, where some other functions were also moved, with corresponding code optimizations. The algorythm is applied to the standard transprocessor flow, as well as to the recover_lines flow.","title":"Anomaly Detection Algorythm"},{"location":"working/transfactory/anomaly_detection_summary/#anomalies-data-object","text":"The algo produces a JSON object that must have at least 1 key, anomalies , which is a boolean - this would be the contents of either anomalies_transcription or anomalies_translation columns. If it's False , no other keys are expected, so the object is {\"anomalies\": False} - this is saved to indicate that the corresponding text array was, in fact, analyzed, and not forgotten. If it's True , 2 other keys are expected - raw_faulty_patches , which is a list of dicts produced by detect_faulty_patches , and operational_timeframes , which is a list of tuples, each consisting of 2 floats that indicate start and end time relative to the beginning of the corresponding audiofile, produced by group_patches .","title":"Anomalies data object"},{"location":"working/transfactory/anomaly_detection_summary/#anomaly-stats","text":"as of Sep 29, 2023 Records in prabyss table # Total 7035 Analyzed transcriptions 4839 Analyzed translations 2013 With anomalies in transcription 1024 With anomalies in translation 887 With anomalies in both 542 Transcription: total anomaly duration \\<= 1 min 204/1024 Transcription: total anomaly duration btw 1 min & 5 min 262/1024 Transcription: total anomaly duration btw 5 min & 10 min 143/1024 Transcription: total anomaly duration btw 10 min & 30 min 154/1024 Transcription: total anomaly duration btw 30 min & 60 min 120/1024 Transcription: total anomaly duration btw 60 min & 120 min 110/1024 Transcription: total anomaly duration > 120 min 31/1024 Translation: total anomaly duration \\<= 1 min 273/887 Translation: total anomaly duration btw 1 min & 5 min 260/887 Translation: total anomaly duration btw 5 min & 10 min 133/887 Translation: total anomaly duration btw 10 min & 30 min 118/887 Translation: total anomaly duration btw 30 min & 60 min 60/887 Translation: total anomaly duration btw 60 min & 120 min 31/887 Translation: total anomaly duration > 120 min 12/887","title":"Anomaly Stats"},{"location":"working/transfactory/anomaly_study_summary/","text":"Context Transfactory is the app to process audio files into text using OpenAI's Whisper. Occasionally, the produced result ( .srt files generated by Whisper and, subsequently, text array saved to the database) would contain anomalies , which (in most cases) are patches of some repeat line that go on consecutively for some time. Study A study was undertaken, to figure out possible causes of these anomalies, and ways to handle them. Study includes 15 specific usecases described in more detail here: DVE-A-94. Each usecase is a media episode where either trascription, or translation, or both contained at least 1 anomaly (identified manually). Some of the files from the test set did not contain anomalies. Repeat lines In half of the studied cases, repeat line was something that actually was (or, at least, may have been) said; Some cases that may not be anomalies are template phrases, such as \u0422\u041e\u0420\u0416\u0415\u0421\u0422\u0412\u0415\u041d\u041d\u0410\u042f \u041c\u0423\u0417\u042b\u041a\u0410 , \u0413\u041e\u0412\u041e\u0420\u0418\u0422 \u041d\u0410 \u0418\u041d\u041e\u0421\u0422\u0420\u0410\u041d\u041d\u041e\u041c \u042f\u0417\u042b\u041a\u0415 , \u041c\u0423\u0417\u042b\u041a\u0410\u041b\u042c\u041d\u0410\u042f \u0417\u0410\u0421\u0422\u0410\u0412\u041a\u0410 etc. Quite a lot of cases were lines that never occurred in the audio in question, or never could have occurred, for example, TASTY DIALOGUE WITH ELENA BAZHENOVA , \u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0421\u0438\u043d\u0435\u0446\u043a\u0430\u044f \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 , \u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 , \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 etc. My guess is that things containing these lines were used as source material for Whisper ( https://www.youtube.com/@bazhenova/ ), and, possibly, contained leading silence or some other artifacts. Possible causes Some possible causes are: prolonged silence, especially leading silence; prolonged non-verbal stretches (music, background noise); a system event occurring outside of the transfactory process but one that causes a video memory consumption spike; TBD. Resolution and Remaining Issues To detect anomalies, a Python function was developed, and then applied to the available usecases. Of the 15 usecases 7 can be considered fully resolved after this, i.e. analysis of both translation and transcription produced correct and expected output. The rest have certain issues that can be grouped like so: There is actually a single long patch, but the output is a list of consecutive (and overlapping) patches that cover the same (correct) range. Text items were tested to be identical, so the reason for this split remains unclear. Relevant usecases: #2, #8, #13 With usecase #13, the cause of the split is different case of the strings which are otherwise identical Possible solutions: Post-hoc merging of produced patches into one patch; TBD Instead of 1 line being repeated, it's a block of 4 lines. Relevant usecases: #5 Possible solutions: In addition to the repeat lines, there is another pattern related to anomalies: while normally transcribed lines have highly variable duration (difference between start and end time) that rarely repeats from one line to another, the lines in the faulty patches have relatively uniform duration of 28-30 seconds. The algorythm correctly produces several patches (because repeat line is different), but some of them are so close to each other that it would make sense to merge them. Relevant usecases: #6, #9, #12 Possible solutions: Post-hoc merging of produced patches into one patch, or several non-consecutive patches; Anomaly is a just 1 random line in the middle of otherwise proper array Relevant usecases: #14 Possible solutions: WTFK Next steps What happens once an anomaly is identified in a given factory job? Suggested flow: Drop faulty lines from the database Locate original audio file Cut out fragment(s) corresponding to anomaly's parameters Create transfactory job for each such fragment, accounting for initial job parameters. When the re-work job is complete, analyze the result try to detect anomaly, if found, compare to the original anomaly's content - if the same happens again, that should mean something if no new anomaly occurs, i.e. produced lines are valid, they need to be inserted into the database. This is also a challenge, since a different number of lines is to be expected - that is, int_sequence will be a mismatch, and would need to be reconciled. Alternatively, the episode could be re-processed in full. Upside: no issues with int_sequence . Downside: resources are wasted on processing what was already processed successfully (and might still fail the 2nd time around, btw, there's no guarantee against that); we discard totally proper lines that may have been transcribed better the 1st time.","title":"Transfactory Anomaly Study Summary"},{"location":"working/transfactory/anomaly_study_summary/#context","text":"Transfactory is the app to process audio files into text using OpenAI's Whisper. Occasionally, the produced result ( .srt files generated by Whisper and, subsequently, text array saved to the database) would contain anomalies , which (in most cases) are patches of some repeat line that go on consecutively for some time.","title":"Context"},{"location":"working/transfactory/anomaly_study_summary/#study","text":"A study was undertaken, to figure out possible causes of these anomalies, and ways to handle them. Study includes 15 specific usecases described in more detail here: DVE-A-94. Each usecase is a media episode where either trascription, or translation, or both contained at least 1 anomaly (identified manually). Some of the files from the test set did not contain anomalies.","title":"Study"},{"location":"working/transfactory/anomaly_study_summary/#repeat-lines","text":"In half of the studied cases, repeat line was something that actually was (or, at least, may have been) said; Some cases that may not be anomalies are template phrases, such as \u0422\u041e\u0420\u0416\u0415\u0421\u0422\u0412\u0415\u041d\u041d\u0410\u042f \u041c\u0423\u0417\u042b\u041a\u0410 , \u0413\u041e\u0412\u041e\u0420\u0418\u0422 \u041d\u0410 \u0418\u041d\u041e\u0421\u0422\u0420\u0410\u041d\u041d\u041e\u041c \u042f\u0417\u042b\u041a\u0415 , \u041c\u0423\u0417\u042b\u041a\u0410\u041b\u042c\u041d\u0410\u042f \u0417\u0410\u0421\u0422\u0410\u0412\u041a\u0410 etc. Quite a lot of cases were lines that never occurred in the audio in question, or never could have occurred, for example, TASTY DIALOGUE WITH ELENA BAZHENOVA , \u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0421\u0438\u043d\u0435\u0446\u043a\u0430\u044f \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 , \u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 , \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 etc. My guess is that things containing these lines were used as source material for Whisper ( https://www.youtube.com/@bazhenova/ ), and, possibly, contained leading silence or some other artifacts.","title":"Repeat lines"},{"location":"working/transfactory/anomaly_study_summary/#possible-causes","text":"Some possible causes are: prolonged silence, especially leading silence; prolonged non-verbal stretches (music, background noise); a system event occurring outside of the transfactory process but one that causes a video memory consumption spike; TBD.","title":"Possible causes"},{"location":"working/transfactory/anomaly_study_summary/#resolution-and-remaining-issues","text":"To detect anomalies, a Python function was developed, and then applied to the available usecases. Of the 15 usecases 7 can be considered fully resolved after this, i.e. analysis of both translation and transcription produced correct and expected output. The rest have certain issues that can be grouped like so: There is actually a single long patch, but the output is a list of consecutive (and overlapping) patches that cover the same (correct) range. Text items were tested to be identical, so the reason for this split remains unclear. Relevant usecases: #2, #8, #13 With usecase #13, the cause of the split is different case of the strings which are otherwise identical Possible solutions: Post-hoc merging of produced patches into one patch; TBD Instead of 1 line being repeated, it's a block of 4 lines. Relevant usecases: #5 Possible solutions: In addition to the repeat lines, there is another pattern related to anomalies: while normally transcribed lines have highly variable duration (difference between start and end time) that rarely repeats from one line to another, the lines in the faulty patches have relatively uniform duration of 28-30 seconds. The algorythm correctly produces several patches (because repeat line is different), but some of them are so close to each other that it would make sense to merge them. Relevant usecases: #6, #9, #12 Possible solutions: Post-hoc merging of produced patches into one patch, or several non-consecutive patches; Anomaly is a just 1 random line in the middle of otherwise proper array Relevant usecases: #14 Possible solutions: WTFK","title":"Resolution and Remaining Issues"},{"location":"working/transfactory/anomaly_study_summary/#next-steps","text":"What happens once an anomaly is identified in a given factory job? Suggested flow: Drop faulty lines from the database Locate original audio file Cut out fragment(s) corresponding to anomaly's parameters Create transfactory job for each such fragment, accounting for initial job parameters. When the re-work job is complete, analyze the result try to detect anomaly, if found, compare to the original anomaly's content - if the same happens again, that should mean something if no new anomaly occurs, i.e. produced lines are valid, they need to be inserted into the database. This is also a challenge, since a different number of lines is to be expected - that is, int_sequence will be a mismatch, and would need to be reconciled. Alternatively, the episode could be re-processed in full. Upside: no issues with int_sequence . Downside: resources are wasted on processing what was already processed successfully (and might still fail the 2nd time around, btw, there's no guarantee against that); we discard totally proper lines that may have been transcribed better the 1st time.","title":"Next steps"},{"location":"working/transfactory/anomaly_usecases/","text":"This document lists some specific usecase when the transfactory's main process goes wrong, producing anomalies . As a rule, such anomalies take the form of some line that gets repeated over and over again. The cause of such incidents is unclear. Usecase 1 Parameter Value prabyss path amerikantsy-podtyorlis-gensekom-oon-efir-ot-20072023-2651641-na.mp3 prabyss_id 7070 transcript_id 6899 transcribed lines 1587 failed transcribed lines 1-295 translated lines 673 failed transcribed lines 1-673 Status Fully resolved With transcription: problematic line is \u0421\u0443\u0431\u0442\u0438\u0442\u0440\u044b \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u0410.\u0421\u0438\u043d\u0435\u0446\u043a\u0430\u044f \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 ; it appears under int_sequence s 1, 2-295 out of total 1587. With translation: problematic line is TASTY DIALOGUE WITH ELENA BAZHENOVA ; it appears under 1 and goes to the end. Detection status With transcription: Faulty patch identified correctly, but starting with #3 due to theshold settings. With translation: Faulty patch of the entire length of file identified correctly Usecase 2 Parameter Value prabyss path amerikantsev-ustraivaet-zatyagivanie-konflikta-efir-ot-26052023-2621753-na.mp3 prabyss_id 7055 transcript_id 6892 transcribed lines 243 failed transcribed lines 1-243 translated lines 2197 failed transcribed lines None Status Partially resolved \\~ TODO: Fix unsolicited patch split With transcription: problematic line is \u0420\u0410\u0417\u0413\u041e\u0412\u041e\u0420 \u041f\u041e-\u041d\u0415\u041c\u0415\u0426\u041a\u0418 occasionally followed by \u0410\u041f\u041b\u041e\u0414\u0418\u0421\u041c\u0415\u041d\u0422\u042b ; it appears at the beginning at goes till the end (1-243). Detection status With transcription: Faulty patch identified overall correcly, but split into 5 different patches for unclear reason. Some of the patches are overlapping. With translation: Usecase 3 Parameter Value prabyss path Rossiya_Ukraina--Tochka-nevozvrata-i-Kaska-s-pechalnym-kontsomDmDzhangirov-v-pryamom-efire-rqbW-bH4FgA-20220203.mp3 prabyss_id 5145 transcript_id transcribed lines 294 failed transcribed lines 86-294 translated lines 2381 failed transcribed lines None Status Fully resolved With transcription: problematic line is \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, , which start occurring after line 85 \u0447\u0442\u043e\u0431 \u0435\u0435 \u0442\u0443\u0434\u0430 \u0432\u043d\u0435\u0441\u043b\u0438, \u0438 \u0442\u043e, \u0447\u0442\u043e, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, , presumable, also failed one, and goes to the end. Detection status With transcription: Faulty patch identified correctly With translation: Lack of anomaly identified correctly Usecase 4 Parameter Value prabyss path gde-prokhodit-peredovaya-efir-ot-04062023-2626712-na.mp3 prabyss_id transcript_id transcribed lines 3140 failed transcribed lines 2021-3140 translated lines 3415 failed transcribed lines 1347-1429 Status Fully resolved With transcription: \u043a\u0430\u043a\u0438\u0435 \u0442\u0435\u0430\u0442\u0440\u044b \u0431\u044b\u043b\u0438, With translation: Day Z , Detection status With transcription: Known faulty patch was identified correcly. Another was identified between #358 and #413. With translation: First faulty patch between #1347 and #1429 identified correcly. Second patch was identified between #2819 and #2919. Usecase 5 Parameter Value prabyss path briks-trevozhit-zapadnyy-mir-efir-ot-28062023-2640212-na.mp3 prabyss_id transcript_id transcribed lines 2163 failed transcribed lines None translated lines 2398 failed transcribed lines 776-895 Status Partially resolved \\: TODO: special usecase with the repeat block instead of repeat line With translation: We will continue after the news over 4 lines. Detection status With transcription: A previously unidentified batch was identified between #12 and #24 With translation: A small, previously unidentified faulty patch between #58 and #77, was identified. However, the main case was not identified, because it's not a single line being repeated, but a set of 4 lines. Usecase 6 Parameter Value prabyss path bronepoezd-tovarishcha-kim-chen-yna-efir-ot-17092023-2683340-na.mp3 prabyss_id transcript_id transcribed lines 1943 failed transcribed lines 1-321 translated lines 2296 failed transcribed lines None Status Partially resolved ; TODO: merge consecutive batches, including with small intervals in between them With transcription: \u0412\u043e\u043b\u0433\u043e\u043b\u0430\u0439\u0444\u0437\u0435\u0442 , \u041c\u0430\u043a\u0441\u0438\u043c \u041b\u0430\u0432\u0440\u0443\u0445\u0438\u043d and a few others Detection status With transcription: Faulty patch identified almost correctly: range is fine but some lines in between are lost. With translation: Previously unidentified patch identified between #1264 and #1274 Usecase 7 Parameter Value prabyss path burning-man-pochemu-tysyachi-lyudey-zastryali-v-pustyne-efir-ot-04092023-2676965-na.mp3 prabyss_id transcript_id transcribed lines 1112 failed transcribed lines 972-1112 translated lines 3056 failed transcribed lines None Status Fully resolved With transcription: \u041f\u0415\u0421\u041d\u042f \u041d\u0410 \u041d\u0415\u041c\u0415\u0426\u041a\u041e\u041c \u042f\u0417\u042b\u041a\u0415 Detection status With transcription: faulty patch identified correctly With translation: lack of anomaly identified correctly Usecase 8 Parameter Value prabyss path chelovek-z-ya-ne-uznal-stranu_14129.mp3 prabyss_id transcript_id transcribed lines 489 failed transcribed lines 1-60 translated lines 1296 failed transcribed lines 816-1296 Status Partially resolved \\~ TODO: Fix unsolicited patch split With transcription: \u041c\u0430\u0441\u0442\u0435\u0440-\u043a\u043b\u0430\u0441\u0441, With translation: I Detection status With transcription: Faulty patch identified correcly With translation: Faulty patch identified overall correcly, but split into 6 different batches for unclear reason. Some of the batches are overlapping. Usecase 9 Parameter Value prabyss path efir-ot-03042023-2591188-na.mp3 prabyss_id transcript_id transcribed lines 4303 failed transcribed lines 1068-1796, 3741-3768 (...) translated lines 4994 failed transcribed lines 497-543?, 4854-4994 Status Partially resolved ; TODO: merge consecutive batches, including with small intervals in between them With transcription: \u042f \u0434\u0443\u043c\u0430\u044e, \u0447\u0442\u043e \u044d\u0442\u043e \u043e\u0447\u0435\u043d\u044c \u0432\u0430\u0436\u043d\u043e. , \u041f\u043e\u0434\u043f\u0438\u0441\u044b\u0432\u0430\u0439\u0442\u0435\u0441\u044c \u043d\u0430 \u0442\u0435\u043b\u0435\u0433\u0440\u0430\u043c-\u043a\u0430\u043d\u0430\u043b \u0410\u043d\u043d\u044b. , 8 \u0447\u0430\u0441\u043e\u0432 8 \u043c\u0438\u043d\u0443\u0442. , 9arenko. ; ... With translation: To be continued... ?, Do not hurry, when you can not hurry. , He got it. Detection status With transcription: 7 fault patches were identified: 1068-1095, 1094-1121, 1120-1179, 1778-1793, 2240-2276, 2343-2364, 3739-3768, 3785-3803, all seem to be correct With translation: 7 faulty patches were identified: 476-492, 495-543, 2048-2097, 2105-2115, 2575-2585, 3164-3207, 4852-4869, 4871-4993, all seem to be correct. Usecase 10 Parameter Value prabyss path novaya-os-zla-i-skuchnyy-evroparlament-efir-ot-14092023-2682103-na.mp3 prabyss_id transcript_id transcribed lines 279 failed transcribed lines 4-14, 106-279 translated lines 1408 failed transcribed lines 1-5 Status Fully resolved With transcription: \u0412\u0440\u0430\u0447\u0435\u0431\u043d\u0430\u044f \u043f\u043e\u043c\u043e\u0449\u044c \u0440\u0430\u043d\u0435\u043d\u044b\u043c \u0431\u043e\u0439\u0446\u0430\u043c \u0441\u0438\u043b\u0430\u043c\u0438 \u043c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u0438\u0445 \u0432\u0437\u0432\u043e\u0434\u043e\u0432. , \u041a\u041e\u041d\u0415\u0426 With translation: TASTY DIALOGUE WITH ELENA BAZHENOVA Detection status With transcription: Both known faulty patches identified correctly. With translation: Known faulty patch not identified due to theshold settings; another batch identified between #258 and #276 Usecase 11 Parameter Value prabyss path esli-bpla-zapushcheny-s-territorii-rossii-est-kogo-polovit-efir-ot-300523-2623654-na.mp3 prabyss_id transcript_id transcribed lines 856 failed transcribed lines 1-131, 215-856 translated lines 2907 failed transcribed lines 1-8 Status Fully resolved With transcription: \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u041a\u0443\u043b\u0430\u043a\u043e\u0432\u0430 , \u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0421\u0438\u043d\u0435\u0446\u043a\u0430\u044f \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 , \u0416\u0435\u043b\u0435\u0437\u043d\u0430\u044f \u043b\u043e\u0433\u0438\u043a\u0430 With translation: TASTY DIALOGUE WITH ELENA BAZHENOVA Detection status With transcription: First fault patch identified correctly with start at #4, with first 3 lines being also a repeat, but not identified due to threshold settings. Second fault patch identified correcly. With translation: Faulty patch not identified due to threshold settings. Usecase 12 Parameter Value prabyss path esli-pashinyan-otdast-karabakh-to-otvetstvennost-budet-na-bryussele-efir-ot-25052023-2621029-na_en.mp3 prabyss_id transcript_id transcribed lines 1620 failed transcribed lines 1-168, 1071-1165 translated lines 1718 failed transcribed lines 1-28, 780-826 Status Partially resolved ; TODO: merge consecutive batches, including with small intervals in between them With transcription: \u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0421\u0438\u043d\u0435\u0446\u043a\u0430\u044f \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 , \u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 , \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 , \u0417\u0434\u0440\u0430\u0432\u0441\u0442\u0432\u0443\u0439\u0442\u0435, \u0442\u043e\u0432\u0430\u0440\u0438\u0449\u0438 \u0433\u0432\u0430\u0440\u0434\u0435\u0439\u0446\u044b! , \u0416\u0435\u043b\u0435\u0437\u043d\u0430\u044f \u041b\u043e\u0433\u0438\u043a\u0430 , \u0422\u041e\u0420\u0416\u0415\u0421\u0422\u0412\u0415\u041d\u041d\u0410\u042f \u041c\u0423\u0417\u042b\u041a\u0410 , \u0413\u041e\u0412\u041e\u0420\u0418\u0422 \u041d\u0410 \u0418\u041d\u041e\u0421\u0422\u0420\u0410\u041d\u041d\u041e\u041c \u042f\u0417\u042b\u041a\u0415 , \u041c\u0423\u0417\u042b\u041a\u0410\u041b\u042c\u041d\u0410\u042f \u0417\u0410\u0421\u0422\u0410\u0412\u041a\u0410 With translation: TASTY DIALOGUE WITH ELENA BAZHENOVA , We are proud of you! Detection status With transcription: First 2 consecutive faulty patches (up to #163) identified almost correctly: there is a small gap in between, and a few lines are not counted at the end of the 2nd patch due to threshold being set to 10. Second 2 consecutive faulty patches (between #1092 and #1165) identified almost correctly: a few lines at the beggining of the 1st are not counted due to the threshold settings. With translation: Both known faulty patches identified correctly; in addition, another patch identifed between #778 and #826 Usecase 13 Parameter Value prabyss path genotsid-v-otnoshenii-sovetskogo-naroda-efir-ot-23032023-2585101-na.mp3 prabyss_id transcript_id transcribed lines 528 failed transcribed lines 1-528 translated lines 1896 failed transcribed lines None Status Partially resolved ; TODO: take case into account? With transcription: \u0416\u0415\u041b\u0415\u0417\u041d\u0410\u042f \u041b\u041e\u0413\u0418\u041a\u0410 Detection status With transcription: Two consecutive faulty patches were identified, first starting with #3 due to threshold settings, second ending at #527. Batches split is due to different case of the repeat line. With translation: Lack of anomaly identified correcly. Usecase 14 Parameter Value prabyss path triumfalnaya-pobeda-edinoy-rossii-efir-ot-11092023-2680088-na.mp3 prabyss_id transcript_id transcribed lines 1316 failed transcribed lines 1-134, 320 (\u043d\u0435\u0442) translated lines 4397 failed transcribed lines None Status Partially resolved ; TODO: do something with the solitary line With transcription: \u041f\u041e\u0414\u041f\u0418\u0421\u042b\u0412\u0410\u042e\u0429\u0418\u0415\u0421\u042f \u0422\u0415\u041b\u0415\u0424\u041e\u041d\u041d\u042b\u0415 \u0421\u0418\u0413\u041d\u0410\u041b\u0418\u0417\u0410\u0426\u0418\u0418 , \u041f\u041e\u0414\u041f\u0418\u0421\u042b\u0412\u0410\u042e\u0429\u0418\u0415 \u0421\u0418\u0413\u041d\u0410\u041b\u0418\u0417\u0410\u0426\u0418\u0418 Detection status With transcription: 2 consecutive faulty patches identified correctly (up to #134). Single faulty item (#320) not identified With translation: Lack of anomaly identified correctly Usecase 15 Parameter Value prabyss path vsyo-vertitsya-vokrug-rossii-efir-ot-11092023-2679995-na.mp3 prabyss_id transcript_id transcribed lines 2023 failed transcribed lines 1-99 translated lines 717 failed transcribed lines 1-717 Status Fully resolved With transcription: \u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0421\u0438\u043d\u0435\u0446\u043a\u0430\u044f \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 , \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u041a\u0443\u043b\u0430\u043a\u043e\u0432\u0430 , \u0410.\u041a\u0443\u043b\u0430\u043a\u043e\u0432\u0430 , \u0411\u043e\u043b\u0435\u0435! , \u0411\u044b\u0441\u0442\u0440\u0435\u0435! With translation: TASTY DIALOGUE WITH ELENA BAZHENOVA Detection status With transcription: 4 faulty patches identified: 2-26 + 3-99; 581-593, 1052-1113, all seem to be correct With translation: Faulty patch identified correctly","title":"Transfactory Anomaly Usecases"},{"location":"working/transfactory/anomaly_usecases/#usecase-1","text":"Parameter Value prabyss path amerikantsy-podtyorlis-gensekom-oon-efir-ot-20072023-2651641-na.mp3 prabyss_id 7070 transcript_id 6899 transcribed lines 1587 failed transcribed lines 1-295 translated lines 673 failed transcribed lines 1-673 Status Fully resolved With transcription: problematic line is \u0421\u0443\u0431\u0442\u0438\u0442\u0440\u044b \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u0410.\u0421\u0438\u043d\u0435\u0446\u043a\u0430\u044f \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 ; it appears under int_sequence s 1, 2-295 out of total 1587. With translation: problematic line is TASTY DIALOGUE WITH ELENA BAZHENOVA ; it appears under 1 and goes to the end.","title":"Usecase 1"},{"location":"working/transfactory/anomaly_usecases/#detection-status","text":"With transcription: Faulty patch identified correctly, but starting with #3 due to theshold settings. With translation: Faulty patch of the entire length of file identified correctly","title":"Detection status"},{"location":"working/transfactory/anomaly_usecases/#usecase-2","text":"Parameter Value prabyss path amerikantsev-ustraivaet-zatyagivanie-konflikta-efir-ot-26052023-2621753-na.mp3 prabyss_id 7055 transcript_id 6892 transcribed lines 243 failed transcribed lines 1-243 translated lines 2197 failed transcribed lines None Status Partially resolved \\~ TODO: Fix unsolicited patch split With transcription: problematic line is \u0420\u0410\u0417\u0413\u041e\u0412\u041e\u0420 \u041f\u041e-\u041d\u0415\u041c\u0415\u0426\u041a\u0418 occasionally followed by \u0410\u041f\u041b\u041e\u0414\u0418\u0421\u041c\u0415\u041d\u0422\u042b ; it appears at the beginning at goes till the end (1-243).","title":"Usecase 2"},{"location":"working/transfactory/anomaly_usecases/#detection-status_1","text":"With transcription: Faulty patch identified overall correcly, but split into 5 different patches for unclear reason. Some of the patches are overlapping. With translation:","title":"Detection status"},{"location":"working/transfactory/anomaly_usecases/#usecase-3","text":"Parameter Value prabyss path Rossiya_Ukraina--Tochka-nevozvrata-i-Kaska-s-pechalnym-kontsomDmDzhangirov-v-pryamom-efire-rqbW-bH4FgA-20220203.mp3 prabyss_id 5145 transcript_id transcribed lines 294 failed transcribed lines 86-294 translated lines 2381 failed transcribed lines None Status Fully resolved With transcription: problematic line is \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, , which start occurring after line 85 \u0447\u0442\u043e\u0431 \u0435\u0435 \u0442\u0443\u0434\u0430 \u0432\u043d\u0435\u0441\u043b\u0438, \u0438 \u0442\u043e, \u0447\u0442\u043e, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, \u0434\u0430, , presumable, also failed one, and goes to the end.","title":"Usecase 3"},{"location":"working/transfactory/anomaly_usecases/#detection-status_2","text":"With transcription: Faulty patch identified correctly With translation: Lack of anomaly identified correctly","title":"Detection status"},{"location":"working/transfactory/anomaly_usecases/#usecase-4","text":"Parameter Value prabyss path gde-prokhodit-peredovaya-efir-ot-04062023-2626712-na.mp3 prabyss_id transcript_id transcribed lines 3140 failed transcribed lines 2021-3140 translated lines 3415 failed transcribed lines 1347-1429 Status Fully resolved With transcription: \u043a\u0430\u043a\u0438\u0435 \u0442\u0435\u0430\u0442\u0440\u044b \u0431\u044b\u043b\u0438, With translation: Day Z ,","title":"Usecase 4"},{"location":"working/transfactory/anomaly_usecases/#detection-status_3","text":"With transcription: Known faulty patch was identified correcly. Another was identified between #358 and #413. With translation: First faulty patch between #1347 and #1429 identified correcly. Second patch was identified between #2819 and #2919.","title":"Detection status"},{"location":"working/transfactory/anomaly_usecases/#usecase-5","text":"Parameter Value prabyss path briks-trevozhit-zapadnyy-mir-efir-ot-28062023-2640212-na.mp3 prabyss_id transcript_id transcribed lines 2163 failed transcribed lines None translated lines 2398 failed transcribed lines 776-895 Status Partially resolved \\: TODO: special usecase with the repeat block instead of repeat line With translation: We will continue after the news over 4 lines.","title":"Usecase 5"},{"location":"working/transfactory/anomaly_usecases/#detection-status_4","text":"With transcription: A previously unidentified batch was identified between #12 and #24 With translation: A small, previously unidentified faulty patch between #58 and #77, was identified. However, the main case was not identified, because it's not a single line being repeated, but a set of 4 lines.","title":"Detection status"},{"location":"working/transfactory/anomaly_usecases/#usecase-6","text":"Parameter Value prabyss path bronepoezd-tovarishcha-kim-chen-yna-efir-ot-17092023-2683340-na.mp3 prabyss_id transcript_id transcribed lines 1943 failed transcribed lines 1-321 translated lines 2296 failed transcribed lines None Status Partially resolved ; TODO: merge consecutive batches, including with small intervals in between them With transcription: \u0412\u043e\u043b\u0433\u043e\u043b\u0430\u0439\u0444\u0437\u0435\u0442 , \u041c\u0430\u043a\u0441\u0438\u043c \u041b\u0430\u0432\u0440\u0443\u0445\u0438\u043d and a few others","title":"Usecase 6"},{"location":"working/transfactory/anomaly_usecases/#detection-status_5","text":"With transcription: Faulty patch identified almost correctly: range is fine but some lines in between are lost. With translation: Previously unidentified patch identified between #1264 and #1274","title":"Detection status"},{"location":"working/transfactory/anomaly_usecases/#usecase-7","text":"Parameter Value prabyss path burning-man-pochemu-tysyachi-lyudey-zastryali-v-pustyne-efir-ot-04092023-2676965-na.mp3 prabyss_id transcript_id transcribed lines 1112 failed transcribed lines 972-1112 translated lines 3056 failed transcribed lines None Status Fully resolved With transcription: \u041f\u0415\u0421\u041d\u042f \u041d\u0410 \u041d\u0415\u041c\u0415\u0426\u041a\u041e\u041c \u042f\u0417\u042b\u041a\u0415","title":"Usecase 7"},{"location":"working/transfactory/anomaly_usecases/#detection-status_6","text":"With transcription: faulty patch identified correctly With translation: lack of anomaly identified correctly","title":"Detection status"},{"location":"working/transfactory/anomaly_usecases/#usecase-8","text":"Parameter Value prabyss path chelovek-z-ya-ne-uznal-stranu_14129.mp3 prabyss_id transcript_id transcribed lines 489 failed transcribed lines 1-60 translated lines 1296 failed transcribed lines 816-1296 Status Partially resolved \\~ TODO: Fix unsolicited patch split With transcription: \u041c\u0430\u0441\u0442\u0435\u0440-\u043a\u043b\u0430\u0441\u0441, With translation: I","title":"Usecase 8"},{"location":"working/transfactory/anomaly_usecases/#detection-status_7","text":"With transcription: Faulty patch identified correcly With translation: Faulty patch identified overall correcly, but split into 6 different batches for unclear reason. Some of the batches are overlapping.","title":"Detection status"},{"location":"working/transfactory/anomaly_usecases/#usecase-9","text":"Parameter Value prabyss path efir-ot-03042023-2591188-na.mp3 prabyss_id transcript_id transcribed lines 4303 failed transcribed lines 1068-1796, 3741-3768 (...) translated lines 4994 failed transcribed lines 497-543?, 4854-4994 Status Partially resolved ; TODO: merge consecutive batches, including with small intervals in between them With transcription: \u042f \u0434\u0443\u043c\u0430\u044e, \u0447\u0442\u043e \u044d\u0442\u043e \u043e\u0447\u0435\u043d\u044c \u0432\u0430\u0436\u043d\u043e. , \u041f\u043e\u0434\u043f\u0438\u0441\u044b\u0432\u0430\u0439\u0442\u0435\u0441\u044c \u043d\u0430 \u0442\u0435\u043b\u0435\u0433\u0440\u0430\u043c-\u043a\u0430\u043d\u0430\u043b \u0410\u043d\u043d\u044b. , 8 \u0447\u0430\u0441\u043e\u0432 8 \u043c\u0438\u043d\u0443\u0442. , 9arenko. ; ... With translation: To be continued... ?, Do not hurry, when you can not hurry. , He got it.","title":"Usecase 9"},{"location":"working/transfactory/anomaly_usecases/#detection-status_8","text":"With transcription: 7 fault patches were identified: 1068-1095, 1094-1121, 1120-1179, 1778-1793, 2240-2276, 2343-2364, 3739-3768, 3785-3803, all seem to be correct With translation: 7 faulty patches were identified: 476-492, 495-543, 2048-2097, 2105-2115, 2575-2585, 3164-3207, 4852-4869, 4871-4993, all seem to be correct.","title":"Detection status"},{"location":"working/transfactory/anomaly_usecases/#usecase-10","text":"Parameter Value prabyss path novaya-os-zla-i-skuchnyy-evroparlament-efir-ot-14092023-2682103-na.mp3 prabyss_id transcript_id transcribed lines 279 failed transcribed lines 4-14, 106-279 translated lines 1408 failed transcribed lines 1-5 Status Fully resolved With transcription: \u0412\u0440\u0430\u0447\u0435\u0431\u043d\u0430\u044f \u043f\u043e\u043c\u043e\u0449\u044c \u0440\u0430\u043d\u0435\u043d\u044b\u043c \u0431\u043e\u0439\u0446\u0430\u043c \u0441\u0438\u043b\u0430\u043c\u0438 \u043c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u0438\u0445 \u0432\u0437\u0432\u043e\u0434\u043e\u0432. , \u041a\u041e\u041d\u0415\u0426 With translation: TASTY DIALOGUE WITH ELENA BAZHENOVA","title":"Usecase 10"},{"location":"working/transfactory/anomaly_usecases/#detection-status_9","text":"With transcription: Both known faulty patches identified correctly. With translation: Known faulty patch not identified due to theshold settings; another batch identified between #258 and #276","title":"Detection status"},{"location":"working/transfactory/anomaly_usecases/#usecase-11","text":"Parameter Value prabyss path esli-bpla-zapushcheny-s-territorii-rossii-est-kogo-polovit-efir-ot-300523-2623654-na.mp3 prabyss_id transcript_id transcribed lines 856 failed transcribed lines 1-131, 215-856 translated lines 2907 failed transcribed lines 1-8 Status Fully resolved With transcription: \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u041a\u0443\u043b\u0430\u043a\u043e\u0432\u0430 , \u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0421\u0438\u043d\u0435\u0446\u043a\u0430\u044f \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 , \u0416\u0435\u043b\u0435\u0437\u043d\u0430\u044f \u043b\u043e\u0433\u0438\u043a\u0430 With translation: TASTY DIALOGUE WITH ELENA BAZHENOVA","title":"Usecase 11"},{"location":"working/transfactory/anomaly_usecases/#detection-status_10","text":"With transcription: First fault patch identified correctly with start at #4, with first 3 lines being also a repeat, but not identified due to threshold settings. Second fault patch identified correcly. With translation: Faulty patch not identified due to threshold settings.","title":"Detection status"},{"location":"working/transfactory/anomaly_usecases/#usecase-12","text":"Parameter Value prabyss path esli-pashinyan-otdast-karabakh-to-otvetstvennost-budet-na-bryussele-efir-ot-25052023-2621029-na_en.mp3 prabyss_id transcript_id transcribed lines 1620 failed transcribed lines 1-168, 1071-1165 translated lines 1718 failed transcribed lines 1-28, 780-826 Status Partially resolved ; TODO: merge consecutive batches, including with small intervals in between them With transcription: \u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0421\u0438\u043d\u0435\u0446\u043a\u0430\u044f \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 , \u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 , \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 , \u0417\u0434\u0440\u0430\u0432\u0441\u0442\u0432\u0443\u0439\u0442\u0435, \u0442\u043e\u0432\u0430\u0440\u0438\u0449\u0438 \u0433\u0432\u0430\u0440\u0434\u0435\u0439\u0446\u044b! , \u0416\u0435\u043b\u0435\u0437\u043d\u0430\u044f \u041b\u043e\u0433\u0438\u043a\u0430 , \u0422\u041e\u0420\u0416\u0415\u0421\u0422\u0412\u0415\u041d\u041d\u0410\u042f \u041c\u0423\u0417\u042b\u041a\u0410 , \u0413\u041e\u0412\u041e\u0420\u0418\u0422 \u041d\u0410 \u0418\u041d\u041e\u0421\u0422\u0420\u0410\u041d\u041d\u041e\u041c \u042f\u0417\u042b\u041a\u0415 , \u041c\u0423\u0417\u042b\u041a\u0410\u041b\u042c\u041d\u0410\u042f \u0417\u0410\u0421\u0422\u0410\u0412\u041a\u0410 With translation: TASTY DIALOGUE WITH ELENA BAZHENOVA , We are proud of you!","title":"Usecase 12"},{"location":"working/transfactory/anomaly_usecases/#detection-status_11","text":"With transcription: First 2 consecutive faulty patches (up to #163) identified almost correctly: there is a small gap in between, and a few lines are not counted at the end of the 2nd patch due to threshold being set to 10. Second 2 consecutive faulty patches (between #1092 and #1165) identified almost correctly: a few lines at the beggining of the 1st are not counted due to the threshold settings. With translation: Both known faulty patches identified correctly; in addition, another patch identifed between #778 and #826","title":"Detection status"},{"location":"working/transfactory/anomaly_usecases/#usecase-13","text":"Parameter Value prabyss path genotsid-v-otnoshenii-sovetskogo-naroda-efir-ot-23032023-2585101-na.mp3 prabyss_id transcript_id transcribed lines 528 failed transcribed lines 1-528 translated lines 1896 failed transcribed lines None Status Partially resolved ; TODO: take case into account? With transcription: \u0416\u0415\u041b\u0415\u0417\u041d\u0410\u042f \u041b\u041e\u0413\u0418\u041a\u0410","title":"Usecase 13"},{"location":"working/transfactory/anomaly_usecases/#detection-status_12","text":"With transcription: Two consecutive faulty patches were identified, first starting with #3 due to threshold settings, second ending at #527. Batches split is due to different case of the repeat line. With translation: Lack of anomaly identified correcly.","title":"Detection status"},{"location":"working/transfactory/anomaly_usecases/#usecase-14","text":"Parameter Value prabyss path triumfalnaya-pobeda-edinoy-rossii-efir-ot-11092023-2680088-na.mp3 prabyss_id transcript_id transcribed lines 1316 failed transcribed lines 1-134, 320 (\u043d\u0435\u0442) translated lines 4397 failed transcribed lines None Status Partially resolved ; TODO: do something with the solitary line With transcription: \u041f\u041e\u0414\u041f\u0418\u0421\u042b\u0412\u0410\u042e\u0429\u0418\u0415\u0421\u042f \u0422\u0415\u041b\u0415\u0424\u041e\u041d\u041d\u042b\u0415 \u0421\u0418\u0413\u041d\u0410\u041b\u0418\u0417\u0410\u0426\u0418\u0418 , \u041f\u041e\u0414\u041f\u0418\u0421\u042b\u0412\u0410\u042e\u0429\u0418\u0415 \u0421\u0418\u0413\u041d\u0410\u041b\u0418\u0417\u0410\u0426\u0418\u0418","title":"Usecase 14"},{"location":"working/transfactory/anomaly_usecases/#detection-status_13","text":"With transcription: 2 consecutive faulty patches identified correctly (up to #134). Single faulty item (#320) not identified With translation: Lack of anomaly identified correctly","title":"Detection status"},{"location":"working/transfactory/anomaly_usecases/#usecase-15","text":"Parameter Value prabyss path vsyo-vertitsya-vokrug-rossii-efir-ot-11092023-2679995-na.mp3 prabyss_id transcript_id transcribed lines 2023 failed transcribed lines 1-99 translated lines 717 failed transcribed lines 1-717 Status Fully resolved With transcription: \u0420\u0435\u0434\u0430\u043a\u0442\u043e\u0440 \u0441\u0443\u0431\u0442\u0438\u0442\u0440\u043e\u0432 \u0410.\u0421\u0438\u043d\u0435\u0446\u043a\u0430\u044f \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u0415\u0433\u043e\u0440\u043e\u0432\u0430 , \u041a\u043e\u0440\u0440\u0435\u043a\u0442\u043e\u0440 \u0410.\u041a\u0443\u043b\u0430\u043a\u043e\u0432\u0430 , \u0410.\u041a\u0443\u043b\u0430\u043a\u043e\u0432\u0430 , \u0411\u043e\u043b\u0435\u0435! , \u0411\u044b\u0441\u0442\u0440\u0435\u0435! With translation: TASTY DIALOGUE WITH ELENA BAZHENOVA","title":"Usecase 15"},{"location":"working/transfactory/anomaly_usecases/#detection-status_14","text":"With transcription: 4 faulty patches identified: 2-26 + 3-99; 581-593, 1052-1113, all seem to be correct With translation: Faulty patch identified correctly","title":"Detection status"},{"location":"working/transfactory/early_plan/","text":"Set up a PostgreSQL database to store user information, job details, and the results of Whisper AI processing. Install and configure Supabase to provide secure file storage for uploaded files. Develop the web user interface for the application, allowing users to create accounts, log in, submit jobs for Whisper AI processing, and view the results of their jobs. Implement the API for interacting with the GPU farm, allowing the application to send jobs for processing and retrieve the results. Test the web application and API to ensure they are working correctly and can handle the expected workload. Deploy the web application and API to a server, and configure the database and file storage to ensure they are accessible to the application. Monitor the application and make any necessary adjustments to ensure it is running smoothly and efficiently.","title":"Speech-to-Text via openai/Whisper. A Subproject Plan"},{"location":"working/transfactory/node_requirements/","text":"OUTDATED Overview Transfactory is a distributed application aimed at transforming audio into text using OpenAI's Whisper. Infrastructure-wise, it consists of 2 main entities: 1) a single Supply & Processing Station (SPS), and 2) a number of Transfactory Nodes (TN), that communicate between themselves using a Postgres database. TNs perform the transcribing tasks, and SPS supplies them with new jobs and processes what's been done. Transfactory node minimal requirements Transcribing is a very resource-demanding process as it is, and using Whisper's large model (default standard) makes it even more so. Below is the approximate minimally acceptable configuration. GPU (Graphics card): NVIDIA-based with at least 12 GB of memory and non-limited processing speed. Model line RTX 4070 would do well, for example. Watch out for cards that are manufactured with processing speed limitation: if they can't be used by virtual currency miners, they can't be used for machine learning, either; The amount of memory lesser than 12 GB won't allow us to use large Whisper model, and this is problematic not only because smaller models provide lesser quality, but also because in this case we would need to differentiate and account for the model used, and this is an unwanted spike in complexity. If you have a PC with a minimally viable GPU (or a better one), then you probably don't need any more instructions. If you're assembling a new PC, use the GPU as a starting point in some online PC-constructor (take https://pcbuilder.net/list/ as a generic example, and keep in mind that localized variants are usually better). Setting up transfactory node on Windows machine This process is kinda tricky, so it's advisable to follow the following steps in the order they are provided: any digression may lead to unwanted complications. Windows 10 or 11 is assumed. Preconditions An existing Poetry -based project whisper , torch , torchaudio , torchvision are not installed. If they are, it's better to remove them. Steps Add llvmlite as explicit dependency. It's a package required by pytorch ; when installing just the torch, it may try to use a version of this library that would cause failure. As of writing this, version 0.40.1 was most recent; Add numba as explicit dependency. Same story as with llvmlite . As of writing this, version 0.57.1 was the most recent; Add koila as a dependency. This is a thin wrapper over torch that resolves some of its common issues. Add poethepoet as a dependency. This is a task-runner for Poetry that can be used for many things, but here it will be used to install CUDA-based version of pytorch ; Add openai-whisper package as a dependency using https://github.com/openai/whisper.git . If llvmlite and numba were pre-installed successfully, this is likely to go without hiccup; Add following section to pyproject.toml : [tool.poe.tasks] force-cuda = \"python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\" force-cuda-out = \"python -m pip uninstall torch torchvision torchaudio\" This adds 2 Poe tasks. Unlike regular dependencies, Poe tasks are always installed (ran) manually. Run the uninstall task with the following: poetry run poe force-cuda-out This would uninstall torch family if they are present in the system. If they aren't, no harm will be done. Then run the install task: poetry run poe force-cuda This would install the version of the torch family referenced in the --index-url . Cu118 (i.e. v. 11.8) was the most recent version as of writing this. This workaround with poe is required because Poetry can only deal with the default index (PyPI), and has not way of dealing with alternative ones. To make sure torch can be properly used, open a console, import whisper and run one of the following commands: whisper.torch.cuda.is_available() . True is good. whisper.torch.version.cuda . CUDA version ( 11.8 in this case) is good. whisper.torch.version.__version__ . This prints out the version of torch used; the one including something like +cu118 is good. Finally, in the main script, when using whisper or torch , wrap them into lazy() method of the koila library. In case of whisper it looks like this: ``` from koila import lazy model = lazy(whisper.load_model(name=\"medium\", device=\"cuda\", download_root=\"D:/openai\"), batch=0) ``` This should resolve the infamous CUDA out of memory error (more details here ) If you're still getting the error while using lazy() method, you probably don't have enough memory (for example, this still occurs when trying to load large model with only 8 GB of memory in the system).","title":"Transfactory Node Requirements and Setup"},{"location":"working/transfactory/node_requirements/#overview","text":"Transfactory is a distributed application aimed at transforming audio into text using OpenAI's Whisper. Infrastructure-wise, it consists of 2 main entities: 1) a single Supply & Processing Station (SPS), and 2) a number of Transfactory Nodes (TN), that communicate between themselves using a Postgres database. TNs perform the transcribing tasks, and SPS supplies them with new jobs and processes what's been done.","title":"Overview"},{"location":"working/transfactory/node_requirements/#transfactory-node-minimal-requirements","text":"Transcribing is a very resource-demanding process as it is, and using Whisper's large model (default standard) makes it even more so. Below is the approximate minimally acceptable configuration. GPU (Graphics card): NVIDIA-based with at least 12 GB of memory and non-limited processing speed. Model line RTX 4070 would do well, for example. Watch out for cards that are manufactured with processing speed limitation: if they can't be used by virtual currency miners, they can't be used for machine learning, either; The amount of memory lesser than 12 GB won't allow us to use large Whisper model, and this is problematic not only because smaller models provide lesser quality, but also because in this case we would need to differentiate and account for the model used, and this is an unwanted spike in complexity. If you have a PC with a minimally viable GPU (or a better one), then you probably don't need any more instructions. If you're assembling a new PC, use the GPU as a starting point in some online PC-constructor (take https://pcbuilder.net/list/ as a generic example, and keep in mind that localized variants are usually better).","title":"Transfactory node minimal requirements"},{"location":"working/transfactory/node_requirements/#setting-up-transfactory-node-on-windows-machine","text":"This process is kinda tricky, so it's advisable to follow the following steps in the order they are provided: any digression may lead to unwanted complications. Windows 10 or 11 is assumed.","title":"Setting up transfactory node on Windows machine"},{"location":"working/transfactory/node_requirements/#preconditions","text":"An existing Poetry -based project whisper , torch , torchaudio , torchvision are not installed. If they are, it's better to remove them.","title":"Preconditions"},{"location":"working/transfactory/node_requirements/#steps","text":"Add llvmlite as explicit dependency. It's a package required by pytorch ; when installing just the torch, it may try to use a version of this library that would cause failure. As of writing this, version 0.40.1 was most recent; Add numba as explicit dependency. Same story as with llvmlite . As of writing this, version 0.57.1 was the most recent; Add koila as a dependency. This is a thin wrapper over torch that resolves some of its common issues. Add poethepoet as a dependency. This is a task-runner for Poetry that can be used for many things, but here it will be used to install CUDA-based version of pytorch ; Add openai-whisper package as a dependency using https://github.com/openai/whisper.git . If llvmlite and numba were pre-installed successfully, this is likely to go without hiccup; Add following section to pyproject.toml : [tool.poe.tasks] force-cuda = \"python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\" force-cuda-out = \"python -m pip uninstall torch torchvision torchaudio\" This adds 2 Poe tasks. Unlike regular dependencies, Poe tasks are always installed (ran) manually. Run the uninstall task with the following: poetry run poe force-cuda-out This would uninstall torch family if they are present in the system. If they aren't, no harm will be done. Then run the install task: poetry run poe force-cuda This would install the version of the torch family referenced in the --index-url . Cu118 (i.e. v. 11.8) was the most recent version as of writing this. This workaround with poe is required because Poetry can only deal with the default index (PyPI), and has not way of dealing with alternative ones. To make sure torch can be properly used, open a console, import whisper and run one of the following commands: whisper.torch.cuda.is_available() . True is good. whisper.torch.version.cuda . CUDA version ( 11.8 in this case) is good. whisper.torch.version.__version__ . This prints out the version of torch used; the one including something like +cu118 is good. Finally, in the main script, when using whisper or torch , wrap them into lazy() method of the koila library. In case of whisper it looks like this: ``` from koila import lazy model = lazy(whisper.load_model(name=\"medium\", device=\"cuda\", download_root=\"D:/openai\"), batch=0) ``` This should resolve the infamous CUDA out of memory error (more details here ) If you're still getting the error while using lazy() method, you probably don't have enough memory (for example, this still occurs when trying to load large model with only 8 GB of memory in the system).","title":"Steps"},{"location":"working/transfactory/stats_derivatives/","text":"Table factory_jobs_run_details in service schema contains various data related to the processing of audio files, which is collected for the purposes of anomaly detection. transfactory and transprocessor scripts are set up to add stats records during their run, but also dump the same data to a local logfile. Collected data known_duration INT Duration retrieved when fetching original data, i.e. returned by the source, in seconds. calculated_duration INT Duration retrieved by reading the audiofile, in seconds. file_size INT Size of the audiofile in bytes job_start_timestamp DATETIME Timestamp for when the transcript job was started job_end_timestamp DATETIME Timestamp for when the translation job was finished, or when the transcription job was finished in case translation was not run. transcription_task_duration INT, in seconds. transcription_endtime INT Value of endtime key of the last line in the transcription's .srt file transcribed_lines_count INT Number of produced lines translation_task_duration INT, in seconds. translation_endtime INT Value of endtime key of the last line in the translation's .srt file translated_lines_count INT Number of produced lines Fields prabyss_id , transcrip_id , origin_id and media_table are kept for matching purposes. Derivatives and possible anomaly indicators Difference between known_duration and calculated_duration ?? Healthy value: from -10 to 10 Significant difference might indicate: file download was incomlete; or returned data is bs because parser contains some error; or returned data is bs because of something else. Difference between calculated_duration and transcription_endtime / translation_endtime . Healthy value: TBD Significant difference might indicate: corresponding job ( transcription or translation ) was botched Ratio of translated_lines_count / transcribed_lines_count to calculated_duration Healthy value: TBD Significant difference might indicate: TBD Ratio of translated_lines_count / transcribed_lines_count to translation_task_duration / transcription_task_duration Healthy value: TBD Significant difference might indicate: TBD","title":"Transfactory Stats Derivatives"},{"location":"working/transfactory/stats_derivatives/#collected-data","text":"known_duration INT Duration retrieved when fetching original data, i.e. returned by the source, in seconds. calculated_duration INT Duration retrieved by reading the audiofile, in seconds. file_size INT Size of the audiofile in bytes job_start_timestamp DATETIME Timestamp for when the transcript job was started job_end_timestamp DATETIME Timestamp for when the translation job was finished, or when the transcription job was finished in case translation was not run. transcription_task_duration INT, in seconds. transcription_endtime INT Value of endtime key of the last line in the transcription's .srt file transcribed_lines_count INT Number of produced lines translation_task_duration INT, in seconds. translation_endtime INT Value of endtime key of the last line in the translation's .srt file translated_lines_count INT Number of produced lines Fields prabyss_id , transcrip_id , origin_id and media_table are kept for matching purposes.","title":"Collected data"},{"location":"working/transfactory/stats_derivatives/#derivatives-and-possible-anomaly-indicators","text":"Difference between known_duration and calculated_duration ?? Healthy value: from -10 to 10 Significant difference might indicate: file download was incomlete; or returned data is bs because parser contains some error; or returned data is bs because of something else. Difference between calculated_duration and transcription_endtime / translation_endtime . Healthy value: TBD Significant difference might indicate: corresponding job ( transcription or translation ) was botched Ratio of translated_lines_count / transcribed_lines_count to calculated_duration Healthy value: TBD Significant difference might indicate: TBD Ratio of translated_lines_count / transcribed_lines_count to translation_task_duration / transcription_task_duration Healthy value: TBD Significant difference might indicate: TBD","title":"Derivatives and possible anomaly indicators"},{"location":"working/transfactory/transcript_processing_flow/","text":"The transcript flow consists of the following parts: Transfactory - a script that accepts audio files as input and produces transcript files for output. It's a subproject that has to do with automatic transcription of the media episodes with openai/Whisper . Associated status: uploaded . Changes to status: in progress , processed Transprocessor - a script that accepts raw transcript files produced by the factory as input, and outputs their final storage versions. Associated status: processed . Changes to status: cleanup Transcleanup - a script that occasionally scans the working bucket and removes files that were processed. Associated status: cleanup . Changes to status: finalized Transsupply - a script that makes sure there are always enough records in the prabyss table with initial status. Communication between scripts is indirect and is achieved via the dedicated prabyss table. Only one script within the flow has (or should have) the permissions to remove files. Flow The transsuply is responsible for adding new files to the prabyss bucket, and, correspondingly, new records to the prabyss table. After an initial batch of files (100) is uploaded, TS would occasionally query the table for number of records with uploaded status, and upload just enough to have 100 of them. The transfactory queries prabyss table and fetches one or more records with uploaded status. TF then uses record fields to access and download corresponding audio files, followed by the whisper magic. The start of the process is marked with updating record's status to in progress . Once done with a file, TF would upload the produced transcript file ( .srt ) and update the status of its record in the table to processed . The transprocessor queries prabyss table and fetches all records with processed status. TP downloads and processes the files. Once done with a record, TP updates its status to cleanup . The transcleanup queries prabyss table and fetches all records with cleanup status, to remove them, along with accompanying .srt files, from the bucket. Once done, TC updates record's status to finalized .","title":"Transcript Processing Flow"},{"location":"working/transfactory/transcript_processing_flow/#flow","text":"The transsuply is responsible for adding new files to the prabyss bucket, and, correspondingly, new records to the prabyss table. After an initial batch of files (100) is uploaded, TS would occasionally query the table for number of records with uploaded status, and upload just enough to have 100 of them. The transfactory queries prabyss table and fetches one or more records with uploaded status. TF then uses record fields to access and download corresponding audio files, followed by the whisper magic. The start of the process is marked with updating record's status to in progress . Once done with a file, TF would upload the produced transcript file ( .srt ) and update the status of its record in the table to processed . The transprocessor queries prabyss table and fetches all records with processed status. TP downloads and processes the files. Once done with a record, TP updates its status to cleanup . The transcleanup queries prabyss table and fetches all records with cleanup status, to remove them, along with accompanying .srt files, from the bucket. Once done, TC updates record's status to finalized .","title":"Flow"}]}